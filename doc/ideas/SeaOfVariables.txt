Sea of Variables


Idea:
  - Design an intermediate representation for code similar to the sea of nodes (from TurboFan), except textual, and all edges in the graph are represented by use of variables.
  - Introduce control and effect variables to express dependencies, rather relying on program order.
  - Each variable can be assigned only once, i.e. SSA.
  - Programs without loops consist of a list of instructions that simply use variables, apply operators, and assign to variables.
  - This implies that programs without loops can be a topologically-sorted list of instructions, with no ordering or control flow, nop scopes, etc.
  - Loops consist of inductively-defined (recursive) variables with a base (i.e. *initial*) case.

==== Values ===================================================================================================

Value ::=
  | on
  | off
  | bottom
  | <TargetVal>
  | <SrcVal>

The most fundamental concept is a *value*.
The IR itself has two *control values*, {on} and {off}, and the rest of the values come from the source language <SrcVal> or the target language <TargetVal>.
Control is a primitive that is used to enable or disable computations and to choose between values.

The IR can be used with multiple different source and target languages.
For example, suppose the source language is Wasm.
Values in Wasm consist of (i32.const 0), (f32.const 3.4), etc.
Further, suppose the target language is x86-64 machine code.
In x86-64 machine code, the values consist of 8-, 16-, 32-, 64-, and 128-bit untyped bit patterns, as well as symbol values like addresses of functions, objects, etc.

The distinction between the source and target languages is only meaningful to a translator (i.e. lowering), so without loss of generality we consider only a source language from here on.

Importantly, the evaluator itself introduces a value called {bottom} _|_ to indicate a lack of a value.
This is used to model an absence of a value, such as when a user computation depends on control that is {off}.
As we'll see, all computations that receive a control input of {off} will produce {bottom} as their results.

==== Types ===================================================================================================

Type ::=
   control
 | <SrcType>
 | <TargetType>

The next most fundamental concept is the notion of a *type* which is an organization of values into sets.
The IR itself has only one special type, {control}, which contains the values {on} and {off}.

The rest of the types come from either the source or target language, and organize source and target values according to their own rules.

==== Operators ===================================================================================================

Operator ::=
   <SrcOp>
 | <TargetOp>

Operators work on *values* and produce new values; thus they are similar to uninterpreted functions.
There are no built-in operators in the IR.
Instead, like values and types, the source and target language define operators.
Each operator has input types and output types, and there is a way to get the input and output types for each source and target operator:

def GetSrcOpInputType(op: SrcOp) -> Type*
def GetSrcOpOutputType(op: SrcOp) -> Type*
def GetTargetOpInputType(op: TargetOp) -> Type*
def GetTargetOpOutputType(op: TargetOp) -> Type*

==== Variables ===================================================================================================

Var ::= id

VarDef ::= id : Type

Variables provide the ability to refer to values and the results of computations more than once.
They are the "storage" mechanism of programs expressed in the IR.
In *valid* programs, each variable appears in *exactly one* VarDef (i.e. SSA), where it is given a type.

==== Params ===================================================================================================

Param ::=
    SrcParam
  | TargetParam

Parameters are how programs can be defined in terms of inputs whose values are unknown at compile time.
They are provided by either the source or target language.
They can be bound to variables via a dedicated instruction described below.

==== Instructions ===================================================================================================

VarWithType ::= Var: Type

Instr ::=
    def VarWithType    = const(Value);
 |  def VarWithType    = param(Param*);
 |  def VarWithType    = if(Var, Var, Var);
 |  def (VarWithType*) = loop(Var, (Var*), (Var*)) { Instr* }
 |  end(Var*);
 |  def (VarWithType*) = Operator(Var*);

Instructions allow creating complete programs.
All instructions have zero or more input variables and zero or more output variables.
The output variables are shown on the left hand side of the {=} and the input variables occur on the right.
Note again that in *valid* programs, each variable must occur in *exactly one* VarDef.
There are five built-in instructions: {const}, {param}, {if}, {loop}, and {end}.

---- Defining constants as variables ---------------------------------------------------------------------
  {def x: Type = const(v)} assigns the value {v} to the variable {x}.
  This allows a constant to be used in a computation.

---- Defining "parameters", such as parameters to a function ---------------------------------------------
  {def x: Type = param} declares a variable that binds a parameter from the source or target language to a variable in the IR.
  For example, suppose the source language is Wasm.
  We could have {def x: Wasm.i32 = param(WasmFuncParam[0])}, which defines a variable that corresponds to a Wasm function's parameter, which can then be used in the program.

---- Choosing between alternatives -------------------------------------------------------------------------
  {def z: Type = if(c, x, y)} is a built-in choice operator that given a variable {c} of type {control}, will choose either {x} or {y}.
  If {c = on}, then this instruction assigns {z = x}.
  If {c = off}, then this instructions it assigns {z = y}.
  Finally, if {c = bottom}, then this instruction assigns {z = bottom}.
  Note that {Type} could be {control}, so we can use {if} to build negation by
    {
      def a: control  = param;
      def t: control  = const(on);
      def f: control  = const(off);
      def na: control = if(a, f, t);
      end(na);
    }
  Note that we can also negate an {if} by simply swapping the second and third inputs.
    {
      def x: T = if(a, b, c);
      def nx: T = if(a, c, b);
    }

---- Syntactic sugar for negation ------------------------------------------------------------------
  For convenience, because negation is common, we allow the {!} operator to negative control variables on inputs by extending the grammar:

Var ::= id
  | !id

  This allows:
  {
    def a: WasmInt = const(Wasm#0);
    def b: WasmInt = const(Wasm#1);
    def x: control = param(Src@1);
    def z: WasmInt = if(!c, a, b);
  }

---- Defining the "end" of a program ---------------------------------------------------------------
  { () = end(v*) } is the built-in instruction that allows define the "end" of a computation, i.e. the return value(s).
  When {end} has all of inputs evaluated, it "produces output".
  The variables {v*} specify the values (including control and effects) that are output, so when we say a program "produces output", the output is exactly those variables flowing to {end}.
  Evaluating a program can then be thought of as finding and evaluating all "end" instructions by recursively evaluating their inputs.

---- Evaluating operators on values -------------------------------------------------------------------
  { (x*) = Operator(v*) } is the instruction that connects source and target level operations to the variables in the program.
  It represents evaluating the Operator with the values of the input variables on the right hand side and assigning the resulting values to the variables defines on the left hand side.

---- Performing inductive computations ----------------------------------------------------------------
  { (x*) = loop(c, (i*), (r*)) { body } } defines an inductive computation where {c} represents a control variable, {i*} represents the *initial value of variables {x*} and {r*} represents the *recursive* value of variables in {x*}.
  Note that {body} consists of *nested instructions*.
  Like other instructions, {body} may validly reference variables defined in {x*}, but *unlike* other instructions, variable definitions in {body} may be referenced by {c} and {r}.
  This allows loops to have recursively-defined evaluations; in fact the control condition variable {c} is itself recursively-defined.
  In valid programs, instructions following the loop may reference variables defined in {x*} and *also* those defined in {body}.

==== Valid programs ====================================================================================

Sea-of-variables programs are like dataflow or dependence graphs, except that all computation, including control and effects (which we will see later are confined to the source/target language), are simply variable assignments.
As we've seen, valid programs must define each variable exactly once.
For validation, an easy syntactic check then establishes SSA.
Instructions are connected to each other through variable references, i.e. simply by name.
This means there is no "state" in the sea of variables, and other than the required nesting in loops, the syntactic order of instructions doesn't matter for evaluation.
However, by convention we topologically-sort instructions so that all variables referenced in right-hand sides occur *after* the instructions that define them.

The {loop} and {if} constructs express choice, which is how control flow from other languages is represented.
These instructions require an input variable which is of type {control}.

==== Typechecking ======================================================================================

The sea-of-variables has only one built-in type, {control}, and all other types are defined by either the source or target language.
The {if} and {loop} require one input to be of type {control} but are otherwise polymorphic and are parameterized over the source and target languages' subtyping relations:

def SrcIsSubtype: Type x Type
def TargetIsSubtype: Type x Type

X = Y
--------
sub(X, Y)

SrcIsSubtype(X, Y)
---------------------
sub(X, Y)

TargetIsSubtype(X, Y)
---------------------
sub(X, Y)


Env |- c: control  Env |- x: X   Env |- y: Y   sub(X, Z)  sub(Y, Z)
----------------------------------------------------------------------------------------------------
Env; (z: Z) = if(c, x, y) |- z: Z

Env |- c: control  Env |- x: X   Env |- y: Y   sub(X, Z)  sub(Y, Z)
----------------------------------------------------------------------------------------------------
Env; (z: Z) = loop(c, x, y) |- z: Z



==== Evaluation =======================================================================================

Evaluating the sea of variables is straightforward.
To evaluate *any* instruction, the evaluator must first evaluate the inputs.
Since a valid program is topologically sorted, we have two completely valid evaluation strategies: *forward* and *backward*.
In the forwards evaluation strategy, we simply execute each instruction in order, assigning its results to variables.
In the backwards evaluation strategy, we can use "backwards" recursion to evaluate each instruction, starting {end} instructions from the bottom.

First, consider a program with no {if} and no {loop}.
In the forward evaluation strategy, since the program is topologically sorted, all inputs to an instruction must have been defined before the instruction, so the evaluation simply keeps a map {Var -> Value}.
Before evaluating an instruction, it simply looks up all variables' values in the map, and then executes the instruction.
For {const}, the evaluator asks the source or target language for a value and sets the variable.
For {param}, the evaluator asks the source or target language to retrieve an initial value, and then updates the map for the new variable defined.
For {Operator}, the evaluator performs an invocation of the source or target operator after first mapping variables to their values and then updates the map with the result.
For {if}, we directly define the results thusly:

E |- c: on    E |- x: xv
--------------------------------
E eval z = if(c, x, y) => E; z: xv

E |- c: off    E |- y: yv 
--------------------------------
E eval z = if(c, x, y) => E; z: yv

E |- c: bottom
--------------------------------
E eval z = if(c, x, y) => E; z: bottom

That is, straightforwardly, if the condition variable is {on}, the instruction evaluates to the value of the variable x, {xv}.
If the condition variable is {off}, the instruction evaluates to the value of the variable y, {yv}.
Otherwise, if the control variable is {bottom}, the instruction also evaluates to bottom.

Loops are a little tricky in that they are recursively defined.
To accomplish recursion, we exploit the fact that environments are immutable, and let the evaluation rules simply recurse, careful to do the base case "first".
We distinguish the base case by first evaluating the loop when the environment has *no binding* for the control condition.

E |- c: off   E |- z: zv
-------------------------------------------------
E eval z = loop(c, i, r) { body } => E; z: zv

E |- c: bottom
-------------------------------------------------
E eval z = loop(c, i, r) { body } => E; z: bottom

E |- c: ?  E |- i: iv    E'=(E; z: iv eval z = loop(c, i, r))  E' |- c: on  E' eval r |- rv
------------------------------------------------------------------------
E eval z = loop(c, i, r) { body } => E'; z: rv

==== An {off} control input causes instructions to produce {bottom} outputs =======================================

The IR uses control values to enable and disable computations (Operators).
As we saw, the {on} and {off} values are used by {if} and {loop} to select between alternatives.
For other instructions, control values enable or disable the computation altogether.
For {Operator} instructions, *any* control input being {off} "short-circuits" the computation, and it produces {bottom} for all of its outputs.
Therefore, for source languages that have stateful {Operator}s, control can be used to guard updates to the source-level state.
We model this in how we define evaluation of {Operator}; if any one of the control inputs evaluates to {off}, then the instruction produces {off} for any control output and {bottom} for any non-control output.
(Incidentally, this sometimes means that infinite loops can be known to be infinite; it's fine to replace uses of the variables outside the loop with {bottom}, just not the control output.

E |- c: off   T != control 
------------------------------------------------------------------------
E eval z: T = Operator(..., c, ...) { body } => E'; z: bottom

E |- c: off 
------------------------------------------------------------------------
E eval z: Control = Operator(..., c, ...) { body } => E'; z: off

This means that in order to evaluate any {Operator}, all of its control inputs must be {on}.
Incidentally, this offers an optimization opportunity for recursive evaluators: try to evaluate control inputs to an {Operator} before all other inputs, and if any is {off}, skip evaluation of the other inputs and produce the appropriate outputs.

==== Using control and effects ========================================================================

Unlike the sea-of-nodes, which has explicit control, effect, and value edges, the sea of variables only distinguishes control so that it can implement {if} and {loop} as primitives and guard stateful source {Operator}s.
Effect is not a separate concept in the sea of variables.
Instead, the source and target languages can model their state with effect variables that require no special support in the IR.
For example, to model a mutable store in a source language, we can do the typical "world passing" approach where the state of the world is captured in a variable that has a special type, e.g. "SrcWorld".
That permits coarse-grained ordering of effectful operations, allowing one to properly order stateful reads and writes.

Program to unconditionally write the global xyz to 0:
{
  def v: SrcInt    = const(0);
  def w: SrcWorld  = param(SrcWorldStart);
  def w1: SrcWorld = SrcWriteGlobal_xyz(w, v);
  end(w1);
}

Program to unconditionally increment global xyz:
{
  def v: SrcInt    = const(Src#1)
  def w: SrcWorld  = param(SrcWorldStart)
  def p: SrcInt    = SrcReadGlobal_xyz(w)
  def r: SrcInt    = SrcIntAdd(p, v)
  def w1: SrcWorld = SrcWriteGlobal_xyz(w, r)
  end(w1);
}

Notice the above program *unconditionally* increments the global.
It doesn't depend on any condition and instead relies on the "effect" dependency of the world renaming to accomplish ordering.

There are two approaches to achieving conditional updates: either integrate control into the effect chain through operators, or make the operators that update state take control.

Program to increment global xyz conditionally, by adding control input to SrcWriteGlobal:
{
  def v: SrcInt    = const(Src#1);
  def w: SrcWorld  = param(SrcWorldStart);
  def c: control   = param(Src@1);
  def p: SrcInt    = SrcReadGlobal_xyz(w);
  def r: SrcInt    = SrcIntAdd(p, v);
  def w1: SrcWorld = SrcWriteGlobal_xyz(c, w, r);
  def m1: SrcWorld = if(c, w1, w);
  end(m1);
}

The above program now has an additional parameter, {c}, which is of type control.
The caller of this code can pass {on} to enable the write, or {off} to disable it.
The operator {SrcWriteGlobal} will execute and produce a new {SrcWorld} if {c = on}, producing {bottom} otherwise.
To merge the two possibilities and produce a {SrcWorld} in either case, the {end} in this program chooses either the input source world {w} or the new world, depending on the same control variable.

In this example, we made the end depend on either the old or new world, depending on the control.
We can think of the evaluator "not demanding" the modified world {w1} if the control condition {c} is {off}.
Depending on how the evaluator is implemented, this can make evaluation more efficient, and remove the need for an explicit, completely immutable representation of worlds.

==== The implicit {start} control  ========================================================================

As we've now seen, control variables can be used to select amongst alternatives and to enable and disable stateful computations.
Often source languages will model state-updating operators with control input parameters.
In the first example, we expressed an unconditional state update first by having no control input, and then in the third example, we guarded conditional update of the global variable with a control input parameter.
Omitting the control input seems to imply an implicit control parameter with the value {on}.
As we'll see later, compositionality of sea-of-variables programs (i.e. to be able to call one from another) will encounter problems if we rely on implicit arguments, especially with important things like control.
Instead of implicit arguments, we define an implicit *parameter* to every program called {start} which is of type control.
Thus {start} can be used instead of a source-defined parameter or a constant control variable.
This makes programs both easier to read, easier to compose, and easier to reason about; inlining a program into another program is more or less substituting the arguments for the parameters, and having the {start} parameter makes things easy.


TODO[1]

==== Slicing the effects for more reorderings =========================================================

Because effects are just values and types from the source and target languages, it's up to them to define the granularity at which effects are modeled, if at all.
For example, in a typed language like Java or Virgil, type-based alias analysis is very effective at distinguishing stores to mutable state (like object fields) that do not affect other reads.
Source language transformations can then "relax" effect dependencies based on alias analysis by introducing more, finer-grained effect variables to model different parts of the heap.

TODO: show example of write to two different fields.


==== Modeling anti-dependencies =======================================================================

Some compilers model write-after-read dependencies (aka *anti-dependencies*), which allows reads to move more freely.
For example, in a source program that read mutable state S, then write mutable state S, the ordering between the write and the read could be expressed in the IR through world renaming; the write somehow depends on a chain of operators that depends on the read.
But world-renaming is too strict: the write doesn't *need* the read at all.
In fact, if nothing else in the program actually needs the result of the read, the compiler should be able to delete it.
Yet in the sea of variables as presented, all references are true dependencies.
This is the role of an anti-dependence edge.

An anti-dependence edge {R = W} prevents a write from being scheduled *before* a read.
It doesn't prevent the read {R} from being deleted if nothing else references it.
The edge also does *not* require that the read *dominate* the write; it could be scheduled on a completely separate path that doesn't even lead to the write.
Instead, the edge only requires that the write not be scheduled *before* the read.

We can add anti-dependencies to the sea of variables through a side-annotation on instructions:

AfterInstr ::=
  Instr after (Var*)

The {after} annotation adds an anti-dependence for this instruction, requiring that it cannot be placed *before* the definition of any variables in the clause.

{
  def w: SrcWorld  = param(SrcWorldStart);
  def k1: SrcInt   = const(Src#1);
  def p: SrcInt    = SrcReadGlobal_xyz(w);
  def w1: SrcWorld = SrcWriteGlobal_xyz(start, w, k1) after p;
  end(w1, p);
}

In the above example, the anti-dependence between the write of the global variable and the read of the global variable is expressed with the {after} annotation.
This enforces the write comes after the read, so that the write doesn't overwrite the source language's state before reading it.
TODO: should this be {after} and instruction that reads the state, {after} a variable defined by an instruction that reads the state, or {after} the variable that defines the state?

==== Optimizing the sea of variables ==================================================================

What's so great about the sea of variables?
After all, it seems like evaluating it recursively (especially with loops) is not particularly efficient.
It's not immediately obvious how expressive it is, how it could be constructed from a CFG easily, and it's not a representation from which code can immediately be generated.

First, yes, direct evaluation of a sea of variables program (with loops) is indeed very inefficient, but it's not a major drawback.
It's much more common to evaluate loop-free programs, and topologically-sorting the instructions makes for efficient evaluation in either the forward or backward direction.
But to run a program with loops at a reasonable speed, it makes more sense to schedule it, which produces a runnable CFG which has linear control flow.
Even a naive scheduling algorithm will produce a program that's efficient (by interpreter standards), and doesn't require explicit world-passing, but can use actual mutable state.

What makes the sea of variable shine is the optimization potential.
It represents the next step in abstraction from CFG -> SSA-CFG -> SoN -> SoV.
In the same way that we see benefits from SSA directly connecting dataflow between computations, making dependence explicit, the sea of nodes made control and effect dependence explicit.
That makes local reasoning, including pattern-matching, strength reduction, folding, and common subexpression elimination, more thorough.
They "canonicalize" the representation of computations which are directed acyclic graphs.
In both representations, phis, effect phis, and merges (which are really control phis) make merges of alternatives explicit, allowing data flow equations and local reasoning to be sound.
But the drawback of merges and phis is that they don't encode *why* the different alternatives would be chosen; they encode information about execution paths, not about conditions.
Now, the sea of variable makes all conditions explicit and uniform by modeling them with control variables.
What would have been a {phi} in SSA/SoN that relates control flow edges because an {if} on a condition variable.
Because of this decoupling of paths and reconnection with conditions, possibilities for finding equivalences are increased.

==== Double diamonds ====================================================================================

A good illustration of the difference between CFG, SSA-CFG, SoN, and SoV is two chained conditionals.

Consider the Virgil function:

def foo(c: bool) -> (int, int) {
    var z: int;
    var w: int;
    if (c) z = 0;
    else z = 1;
    if (c) w = 0;
    else w = 1;
    return (z, w);
}

Notice that this program has two source language if's that both use {c} as a condition.
In a traditional CFG, this will result in the classic "double diamond" where control splits, merges, splits again, and then merges again.

   B0
  /  \
B1    B2
  \  /
   M1
  /  \
B2    B3
  \  /
   M2

In SSA-CFG, at both merges M1 and M2, each will have a phi which represents the alternatives for the original source variables {z} and {w}, respectively.
If we assume that {M1: z = phi(1, 0)} and {M2: w = phi(1, 0)}, we notice that they are almost the same computation...except they are in different merge blocks M1 and M2.
We can't just conclude that phis with equivalent inputs are equivalent--they are in different blocks.

In this example, the sea of nodes representation would also produce similar-looking phis, but their control inputs would be explicit control edges that point to merges, looking something like {z = phi(M1, 1, 0)} and {w = phi(M2, 1, 0)}.
This is a major benefit because GVN on the sea of nodes can now do local reasoning using all inputs; the explicit control input means GVN will determine that this phis are not equivalent.
(If it did happen that the control inputs to the phis were the same, then GVN would *correctly* conclude they are equivalent, and merge them, which is nice!)


Now, as it happens, as humans we can see that the original program is equivalent to this:

def foo(c: bool) -> (int, int) {
    var z = if(c, 1, 0);
    var w = if(c, 1, 0);
    return (z, w);
}

Not only is this more compact (using Virgil if expressions, or C ternary), but it makes it more obvious that actually {z} and {w} are the same computation.
Why is that?
Because the *condition* is in the computation.
This is basically what the sea of variables representation would produce; it makes all *conditions* explicit:

{
  def k0: SrcInt = const(Src#0);
  def k1: SrcInt = const(Src#1);
  def c: control = param(Param@0);
  def z: SrcInt  = if(c, k1, k0);
  def w: SrcInt  = if(c, k1, k0);
  end(z, w)
}

Great, now we can see that {z} and {w} are equivalent instructions on equivalent inputs.
In the sea of variables, these are *always* equivalent computations and can be merged:

{
  def k0: SrcInt = const(Src#0);
  def k1: SrcInt = const(Src#1);
  def c: control = param(Param@0);
  def z: SrcInt  = if(c, k1, k0);
  end(z, z);
}

Like phis from SSA and the sea of nodes, we can do all kinds of smart dataflow reasoning about {if}, such as {if(c, v, v) = v}, we can do the obvious constant folding: { c = const(on); z = if(c, x, y) => c = const(on); z = x }, etc.
The benefit is that *all phis* from all different choices of the same condition, no matter where they occured in order in the source program, will eventually become choies on equivalent conditions.

==== What about double diamonds where effects are involved? =============================================

TODO: the construction algorithm will thread effects through, and inputs to ifs will end up depending on outputs of other ifs.
It all works out: the schedule will chain or nested repeated conditionals into a valid order, rebuilding a new CFG that may be equivalent to the original CFG, or may mereley resemble it.
In general, the program produced by the scheduler might do branches in a completely new, but equivalent way.

==== Strength reduction and folding =====================================================================

Obviously, all strength-reduction and constant-folding optimizations apply.
If during an optimization all the input variables to an instruction are known to be constants, the instruction can be folded.
This works for effectful operations as well, with careful design to modeling effects, as the "world-passing" would need to have actual values representing worlds.
Control constructs such as {if} easily fold away; but {loop} can also fold away if the condition is not {on}.
For a {loop} whose condition becomes {on}, we make all {loops} define a control variable that is effectively the negation of the condition, and code following the loop must depend on it.
Since this "termination" control value then becomes known to be {bottom}, code after the loop can be removed, except we don't remove {end}.


==== Dead code elimination ==============================================================================

Easy; if no {end} depends on an instruction (transitively), the instruction is dead.
This holds for variables defined by a loop too.

// TODO[1]
Program to increment global xyz conditionally, by adding control input to SrcWriteGlobal:
{
  def v: SrcInt    = const(Src#1);
  def w: SrcWorld  = param(SrcWorldStart);
  def c: control   = param(Src@1);
  def p: SrcInt    = SrcReadGlobal_xyz(w);
  def r: SrcInt    = SrcIntAdd(p, v);
  def w1: SrcWorld = SrcSplitWorld(c, w);
  def w2: SrcWorld = SrcWriteGlobal_xyz(w1, r);
  end(w2);
}

We can do a similar thing, but in a perhaps more convoluted way, by integrating the control into the effect variable using the {SrcSplitWorld} operator, which will return a new "copy" of the world or the old world, depending the control variable.

==== What is bottom, really? ==============================================================================

As we've seen, {bottom} is a special indicator for an absence of a value.
It appears during evaluation of {Operator}s whose control is {off}, and serves to indicate that the computation wasn't evaluated.
If we say that {end} produces an "output" for a program. is it possible that a program produces {bottom} as output?
Consider a program with a control parameter and a source integer parameter:

P1 {
  def c: control  = param(SrcParam1);
  def p: SrcInt   = param(SrcParam2);
  def x: SrcInt   = SrcIntNeg(c, p);
  def y: SrcInt   = SrcIntNeg(c, x);
  end(y);
}

In this example, the program takes the input {p}, negates it, then negates the result of that, and returns it.
Yet, notice that source negation takes a control input.
Per the evaluation rules, all operators a short-circuited, so when {c=off}, the program should produce {y=bottom}.

Let's assume that {SrcIntNeg} is integer negation {-}, so we know some facts about it.
So we have:

P1(p, c=on) = p
P1(p, c=off) = bottom

That is, because of this pesky control, we have a program that produces {bottom} for some inputs.
This seems like a problem.
Maybe it means it's a buggy program?

Now, as humans we want to build compilers that optimize programs.
Since the operator {SrcIntNeg} is integer negation, our source program optimizer is tempted to recognize {-(-(v))=v} for all source integers {v} and thus optimize the program by strength-reducing two negations:

P2 {
  def c: control  = param(SrcParam1);
  def p: SrcInt   = param(SrcParam2);
  end(p);
}

We have:

P2(p, c=on)=p
P2(p, c=off)=p

They differ!
We have optimized the program in a way that seems wrong, because it no longer produces {bottom}, i.e. fails to produce an output.
It seems to have different behavior.

Then, a further dead-code elimination pass could produce:

P3 {
  def p: SrcInt   = param(SrcParam2);
  end(p);
}

The new program now no longer depends on any control at all!
It will always produce an output that is its (source) parameter.

P3(p)=p

==== Bottom means the program execution was incomplete  =========================================================================================

We can now have an interpretation for {bottom} that makes sense from both an evaluation perspective and a program optimization perspective.

Definition: An /input/ to a program is a labelled set of assignments of variables to values to {param} instructions.

Definition: An /output/ of a program is the labelled set of assignments of variables to values resulting from evaluate a program's ends on an input.

Definition: An assignment of variables is /well-defined/ if each has a value appropriate to its type, and the value is not {bottom}.

Definition: An assignment of variables is /partial/ if at least one has the value {bottom}.

Definition: A program is /well-defined/ if, for all inputs, its ends are well-defined.

Definition: A program is /partial/ if there is at least one input which produces a partial output.

Definition: Program p is /observationally-equivalent/ to q if for every input to q that is well-defined, p's output is equivalent.

Definition: Program p is /equivalent/ to q if it is observationally-equivalent to q, and q is well-defined.


Intuitively, a program is well-defined if it always returns full results, and programs are equivalent if they always return the same results.
Some programs don't produce results for all inputs.
We can think of the programs as bad or the inputs as bad--it's doesn't really matter.
Observational equivalence allows us to say that programs are equivalent when the output matches for all the "good" inputs.
Observational equivalence is a partial order, meaning that a oeq b does not necessarily imply b oeq a.

Also note that once there is *one* output that is a {bottom}, then this definition of observational equivalence does not require equivalence on the remaining variables.
So in some sense, a bottom for any output variable is poisonous.

---- Bottom and optimization --------------------------------------------------------------------------------------

The definitions allow us to talk about *correct* program transformations (i.e. optimizations).

An program transform is *correct* if for any program p, it produces a program p' that is *observationally equivalent* to p.
