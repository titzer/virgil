// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def I_ADDD	= 0x01;		def I_ADDQ	= 0x11;
def I_ORD	= 0x02;		def I_ORQ	= 0x12;
def I_ADCD	= 0x03;		def I_ADCQ	= 0x13;
def I_ANDD	= 0x04;		def I_ANDQ	= 0x14;
def I_SUBD	= 0x05;		def I_SUBQ	= 0x15;
def I_XORD	= 0x06;		def I_XORQ	= 0x16;
def I_CMPD	= 0x07;		def I_CMPQ	= 0x17;
def I_MULD	= 0x08;		def I_MULQ	= 0x18;
def I_NEGD	= 0x09;		def I_NEGQ	= 0x19;
def I_NOTD	= 0x0A;		def I_NOTQ	= 0x1A;
def I_TESTD	= 0x0B;		def I_TESTQ	= 0x1B;
def I_LEAD	= 0x0C;		def I_LEAQ	= 0x1C;
def I_DIVD	= 0x0D;		def I_DIVQ	= 0x1D;
def I_IDIVD	= 0x0E;		def I_IDIVQ	= 0x1E;
def I_INCD	= 0x0F;		def I_INCQ	= 0x1F;

def I_DECD	= 0x20;		def I_DECQ	= 0x30;
def I_SHLD	= 0x21;		def I_SHLQ	= 0x31;
def I_SARD	= 0x22;		def I_SARQ	= 0x32;
def I_SHRD	= 0x23;		def I_SHRQ	= 0x33;
def I_CDQ	= 0x24;		def I_CQO	= 0x34;
def I_SWITCHD	= 0x25;		def I_SWITCHQ	= 0x35;
def I_ADDSS	= 0x26;		def I_ADDSD	= 0x36;
def I_SUBSS	= 0x27;		def I_SUBSD	= 0x37;
def I_MULSS	= 0x28;		def I_MULSD	= 0x38;
def I_DIVSS	= 0x29;		def I_DIVSD	= 0x39;
def I_SQRTSS	= 0x2A;		def I_SQRTSD	= 0x3A;
def I_MOVSS	= 0x2B;		def I_MOVSD	= 0x3B;
def I_CVTSS2SD	= 0x2C;		def I_CVTSD2SS	= 0x3C;
def I_PCMPEQD	= 0x2D;		def I_PCMPEQQ	= 0x3D;
def I_UCOMISS	= 0x2F;		def I_UCOMISD	= 0x3F;

def I_CVTSS2SID	= 0x40;		def I_CVTSS2SIQ = 0x50;
def I_CVTSD2SID	= 0x41;		def I_CVTSD2SIQ = 0x51;
def I_CVTSI2SSD	= 0x42;		def I_CVTSI2SSQ = 0x52;
def I_CVTSI2SDD	= 0x43;		def I_CVTSI2SDQ = 0x53;
def I_ROUNDSS	= 0x45;		def I_ROUNDSD	= 0x55;
def I_PSLLD	= 0x46;		def I_PSLLQ	= 0x56;
def I_PSRLD	= 0x47;		def I_PSRLQ	= 0x57;

def I_TRUNCS_U64 = 0x48;	def I_TRUNCD_U64 = 0x58;

def I_MOVB		= 0x60;
def I_MOVBSX		= 0x61;
def I_MOVBZX		= 0x62;
def I_MOVW		= 0x63;
def I_MOVWSX		= 0x64;
def I_MOVWZX		= 0x65;
def I_JMP		= 0x66;
def I_JC		= 0x67;
def I_SETC		= 0x68;
def I_CALL		= 0x69;
def I_CALLER_IP		= 0x6A;
def I_CALLER_SP		= 0x6B;
def I_TEST_ALLOC	= 0x6C;
def I_CMPB		= 0x6F;
def I_THROW		= 0x70;
def I_THROWC		= 0x71;
def I_CMPXCHG8		= 0x72;
def I_CMPXCHG16		= 0x73;
def I_CMPXCHG32		= 0x74;
def I_CMPXCHG64		= 0x75;
def I_MOVD		= 0x76;
def I_MOVQ		= 0x77;
def I_SYSCALL		= 0x78;

def I_QD_DIFF = I_ADDQ - I_ADDD; // Used to compute 64-bit opcode from 32-bit opcode

// Addressing modes:
// REG = GPR register, XMM = XMM register, IMM = immediate
// OP = register|stack, XOP = XMM register|stack
// MRRSD = [reg + reg * scale + disp]
def AM_NONE	 = 0x00;
def AM_REG_OP	 = 0x01;
def AM_MRRSD_REG = 0x02;
def AM_MRRSD_IMM = 0x03;
def AM_REG_MRRSD = 0x04;
def AM_OP	 = 0x05;
def AM_OP_IMM	 = 0x06;
def AM_OP_REG	 = 0x07;
def AM_XMM_REG	 = 0x08;
def AM_XMM_OP	 = 0x09;
def AM_OP_XMM	 = 0x0A;
def AM_XMM_MRRSD = 0x0B;
def AM_MRRSD_XMM = 0x0C;
def AM_XMM_IMM	 = 0x0D;
def AM_REG_XOP	 = 0x0E;
def AM_XMM_XMM	 = 0x0F;

// Opcode = rounding # cond # addressing mode # opcode
def AM_SHIFT = u5.view(8);
def COND_SHIFT = u5.view(13);
def ROUNDING_SHIFT = u5.view(17);

def ABS_MARKER = 0x11223344;
def REL_MARKER = 0xAABBCCDD;

def MATCH_NEG = true;
def MATCH_OP_I = true;
def MATCH_ADDR_ADD = true;
def MATCH_SCALE = true;

def Regs: X86_64RegSet;
def Conds: X86_64Conds;

// Generates X86-64 machine code from SSA.
class SsaX86_64Gen extends SsaMachGen {
	def asm: X86_64MacroAssembler;
	def m = SsaInstrMatcher.new();
	var labelUses: List<(int, Label)>;
	var kernel: Kernel;
	def dwarf: Dwarf;

	new(context: SsaContext, mach: MachProgram, asm, w: MachDataWriter, dwarf) super(context, mach, Regs.SET, w) {
		if (dwarf != null) dwarf.context = context;
	}

	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => {
				emitIntBinop(I_ADDD, i);
			}
			IntSub => {
				var yval = m.intbinop(i);
				if (MATCH_NEG && m.xconst && m.xint == 0) return emit1(I_NEGQ | (AM_OP << AM_SHIFT), ovwReg(i, m.y));
				emitIntBinop(I_SUBD, i);
			}
			IntMul => {
				// XXX: multiply by 1, 2, 3, 4, 5, 9, powers of 2
				var opcode = selectWidth(i, I_MULD);
				m.intbinop(i);
				ovwReg(i, m.x);
				if (tryUseImm32(m.y)) return emitN(opcode | (AM_OP_IMM << AM_SHIFT));
				use(m.y);
				return emitN(opcode | (AM_REG_OP << AM_SHIFT));
			}
			IntDiv => emitIntDivMod(i, false);
			IntMod => emitIntDivMod(i, true);
			BoolAnd => emitSimpleBinop(I_ANDD, i, m.boolbinop(i));
			BoolOr => emitSimpleBinop(I_ORD, i, m.boolbinop(i));
			IntAnd => emitIntBinop(I_ANDD, i);
			IntOr => emitIntBinop(I_ORD, i);
			IntXor => emitIntBinop(I_XORD, i);
			IntShl => emitShift(i, selectWidth(i, I_SHLD));
			IntSar => emitShift(i, selectWidth(i, I_SARD));
			IntShr => emitShift(i, selectWidth(i, I_SHRD));
			IntViewI => emitIntViewI(i);
			FloatViewI(is64) => {
				emit2(if(is64, I_MOVSD, I_MOVSS) | (AM_XMM_REG << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			IntCastF(is64) => emitIntConvertF(i, is64);
			FloatPromoteI(is64) => emitFloatRoundI(i, is64);
			FloatRoundI(is64) => emitFloatRoundI(i, is64);
			FloatRound(is64) => {
				var round = int.view(X86_64Rounding.TO_NEAREST.tag) << ROUNDING_SHIFT;
				emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			FloatAbs(is64) => {
				var t = newTmp(if(is64, Float.FLOAT64, Float.FLOAT32));
				emit2(if(is64, I_PSLLQ, I_PSLLD) | (AM_XMM_IMM << AM_SHIFT), ovwv(t, getVReg(i.input0()), Regs.XMM_CLASS), useInt(1));
				emit2(if(is64, I_PSRLQ, I_PSRLD) | (AM_XMM_IMM << AM_SHIFT), ovwv(getVReg(i), t, Regs.XMM_CLASS), useInt(1));
			}
			FloatCeil(is64) => {
				var round = int.view(X86_64Rounding.TO_POS_INF.tag) << ROUNDING_SHIFT;
				emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			FloatFloor(is64) => {
				var round = int.view(X86_64Rounding.TO_NEG_INF.tag) << ROUNDING_SHIFT;
				emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			FloatRoundD => {
				emit2(I_CVTSD2SS | (AM_XMM_OP << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			IntViewF(is64) => {
				emit2(if(is64, I_MOVSD, I_MOVSS) | (AM_REG_XOP << AM_SHIFT), dfnReg(i), use(i.input0()));
			}
			IntTruncF(is64) => emitIntConvertF(i, is64);
			FloatPromoteF => {
				emit2(I_CVTSS2SD | (AM_XMM_OP << AM_SHIFT), dfnReg(i), use(i.input0()));
			}
			BoolEq,
			BoolNot,
			RefEq,
			IntEq,
			IntLt,
			IntLteq,
			FloatEq,
			FloatNe,
			FloatLt,
			FloatLteq,
			PtrLt,
			PtrLteq => {
				var conds = emitCmp(i, false);
				emitSetC(getVReg(i), conds);
			}
			FloatAdd(is64) => emitFloatBinop(i, if(is64, I_ADDSD, I_ADDSS));
			FloatSub(is64) => emitFloatBinop(i, if(is64, I_SUBSD, I_SUBSS));
			FloatMul(is64) => emitFloatBinop(i, if(is64, I_MULSD, I_MULSS));
			FloatDiv(is64) => emitFloatBinop(i, if(is64, I_DIVSD, I_DIVSS));
			FloatBitEq(is64) => {
				var t1 = newTmp(i.op.typeArgs[0]), t2 = newTmp(Int.TYPE);
				emit2(if(is64, I_PCMPEQQ, I_PCMPEQD) | (AM_XMM_XMM << AM_SHIFT),
					ovwv(t1, getVReg(i.input0()), Regs.XMM_CLASS),
					useReg(i.input1()));
				emit2(I_MOVSD | (AM_REG_XOP << AM_SHIFT), dfnv(t2, Regs.GPR_CLASS), usev(t1, 0));
				emit2(I_ANDD | (AM_OP_IMM << AM_SHIFT), ovwv(getVReg(i), t2, Regs.GPR_CLASS), useInt(1));
			}
			FloatSqrt(is64) => {
				emit2(if(is64, I_SQRTSD, I_SQRTSS) | (AM_XMM_OP << AM_SHIFT), dfnReg(i), use(i.input0()));
			}
			ConditionalThrow(exception) => {
				var conds = emitCmp(i.input0(), true);
				// TODO: handle multiple conditions for conditional throw
				emit1(ArchInstrs.FLAG_NO_GAP | I_THROWC | encodeCond(conds.cond1), useExSource(exception, i.source));
			}
			PtrLoad => {
				var lt = i.op.typeArgs[1], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				if (size == 0) {
					useMrrsd(t);
					useInt(0);
					if (!i.facts.O_NO_NULL_CHECK) useExSource(V3Exception.NullCheck, i.source);
					return emitN(I_TESTQ | (AM_MRRSD_IMM << AM_SHIFT));
				}
				var opcode: int;
				match (size) {
					1 => opcode = if(V3.isSigned(lt), I_MOVBSX, I_MOVBZX) | (AM_REG_MRRSD << AM_SHIFT);
					2 => opcode = if(V3.isSigned(lt), I_MOVWSX, I_MOVWZX) | (AM_REG_MRRSD << AM_SHIFT);
					4 => opcode = if(FloatType.?(lt),
						I_MOVSS | (AM_XMM_MRRSD << AM_SHIFT),
						I_MOVD | (AM_REG_MRRSD << AM_SHIFT));
					8 => opcode = if(FloatType.?(lt),
						I_MOVSD | (AM_XMM_MRRSD << AM_SHIFT),
						I_MOVQ | (AM_REG_MRRSD << AM_SHIFT));
					_ => context.fail("invalid size for load");
				}
				dfnReg(i);
				useMrrsd(t);
				if (!i.facts.O_NO_NULL_CHECK) useExSource(V3Exception.NullCheck, i.source);
				return emitN(opcode);
			}
			PtrStore => {
				var lt = i.op.typeArgs[1], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				useMrrsd(t);
				var y = i.input1();
				var opcode: int;
				if (tryUseImm32(y)) {
					// store of constant to memory; ignore floating point if imm32
					match (size) {
						0 => opcode = I_TESTQ;
						1 => opcode = I_MOVB;
						2 => opcode = I_MOVW;
						4 => opcode = I_MOVD;
						8 => opcode = I_MOVQ;
						_ => context.fail("invalid size for store");
					}
					opcode |= (AM_MRRSD_IMM << AM_SHIFT);
				} else {
					match (size) {
						0 => opcode = I_TESTQ | (AM_MRRSD_REG << AM_SHIFT);
						1 => opcode = I_MOVB | (AM_MRRSD_REG << AM_SHIFT);
						2 => opcode = I_MOVW | (AM_MRRSD_REG << AM_SHIFT);
						4 => opcode = if(FloatType.?(lt),
							I_MOVSS | (AM_MRRSD_XMM << AM_SHIFT),
							I_MOVD | (AM_MRRSD_REG << AM_SHIFT));
						8 => opcode = if(FloatType.?(lt),
							I_MOVSD | (AM_MRRSD_XMM << AM_SHIFT),
							I_MOVQ | (AM_MRRSD_REG << AM_SHIFT));
						_ => context.fail("invalid size for store");
					}
					useReg(y);
				}
				if (!i.facts.O_NO_NULL_CHECK) useExSource(V3Exception.NullCheck, i.source);
				emitN(opcode);
			}
			PtrCmpSwp => {
				var t = matchMrrsd(i.input0());
				useMrrsdConstrained(t, Regs.NOT_RAX);
				var vi = i.inputs[2].dest;
				useFixed(vi, Regs.NOT_RAX);
				var expect = useFixed(i.input1(), Regs.RAX); // expect in RAX as extra operand
				var size = mach.sizeOf(i.op.typeArgs[1]);
				var opcode: int;
				match (size) {
					1 => opcode = I_CMPXCHG8;
					2 => opcode = I_CMPXCHG16;
					4 => opcode = I_CMPXCHG32;
					8 => opcode = I_CMPXCHG64;
					_ => context.fail("invalid size for cmpswp");
				}
				if (!i.facts.O_NO_NULL_CHECK) useExSource(V3Exception.NullCheck, i.source);
				emitN(opcode | (AM_MRRSD_REG << AM_SHIFT));
				emit1(ArchInstrs.FLAG_NO_GAP | I_SETC | encodeCond(X86_64Conds.Z), dfnReg(i));
			}
			PtrAdd => emitSimpleBinop(I_ADDQ, i, (0, m.binop(i)).0);
			PtrSub => emitSimpleBinop(I_SUBQ, i, (0, m.binop(i)).0);
			Alloc => {
				if (mach.allocStub != null) {
					// generate a call to the allocation stub
					dfnFixed(i, Regs.RAX); // TODO: manual calling convention
					kill(Regs.ALL);
					refmap(null);
					useImm(mach.allocStub);
					useFixed(i.input0(), Regs.RSI); // TODO: manual calling convention
					useExSource(null, i.source);
					emitN(I_CALL);
				} else {
					// no allocation stub; emit test allocation
					emit1(I_TEST_ALLOC | (AM_OP << AM_SHIFT), ovwReg(i, i.input0()));
				}
			}
			CallAddress(funcRep) => emitCall(i, funcRep);
			CallerIp => {
				emit1(I_CALLER_IP, dfnReg(i));
			}
			CallerSp => {
				emit1(I_CALLER_SP, dfnReg(i));
			}
			CallKernel(kernel) => emitCallKernel(i, kernel);
			TupleGetElem => ; // do nothing; calls will define their projections
			_ => return context.fail1("unexpected opcode %s", i.op.opcode.name);
		}
	}
	def emitIntConvertF(i: SsaApplyOp, is64: bool) {
		var it = IntType.!(i.op.typeArgs[1]);
		var t1 = newTmp(if(is64, Float.FLOAT64, Float.FLOAT32));
		var round = int.view(RoundingMode.TO_ZERO.tag) << ROUNDING_SHIFT;
		emit2(round | if(is64, I_ROUNDSD, I_ROUNDSS) | (AM_XMM_OP << AM_SHIFT), dfnv(t1, Regs.XMM_CLASS), use(i.input0()));
		match (it.rank) {
			SUBU32, SUBI32, I32 => {
				emit2(if(is64, I_CVTSD2SID, I_CVTSS2SID) | (AM_REG_XOP << AM_SHIFT), dfnReg(i), usev(t1, 0));
			}
			U32, SUBI64, SUBU64, I64 => {
				emit2(if(is64, I_CVTSD2SIQ, I_CVTSS2SIQ) | (AM_REG_XOP << AM_SHIFT), dfnReg(i), usev(t1, 0));
			}
			U64 => {
				var t2 = newTmp(if(is64, Float.FLOAT64, Float.FLOAT32)); // because TRUNCD macro overwrites XMM input
				emit2(if(is64, I_TRUNCD_U64, I_TRUNCS_U64), dfnReg(i), ovwv(t2, t1, Regs.XMM_CLASS));
			}
		}
	}
	def emitFloatRoundI(i: SsaApplyOp, is64: bool) {
		var it = IntType.!(i.op.typeArgs[0]);
		match (it.rank) {
			SUBI32, SUBU32, I32 => {
				var opcode = if(is64, I_CVTSI2SDD, I_CVTSI2SSD);
				emit2(opcode | (AM_XMM_REG << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
			U32, SUBI64, SUBU64, I64, U64 => {
				var opcode = if(is64, I_CVTSI2SDQ, I_CVTSI2SSQ);
				emit2(opcode | (AM_XMM_REG << AM_SHIFT), dfnReg(i), useReg(i.input0()));
			}
		}
	}
	def emitIntViewI(i: SsaApplyOp) {
		var ft = IntType.!(i.op.typeArgs[0]);
		var tt = IntType.!(i.op.typeArgs[1]);
		var y = i.input0();
		match (tt.rank) {
			U32,
			I32 => return emitMovd(i); // nop
			SUBU32 => return emit2(I_ANDD | (AM_OP_IMM << AM_SHIFT), ovwReg(i, y), useImm(tt.max));
			SUBI32 => return emitShiftShift(i, I_SHLD, I_SARD, tt.ishift);
			_ => ;
		}
		match (ft.rank) {
			SUBI32, I32 => match (tt.rank) {
				U64,
				I64,
				SUBI64 => emitShiftShift(i, I_SHLQ, I_SARQ, ft.lshift);
				SUBU64 => {
					var t1 = newTmp(tt), t2 = newTmp(tt), t3 = newTmp(tt), reg = Regs.GPR_CLASS;
					var am = (AM_OP_IMM << AM_SHIFT);
					emit2(I_SHLQ | am, ovwv(t1, getVReg(i.input0()), reg), useInt(32));
					emit2(I_SARQ | am, ovwv(t2, t1, reg), useInt(32));
					emit2(I_SHLQ | am, ovwv(t3, t2, reg), useInt(tt.lshift));
					emit2(I_SHRQ | am, ovwv(getVReg(i), t3, reg), useInt(tt.lshift));
				}
				_ => ; // handled above
			}
			SUBU32, U32 => {
				emitMovd(i);
			}
			SUBI64, I64 => match (tt.rank) {
				U64,
				I64 => emitMovd(i);
				SUBU64 => emitShiftShift(i, I_SHLQ, I_SHRQ, tt.lshift);
				SUBI64 => emitShiftShift(i, I_SHLQ, I_SARQ, tt.lshift);
				_ => ; // handled above
			}
			SUBU64, U64 => match (tt.rank) {
				U64,
				I64 => emitMovd(i);
				SUBU64  => emitShiftShift(i, I_SHLQ, I_SHRQ, tt.lshift);
				SUBI64  => emitShiftShift(i, I_SHLQ, I_SARQ, tt.lshift);
				_ => ; // handled above
			}
		}
	}
	def emitShiftShift(i: SsaApplyOp, op1: int, op2: int, width: int) {
		var t = newTmp(i.op.typeArgs[1]), reg = anyReg(t);
		emit2(op1 | (AM_OP_IMM << AM_SHIFT), ovwv(t, getVReg(i.input0()), reg), useInt(width));
		emit2(op2 | (AM_OP_IMM << AM_SHIFT), ovwv(getVReg(i), t, reg), useInt(width));
	}
	def emitMovd(i: SsaApplyOp) {
		emit2(I_MOVD | (AM_REG_OP << AM_SHIFT), dfnReg(i), use(i.input0()));
	}
	def emitIntDivMod(i: SsaApplyOp, isMod: bool) {
		var it = IntType.!(i.op.typeArgs[0]);
		var x = i.input0();
		var upper: VReg;
		if (it.signed) {
			if (SsaConst.?(x)) { // constant-fold sign-extension of dividend
				var val = Long.unboxSU(SsaConst.!(x).val, true);
				if (val < 0) upper = getVReg(if(it.width > 32, context.graph.longConst(-1), context.graph.intConst(-1)));
				else upper = getVReg(context.graph.zeroConst());
			} else {
				upper = newTmp(it);
				var opcode = if(it.width > 32, I_CQO, I_CDQ);
				emit2(opcode | (AM_OP_REG << AM_SHIFT), dfnv(upper, Regs.RDX), useFixed(i.input0(), Regs.RAX));
			}
		} else {
			upper = getVReg(context.graph.zeroConst());
		}
		dfnFixed(i, if(isMod, Regs.RDX, Regs.RAX));
		kill(if(isMod, Regs.RAX, Regs.RDX));
		useFixed(x, Regs.RAX);
		usev(upper, Regs.RDX);
		useFixed(i.input1(), Regs.NOT_RAX_RDX);
		if (!i.facts.O_NO_ZERO_CHECK) useExSource(V3Exception.DivideByZero, i.source);
		return emitN(selectWidth(i, if(it.signed, I_IDIVD, I_DIVD)));
	}
	def emitCmp(i: SsaInstr, inblock: bool) -> X86_64CondPair {
		if ((inblock && !inSameBlock(i)) || !SsaApplyOp.?(i)) return emitDefaultCmp(i);
		var apply = SsaApplyOp.!(i);
		match (apply.op.opcode) {
			BoolEq =>	return emitCmpBinop(I_CMPB, apply, Conds.Z);
			RefEq =>	return emitCmpBinop(if(mach.refSize > 4, I_CMPQ, I_CMPD), apply, Conds.Z);
			IntEq =>	return emitCmpBinop(intCmpOpcode(apply), apply, Conds.Z);
			IntLt =>	return emitCmpBinop(intCmpOpcode(apply), apply,
						if(V3.isSigned(apply.op.typeArgs[0]), Conds.L, Conds.C));
			IntLteq =>	return emitCmpBinop(intCmpOpcode(apply), apply,
						if(V3.isSigned(apply.op.typeArgs[0]), Conds.LE, Conds.NA));
			PtrLt =>	return emitCmpBinop(I_CMPQ, apply, Conds.C);
			PtrLteq =>	return emitCmpBinop(I_CMPQ, apply, Conds.NA);
			BoolNot =>	return emitCmp(i.input0(), inblock).negate();
			FloatEq(is64) =>	return emitFloatCmp(is64, apply, Conds.Z, Conds.NP);
			FloatNe(is64) =>	return emitFloatCmp(is64, apply, Conds.Z, Conds.NP).negate();
			FloatLt(is64) =>	return emitFloatCmp(is64, apply, Conds.C, null);
			FloatLteq(is64) =>	return emitFloatCmp(is64, apply, Conds.NA, null);
			// XXX: emitCmp of PtrCmpSwp
			_ => ;
		}
		return emitDefaultCmp(i);
	}
	def emitDefaultCmp(i: SsaInstr) -> X86_64CondPair {
		use(i);
		useInt(0);
		emitN(I_CMPB | (AM_OP_IMM << AM_SHIFT));
		return X86_64CondPair.new(Conds.NZ, false, null);
	}
	def emitFloatCmp(is64: bool, i: SsaApplyOp, cond: X86_64Cond, reflex: X86_64Cond) -> X86_64CondPair {
		var x = i.input0(), y = i.input1();
		var opcode = if(is64, I_UCOMISD, I_UCOMISS);
		emit2(opcode | (AM_XMM_OP << AM_SHIFT), useReg(x), use(y));
		if (x == y && reflex != null) return X86_64CondPair.new(reflex, false, null); // reflexive optimization
		if (i.facts.O_NO_NULL_CHECK) return X86_64CondPair.new(cond, false, null); // no NaN check required
		return X86_64CondPair.new(cond, true, Conds.NP);
	}
	def emitCmpBinop(opcode: int, i: SsaApplyOp, cond: X86_64Cond) -> X86_64CondPair {
		var x = i.input0(), y = i.input1();
		if (SsaConst.?(x) && cond.commute != null) {
			var t = x; x = y; y = t; // swap x and y
			cond = cond.commute;
		}
		if (tryUseImm32(y)) {
			var yop = popLastOperand();
			emit2(opcode | (AM_OP_IMM << AM_SHIFT), use(x), op(yop));
		} else {
			useReg(x);
			use(y);
			emitN(opcode | (AM_REG_OP << AM_SHIFT));
		}
		return X86_64CondPair.new(cond, false, null);
	}
	def selectWidth(i: SsaApplyOp, op: int) -> int {
		return if(intOpWidth(i) > 32, op + I_QD_DIFF, op);
	}
	def intOpWidth(i: SsaApplyOp) -> byte {
		// XXX: factor this out and clean it up
		var t = i.op.typeArgs[0];
		if (IntType.?(t)) return IntType.!(t).width;
		if (t.typeCon.kind == V3Kind.ENUM) return V3.getVariantTagType(t).width;
		if (t.typeCon.kind == V3Kind.ENUM_SET) return V3.getEnumSetType(t).width;
		return 64;
	}
	def intCmpOpcode(i: SsaApplyOp) -> int {
		match ((7 + intOpWidth(i)) >> 3) {
			1 => return I_CMPB;
			2, 3, 4 => return I_CMPD;
			_ => return I_CMPQ;
		}
	}
	def emitCall(call: SsaApplyOp, funcRep: Mach_FuncRep) {
		var func = call.input0(), mi: MachInstr;
		var conv = frame.allocCallerSpace(X86_64VirgilCallConv.getForFunc(mach, funcRep));

		// define the return value(s) of the call
		var rv = getProjections(call);
		for (i < rv.length) {
			var r = rv[i];
			if (r != null) dfnFixed(r, conv.calleeRet(i));
		}
		kill(Regs.ALL);
		refmap(conv);
		var skip = 0;
		if (SsaConst.?(func)) {
			var target = Address<IrMethod>.!(SsaConst.!(func).val);
			useImm(target);
			if (target == null || target.val == null || V3.isComponent(target.val.receiver)) skip = 1;
		} else {
			useFixed(func, Regs.NOT_PARAM);
		}

		// use the arguments to the call
		var inputs = call.inputs;
		for (i = 1 + skip; i < inputs.length; i++) {  // input[0] == func
			useFixed(inputs[i].dest, conv.calleeParam(i - 1));
		}
		useExSource(null, call.source);
		emitN(I_CALL);
	}
	def emitCallKernel(call: SsaApplyOp, kernel: Kernel) {
		var rv = getProjections(call);
		// define the return value(s) of the call
		for (i < rv.length) {
			var r = rv[i];
			if (r != null) dfnFixed(r, X86_64Common.KERNEL_RETURN_REGS[i]); // TODO: eflags?
		}
		kill(Regs.ALL);
		// use the arguments to the call
		var inputs = call.inputs;
		for (i < inputs.length) {
			useFixed(inputs[i].dest, X86_64Common.KERNEL_PARAM_REGS[i]);
		}
		emitN(I_SYSCALL);

	}
	def emitShift(i: SsaApplyOp, opcode: int) {
		// XXX: match x << (y & mask)
		var yval = m.intbinop(i);
		if (MATCH_OP_I && m.yconst) {
			var shiftor = if(m.wide, u6.view(m.ylong), u5.view(m.yint));
			return emit2(opcode | (AM_OP_IMM << AM_SHIFT), ovwReg(i, m.x), useInt(shiftor));
		}
		return emit2(opcode | (AM_OP << AM_SHIFT), ovwRegFixed(i, m.x, Regs.NOT_RCX), useFixed(m.y, Regs.RCX));
	}
	def useMrrsd(x: SsaInstr, y: SsaInstr, scale: byte, disp: Val) {
		if (x == null) useInt(0); // no base register, will ignore
		else useReg(x);
		if (y == null) useInt(0); // no index register, will ignore
		else useReg(y);
		useInt(scale);
		useImm(disp);
	}
	def useMrrsdConstrained(t: (SsaInstr, SsaInstr, byte, Val), constraint: int) {
		var x = t.0, y = t.1, scale = t.2, disp = t.3;
		if (x == null) useInt(0); // no base register, will ignore
		else useFixed(x, constraint);
		if (y == null) useInt(0); // no index register, will ignore
		else useFixed(y, constraint);
		useInt(scale);
		useImm(disp);
	}
	def tryUseImm32(i: SsaInstr) -> bool {
		if (i == null) { useInt(0); return true; }
		if (SsaConst.?(i)) {
			var val = SsaConst.!(i).val;
			match (val) {
				null => { useImm(val); return true; }
				x: Box<int> => { useImm(val); return true; }
				x: Box<long> => if(x.val == int.view(x.val)) { useInt(int.view(x.val)); return true; }
				x: Addr => { useImm(val); return true; }
				x: Box<bool> => { useInt(if(x.val, 1, 0)); return true; }
				x: ArrayRangeStart => { useImm(val); return true; }
			}
		}
		return false;
	}
	def popLastOperand() -> Operand { // XXX: clean up uses of this
		var o = operands[operands.length - 1];
		operands.resize(operands.length - 1);
		return o;
	}
	def matchMrrsd(i: SsaInstr) -> (SsaInstr, SsaInstr, byte, Val) {
		if (SsaConst.?(i)) return (null, null, 1, SsaConst.!(i).val);
		var t = matchAddImm(i, null), i = t.0, disp = t.1;
		var xadd: SsaApplyOp;
		if (MATCH_ADDR_ADD && (xadd = cover(Opcode.PtrAdd.tag, i)) != null) {
			var x = xadd.input0(), y = xadd.input1();
			var xs = matchScale(x);
			if (xs.1 != 1) return (y, xs.0, xs.1, disp);
			var ys = matchScale(y);
			return (x, ys.0, ys.1, disp);
		}
		var is = matchScale(i);
		return (null, is.0, is.1, disp);
	}
	def matchScale(i: SsaInstr) -> (SsaInstr, byte) {
		if (!MATCH_SCALE) return (i, 1);
		if (!SsaApplyOp.?(i)) return (i, 1);
		var apply = SsaApplyOp.!(i);
		if (Opcode.IntMul.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 1) return (m.x, 1);
			if (yval == 2) return (m.x, 2);
			if (yval == 4) return (m.x, 4);
			if (yval == 8) return (m.x, 8);
		} else if (Opcode.IntShl.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 0) return (m.x, 1);
			if (yval == 1) return (m.x, 2);
			if (yval == 2) return (m.x, 4);
			if (yval == 3) return (m.x, 8);
		}
		return (apply, 1);
	}
	def emitIntBinop(opcode: int, i: SsaApplyOp) {
		emitSimpleBinop(selectWidth(i, opcode), i, m.intbinop(i));
	}
	def emitSimpleBinop(opcode: int, i: SsaApplyOp, unused: int) {
		// XXX: select better left operand using liveness
		ovwReg(i, m.x);
		if (tryUseImm32(m.y)) {
			opcode |= (AM_OP_IMM << AM_SHIFT);
		} else { // XXX: cover loads with mrrsd operand
			opcode |= (AM_REG_OP << AM_SHIFT);
			use(m.y);
		}
		emitN(opcode);
	}
	def visitThrow(block: SsaBlock, i: SsaThrow) {
		emit1(I_THROW, useExSource(i.exception, i.source));
	}
	def visitIf(block: SsaBlock, i: SsaIf) {
		var key = i.input0(), conds = emitCmp(key, true), succ = i.block().succ;
		var s0 = succ(0).dest, s1 = succ(1).dest, target: SsaBlock, jmp: SsaBlock;
		var fallthru: SsaBlock;
		if (blocks.isImmediatelyAfter(context.block, s1)) { // fall through to s1
			target = s0;
			fallthru = s1;
		} else if (blocks.isImmediatelyAfter(context.block, s0)) {  // fall through to s0
			conds = conds.negate();
			target = s1;
			fallthru = s0;
		} else {  // cannot fall through
			target = s0;
			jmp = s1;
		}
		if (conds.cond2 == null) { // emit single condition
			emitCondJump(conds.cond1, target);
		} else if (conds.and) { // emit && of two conditions
			var other = if(jmp != null, jmp, fallthru);
			emitCondJump(conds.cond1.negate, other);
			emitCondJump(conds.cond2, target);
		} else { // emit || of two conditions
			emitCondJump(conds.cond1, target);
			emitCondJump(conds.cond2, target);
		}
		if (jmp != null) emit1(ArchInstrs.FLAG_NO_GAP | I_JMP, useLabel(jmp));
	}
	def emitCondJump(cond: X86_64Cond, target: SsaBlock) {
		emit1(ArchInstrs.FLAG_NO_GAP | I_JC | encodeCond(cond), useLabel(target));
	}
	def visitSwitch(block: SsaBlock, i: SsaSwitch) {
		var size = mach.sizeOf(i.keyType);
		useReg(i.input0());
		useScratch(RegClass.I32);
		for (s in block.succs()) useLabel(s.dest);
		emitN(if(size > 4, I_SWITCHQ, I_SWITCHD));
	}
	def visitGoto(block: SsaBlock, i: SsaGoto) {
		var target = i.target();
		if (!blocks.isImmediatelyAfter(context.block, target)) {
			emit1(I_JMP, useLabel(target));  // jump to block if not successor
		}
	}

	def genSaveLocal(loc: int, v: VReg) {
		if (regSet.isCallerStack(loc)) return; // defined by caller, nothing to do
		if (regSet.isStack(loc)) {
			var scratch = Regs.SCRATCH_GPR;
			var opcode = if(v.regClass == RegClass.I32, I_MOVD, I_MOVQ); // no need for XMM for stack-to-stack
			emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, scratch)), op(Operand.Use(null, loc)));
			emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(v, v.spill)), op(Operand.Use(null, scratch)));
			return;
		}
		var opcode: int;
		match (v.regClass) {
			I32 => opcode = I_MOVD | (AM_OP_REG << AM_SHIFT);
			REF, I64 => opcode = I_MOVQ | (AM_OP_REG << AM_SHIFT);
			F32 => opcode = I_MOVSS | (AM_OP_XMM << AM_SHIFT);
			F64 => opcode = I_MOVSD | (AM_OP_XMM << AM_SHIFT);
		}
		emit2(opcode, op(Operand.Def(v, v.spill)), op(Operand.Use(null, loc)));
	}
	def genLoadLocalIntoReg(v: VReg, reg: int) {
		var opcode = if(v.regClass == RegClass.I32 || v.regClass == RegClass.F32, I_MOVD, I_MOVQ);
		if (v.isConst()) {
			var val = SsaConst.!(v.ssa).val;
			match (val) {
				x: Box<long> => if(int.view(x.val) != x.val) return genLoadLongConstIntoReg(x.val, reg);
				x: Float64Val => if(int.view(x.bits) != x.bits) return genLoadLongConstIntoReg(long.view(x.bits), reg);
			}
			emit2(opcode | (AM_OP_IMM << AM_SHIFT), op(Operand.Def(null, reg)), op(Operand.Immediate(val)));
			return;
		}
		emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, reg)), op(Operand.Use(v, v.spill)));
	}
	def genLoadLongConstIntoReg(val: long, reg: int) {
		var addr = mach.getLongConstAddr(val);
		op(Operand.Def(null, reg));
		useMrrsd(null, null, 1, addr);
		emitN(I_MOVQ | (AM_REG_MRRSD << AM_SHIFT));
	}
	def genRestoreLocal(v: VReg, loc: int) {
		var from: int;
		if (regSet.isStack(loc)) {
			// destination is stack
			var opcode = if(v.regClass == RegClass.I32, I_MOVD, I_MOVQ);
			if (v.isConst() && tryUseImm32(v.ssa)) { // emit move immediate directly into stack
				var yop = popLastOperand();
				emit2(opcode | (AM_OP_IMM << AM_SHIFT), op(Operand.Def(null, loc)), op(yop));
				return;
			}
			var scratch = Regs.SCRATCH_GPR; // use GPR as scratch to load constant
			genLoadLocalIntoReg(v, scratch);
			emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(null, loc)), op(Operand.Use(v, scratch)));
			return;
		}
		var xmm = Regs.toXmmr(loc);
		if (xmm != null) {
			var opcode = if(v.regClass == RegClass.F32, I_MOVSS, I_MOVSD);
			if (v.isConst()) {
				var scratch = Regs.SCRATCH_GPR; // use GPR as scratch to load constant into XMM
				genLoadLocalIntoReg(v, scratch);
				emit2(opcode | (AM_XMM_REG << AM_SHIFT), op(Operand.Def(null, loc)), op(Operand.Use(v, scratch)));
			} else {
				emit2(opcode | (AM_XMM_OP << AM_SHIFT), op(Operand.Def(null, loc)), op(Operand.Use(v, v.spill)));
			}
		} else {
			genLoadLocalIntoReg(v, loc);
		}
	}
	def genRegRegMove(src: int, dst: int) {
		var xmmd = Regs.toXmmr(dst);
		if (xmmd != null) {
			emit2(I_MOVSD | (AM_XMM_OP << AM_SHIFT), op(Operand.Def(null, dst)), op(Operand.Use(null, src)));
		} else {
			emit2(I_MOVQ | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, dst)), op(Operand.Use(null, src)));
		}
	}

	def moveOpcode(regClass: RegClass) -> int {
		return if(regClass == RegClass.I32 || regClass == RegClass.F32, I_MOVD, I_MOVQ);
	}
	def genMoveLocLoc(src: (VReg, int), dst: (VReg, int), regClass: RegClass) {
		var from: int;
		if (regSet.isStack(dst.1)) {
			if (regSet.isStack(src.1)) {
				var opcode = moveOpcode(regClass), srcloc = src.1;
				emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(null, Regs.SCRATCH_GPR)), op(Operand.Use(src)));
				emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(dst)), op(Operand.Use(null, Regs.SCRATCH_GPR)));
			} else {
				var xmm = Regs.toXmmr(src.1);
				if (xmm != null) {
					var opcode = if(regClass == RegClass.F32, I_MOVSS, I_MOVSD);
					emit2(opcode | (AM_OP_XMM << AM_SHIFT), op(Operand.Def(dst)), op(Operand.Use(src)));
				} else {
					var opcode = moveOpcode(regClass), srcloc = src.1;
					emit2(opcode | (AM_OP_REG << AM_SHIFT), op(Operand.Def(dst)), op(Operand.Use(src)));
				}
			}
		} else {
			var xmm = Regs.toXmmr(dst.1);
			if (xmm != null) {
				var opcode = if(regClass == RegClass.F32, I_MOVSS, I_MOVSD);
				emit2(opcode | (AM_XMM_OP << AM_SHIFT), op(Operand.Def(dst)), op(Operand.Use(src)));
			} else {
				var opcode = moveOpcode(regClass);
				emit2(opcode | (AM_REG_OP << AM_SHIFT), op(Operand.Def(dst)), op(Operand.Use(src)));
			}
		}
	}
	def genMoveValLoc(src: VReg, dst: (VReg, int), regClass: RegClass) {
		var isImm32 = tryUseImm32(src.ssa), immOp = if(isImm32, popLastOperand());
		if (regSet.isStack(dst.1)) {
			if (isImm32) { // emit move immediate directly into stack
				var opcode = moveOpcode(regClass);
				emit2(opcode | (AM_OP_IMM << AM_SHIFT), op(Operand.Def(dst)), op(immOp));
				return;
			}
			var scratch = Regs.SCRATCH_GPR;
			genMoveValLoc(src, (null, scratch), regClass);
			genMoveLocLoc((null, scratch), dst, regClass);
		} else {
			var xmm = Regs.toXmmr(dst.1);
			if (xmm != null) {
				var scratch = Regs.SCRATCH_GPR;
				genMoveValLoc(src, (null, scratch), regClass);
				var opcode = if(regClass == RegClass.F32, I_MOVSS, I_MOVSD);
				emit2(opcode | (AM_XMM_REG << AM_SHIFT), op(Operand.Def(dst)), op(Operand.Use(null, scratch)));
			} else {
				var opcode = moveOpcode(regClass);
				if (isImm32) {
					var opcode = if(regClass == RegClass.I32, I_MOVD, I_MOVQ);
					emit2(opcode | (AM_OP_IMM << AM_SHIFT), op(Operand.Def(dst)), op(immOp));
				} else {
					var val = SsaConst.!(src.ssa).val;
					var bits: long;
					match (val) {
						x: Box<long> => bits = x.val;
						x: Float64Val => bits = long.view(x.bits);
						_ => return emit2(I_MOVD | (AM_OP_IMM << AM_SHIFT), op(Operand.Def(dst)), op(Operand.Immediate(val)));

					}
					var addr = mach.getLongConstAddr(bits);
					op(Operand.Def(dst));
					useMrrsd(null, null, 1, addr);
					emitN(I_MOVQ | (AM_REG_MRRSD << AM_SHIFT));
				}
			}
		}
	}

	def emitFloatBinop(i: SsaApplyOp, opcode: int) {
		var d = getVReg(i), u = getVReg(i.input0()), reg = anyReg(d);
		emit2(opcode | (AM_XMM_OP << AM_SHIFT), ovwv(d, u, reg), use(i.input1()));
	}
	def emitSetC(dreg: VReg, conds: X86_64CondPair) {
		var opcode = ArchInstrs.FLAG_NO_GAP | I_SETC, gloc = Regs.GPR_CLASS;
		if (conds.cond2 == null) return emit1(opcode | encodeCond(conds.cond1), dfnv(dreg, gloc));
		// emit multiple conditions, and/or them together
		var t1 = newTmp(Bool.TYPE), t2 = newTmp(Bool.TYPE);
		emit1(opcode | encodeCond(conds.cond1), dfnv(t1, gloc));
		emit1(opcode | encodeCond(conds.cond2), dfnv(t2, gloc));
		emit2(if(conds.and, I_ANDD, I_ORD) | (AM_REG_OP << AM_SHIFT), ovwv(dreg, t1, gloc), usev(t2, 0));
	}

	def assemble(opcode: int, a: Array<Operand>) {
		if (opcode < 0) {
			match (opcode) {
				ArchInstrs.ARCH_ENTRY => {
					var adjust = frameAdjust();
					if (adjust > 0) asm.sub_r_i(X86_64Regs.RSP, adjust); // allocate frame
					return;
				}
				ArchInstrs.ARCH_BLOCK => {
					var label = toLabel(a[0]);
					label.pos = asm.pos();
					return;
				}
				ArchInstrs.ARCH_SOURCELINE => {
					var label = toLabel(a[0]), source = toExSource(a[1]).1;
					label.pos = asm.pos();
					dwarf.line.addLineEntry(source, label, firstSourceLine);
					firstSourceLine = false;
					return;
				}
				ArchInstrs.ARCH_NEWVAR => {
					var newvar = toNewVar(a[0]);
					var index = -1, start = 1;
					match (a[start]) {
						Immediate(val) => {
							index = toInt(a[1]);
							start++;
						}
						_ => ;
					}
					var locs = Array<(X86_64Reg, int)>.new(a.length-start);
					for (i = start; i < a.length; i++) {
						locs[i-start] = getLoc(a[i]);
					}
					dwarf.addVar(newvar, index, locs, asm.pos());
					return;
				}
				ArchInstrs.ARCH_DELETEVAR => {
					var n = toInt(a[0]);
					dwarf.deleteVar(n, asm.pos());
					return;
				}
				ArchInstrs.ARCH_UPDATEVAR => {
					var index = toInt(a[0]);
					var locs = Array<(X86_64Reg, int)>.new(a.length-1);
					for (i = 1; i < a.length; i++) {
						locs[i-1] = getLoc(a[i]);
					}
					dwarf.updateVarLoc(index, locs, asm.pos());
					return;
				}
				ArchInstrs.ARCH_RET => {
					var adjust = frameAdjust();
					if (adjust > 0) asm.add_r_i(X86_64Regs.RSP, adjust); // deallocate frame
					asm.ret();
					return;
				}
				ArchInstrs.ARCH_RESTORE => {
					var regClass = toVar(a[0]).regClass;
					var xmm = Regs.toXmmr(toLoc(a[0]));
					if (xmm != null) opcode = if(regClass == RegClass.F32, I_MOVSS, I_MOVSD) | (AM_XMM_OP << AM_SHIFT);
					else opcode = moveOpcode(regClass) | (AM_REG_OP << AM_SHIFT);
				}
				_ => return;
			}
		}
		var start = asm.pos(), addr: Addr;
		var mode = ((opcode >> AM_SHIFT) & 0x1F);
		recordExSource(a); // XXX: double-entries for throw/conditional throw
		match (mode) {
			AM_NONE => {
				assemble_none(opcode, a);
			}
			AM_OP => {
				var loc = toLoc(a[0]), a = Regs.toGpr(loc);
				if (a != null) assemble_r(opcode, a);
				else assemble_m(opcode, loc_m(loc));
			}
			AM_OP_REG => {
				var loc = toLoc(a[0]), b = toGpr(a[1]), a = Regs.toGpr(loc);
				if (a != null) assemble_r_r(opcode, a, b);
				else assemble_m_r(opcode, loc_m(loc), b);
			}
			AM_OP_XMM => {
				var loc = toLoc(a[0]), b = toXmmr(a[1]), a = Regs.toXmmr(loc);
				if (a != null) assemble_s_s(opcode, a, b);
				else assemble_m_s(opcode, loc_m(loc), b);
			}
			AM_OP_IMM => {
				var loc = toLoc(a[0]), imm = toImm(a[1]), a = Regs.toGpr(loc);
				if (a != null) assemble_r_i(opcode, a, imm);
				else assemble_m_i(opcode, loc_m(loc), imm);
			}
			AM_REG_OP => {
				var reg = toGpr(a[0]), loc = toLoc(a[1]), b = Regs.toGpr(loc);
				if (b != null) assemble_r_r(opcode, reg, b);
				else assemble_r_m(opcode, reg, loc_m(loc));
			}
			AM_XMM_OP => {
				var reg = toXmmr(a[0]), loc = toLoc(a[1]), b = Regs.toXmmr(loc);
				if (b != null) assemble_s_s(opcode, reg, b);
				else assemble_s_m(opcode, reg, loc_m(loc));
			}
			AM_XMM_IMM => {
				var reg = toXmmr(a[0]), imm = toB32(toImm(a[1]));
				match (opcode & 0xFF) {
					I_PSLLD => asm.pslld_i(reg, u5.view(imm));
					I_PSRLD => asm.psrld_i(reg, u5.view(imm));
					I_PSLLQ => asm.psllq_i(reg, u6.view(imm));
					I_PSRLQ => asm.psrlq_i(reg, u6.view(imm));
				}
			}
			AM_XMM_REG => {
				assemble_s_r(opcode, toXmmr(a[0]), toGpr(a[1]));
			}
			AM_XMM_XMM => {
				assemble_s_s(opcode, toXmmr(a[0]), toXmmr(a[1]));
			}
			AM_REG_XOP => {
				var reg = toGpr(a[0]), loc = toLoc(a[1]), b = Regs.toXmmr(loc);
				if (b != null) assemble_r_s(opcode, reg, b);
				else assemble_r_m(opcode, reg, loc_m(loc));
			}
			AM_MRRSD_REG => {
				var m = toMrrsd(a, 0);
				assemble_m_r(opcode, m, toGpr(a[4]));
			}
			AM_MRRSD_XMM => {
				var m = toMrrsd(a, 0);
				assemble_m_s(opcode, m, toXmmr(a[4]));
			}
			AM_MRRSD_IMM => {
				var m = toMrrsd(a, 0);
				assemble_m_i(opcode, m, toImm(a[4]));
			}
			AM_REG_MRRSD => {
				var m = toMrrsd(a, 1);
				assemble_r_m(opcode, toGpr(a[0]), m);
			}
			AM_XMM_MRRSD => {
				var m = toMrrsd(a, 1);
				assemble_s_m(opcode, toXmmr(a[0]), m);
			}
			_ => return context.fail1("unknown addressing mode %d", mode);
		}
	}
	def assemble_r(opcode: int, a: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_NEGD => asm.d.neg_r(a);
			I_NOTD => asm.d.not_r(a);
			I_MULD => asm.d.imul_r(a);
			I_INCD => asm.d.inc_r(a);
			I_DECD => asm.d.dec_r(a);
			I_SHLD => asm.d.shl_r_cl(a);
			I_SARD => asm.d.sar_r_cl(a);
			I_SHRD => asm.d.shr_r_cl(a);
			I_DIVD => asm.d.div_r(a);
			I_IDIVD => asm.d.idiv_r(a);

			I_NEGQ => asm.neg_r(a);
			I_NOTQ => asm.not_r(a);
			I_MULQ => asm.imul_r(a);
			I_INCQ => asm.inc_r(a);
			I_DECQ => asm.dec_r(a);
			I_SHLQ => asm.shl_r_cl(a);
			I_SARQ => asm.sar_r_cl(a);
			I_SHRQ => asm.shr_r_cl(a);
			I_DIVQ => asm.div_r(a);
			I_IDIVQ => asm.idiv_r(a);
			I_TEST_ALLOC => {
				var addr = X86_64AddrRef.new(null, null, 1, CiRuntimeModule.HEAP_CUR_LOC, false);
				asm.xadd_m_r(addr, a);
			}
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m(opcode: int, a: X86_64Addr) {
		match (opcode & 0xFF) {
			I_NEGD => asm.d.neg_m(a);
			I_NOTD => asm.d.not_m(a);
			I_MULD => asm.d.imul_m(a);
			I_INCD => asm.d.inc_m(a);
			I_DECD => asm.d.dec_m(a);
			I_SHLD => asm.d.shl_m_cl(a);
			I_SARD => asm.d.sar_m_cl(a);
			I_SHRD => asm.d.shr_m_cl(a);
			I_DIVD => asm.d.div_m(a);
			I_IDIVD => asm.d.idiv_m(a);

			I_NEGQ => asm.neg_m(a);
			I_NOTQ => asm.not_m(a);
			I_MULQ => asm.imul_m(a);
			I_INCQ => asm.inc_m(a);
			I_DECQ => asm.dec_m(a);
			I_SHLQ => asm.shl_m_cl(a);
			I_SARQ => asm.sar_m_cl(a);
			I_SHRQ => asm.shr_m_cl(a);
			I_DIVQ => asm.div_m(a);
			I_IDIVQ => asm.idiv_m(a);

			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_r_r(opcode: int, a: X86_64Gpr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.d.add_r_r(a, b);
			I_ORD  => asm.d.or_r_r(a, b);
			I_ADCD => asm.d.adc_r_r(a, b);
			I_ANDD => asm.d.and_r_r(a, b);
			I_SUBD => asm.d.sub_r_r(a, b);
			I_XORD => asm.d.xor_r_r(a, b);
			I_CMPD => asm.d.cmp_r_r(a, b);
			I_CMPB => asm.cmpb_r_r(a, b);
			I_MULD => asm.d.imul_r_r(a, b);
			I_CDQ => asm.cdq();

			I_ADDQ => asm.add_r_r(a, b);
			I_ORQ  => asm.or_r_r(a, b);
			I_ADCQ => asm.adc_r_r(a, b);
			I_ANDQ => asm.and_r_r(a, b);
			I_SUBQ => asm.sub_r_r(a, b);
			I_XORQ => asm.xor_r_r(a, b);
			I_CMPQ => asm.cmp_r_r(a, b);
			I_MULQ => asm.imul_r_r(a, b);
			I_CQO => asm.cqo();

			I_MOVD => asm.movd_r_r(a, b);
			I_MOVQ => asm.movq_r_r(a, b);
			I_MOVB => asm.movb_r_r(a, b);
			I_MOVBSX => asm.movbsx_r_r(a, b);
			I_MOVBZX => asm.movbzx_r_r(a, b);
			I_MOVWSX => asm.movwsx_r_r(a, b);
			I_MOVWZX => asm.movwzx_r_r(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_s_s(opcode: int, a: X86_64Xmmr, b: X86_64Xmmr) {
		match (opcode & 0xFF) {
			I_ADDSS => asm.addss_s_s(a, b);
			I_SUBSS => asm.subss_s_s(a, b);
			I_MULSS => asm.mulss_s_s(a, b);
			I_DIVSS => asm.divss_s_s(a, b);
			I_SQRTSS => asm.sqrtss_s_s(a, b);
			I_ADDSD => asm.addsd_s_s(a, b);
			I_SUBSD => asm.subsd_s_s(a, b);
			I_MULSD => asm.mulsd_s_s(a, b);
			I_DIVSD => asm.divsd_s_s(a, b);
			I_SQRTSD => asm.sqrtsd_s_s(a, b);
			I_MOVSS => asm.movss_s_s(a, b);
			I_MOVSD => asm.movsd_s_s(a, b);
			I_CVTSS2SD => asm.cvtss2sd_s_s(a, b);
			I_CVTSD2SS => asm.cvtsd2ss_s_s(a, b);
			I_PCMPEQD => asm.pcmpeqd_s_s(a, b);
			I_PCMPEQQ => asm.pcmpeqq_s_s(a, b);
			I_UCOMISS => asm.ucomiss_s_s(a, b);
			I_UCOMISD => asm.ucomisd_s_s(a, b);
			I_ROUNDSS => asm.roundss_s_s(a, b, decodeRounding(opcode));
			I_ROUNDSD => asm.roundsd_s_s(a, b, decodeRounding(opcode));
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_s_m(opcode: int, a: X86_64Xmmr, b: X86_64Addr) {
		match (opcode & 0xFF) {
			I_ADDSS => asm.addss_s_m(a, b);
			I_SUBSS => asm.subss_s_m(a, b);
			I_MULSS => asm.mulss_s_m(a, b);
			I_DIVSS => asm.divss_s_m(a, b);
			I_SQRTSS => asm.sqrtss_s_m(a, b);
			I_ADDSD => asm.addsd_s_m(a, b);
			I_SUBSD => asm.subsd_s_m(a, b);
			I_MULSD => asm.mulsd_s_m(a, b);
			I_DIVSD => asm.divsd_s_m(a, b);
			I_SQRTSD => asm.sqrtsd_s_m(a, b);
			I_MOVSS => asm.movss_s_m(a, b);
			I_MOVSD => asm.movsd_s_m(a, b);
			I_CVTSS2SD => asm.cvtss2sd_s_m(a, b);
			I_CVTSD2SS => asm.cvtsd2ss_s_m(a, b);
			I_PCMPEQD => asm.pcmpeqd_s_m(a, b);
			I_PCMPEQQ => asm.pcmpeqq_s_m(a, b);
			I_CVTSI2SSD => asm.d.cvtsi2ss_s_m(a, b);
			I_CVTSI2SSQ => asm.cvtsi2ss_s_m(a, b);
			I_CVTSI2SDD => asm.d.cvtsi2sd_s_m(a, b);
			I_CVTSI2SDQ => asm.cvtsi2sd_s_m(a, b);
			I_UCOMISS => asm.ucomiss_s_m(a, b);
			I_UCOMISD => asm.ucomisd_s_m(a, b);
			I_ROUNDSS => asm.roundss_s_m(a, b, decodeRounding(opcode));
			I_ROUNDSD => asm.roundsd_s_m(a, b, decodeRounding(opcode));
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_r(opcode: int, a: X86_64Addr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.d.add_m_r(a, b);
			I_ORD =>  asm.d.or_m_r(a, b);
			I_ADCD => asm.d.adc_m_r(a, b);
			I_ANDD => asm.d.and_m_r(a, b);
			I_SUBD => asm.d.sub_m_r(a, b);
			I_XORD => asm.d.xor_m_r(a, b);
			I_CMPD => asm.d.cmp_m_r(a, b);
			I_CMPB => asm.cmpb_m_r(a, b);
			I_TESTD => asm.d.test_m_r(a, b);

			I_ADDQ => asm.add_m_r(a, b);
			I_ORQ =>  asm.or_m_r(a, b);
			I_ADCQ => asm.adc_m_r(a, b);
			I_ANDQ => asm.and_m_r(a, b);
			I_SUBQ => asm.sub_m_r(a, b);
			I_XORQ => asm.xor_m_r(a, b);
			I_CMPQ => asm.cmp_m_r(a, b);
			I_TESTQ => asm.test_m_r(a, b);

			I_MOVD => asm.movd_m_r(a, b);
			I_MOVQ => asm.movq_m_r(a, b);
			I_MOVB => asm.movb_m_r(a, b);
			I_MOVW => asm.movw_m_r(a, b);

			I_CMPXCHG8 => asm.cmpxchgb_m_r(a, b);
			I_CMPXCHG16 => asm.cmpxchgw_m_r(a, b);
			I_CMPXCHG32 => asm.d.cmpxchg_m_r(a, b);
			I_CMPXCHG64 => asm.cmpxchg_m_r(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_s(opcode: int, a: X86_64Addr, b: X86_64Xmmr) {
		match (opcode & 0xFF) {
			I_MOVSS => asm.movss_m_s(a, b);
			I_MOVSD => asm.movsd_m_s(a, b);
		}
	}
	def assemble_s_r(opcode: int, a: X86_64Xmmr, b: X86_64Gpr) {
		match (opcode & 0xFF) {
			I_MOVSS => asm.movd_s_r(a, b);
			I_MOVSD => asm.movq_s_r(a, b);
			I_CVTSI2SSD => asm.d.cvtsi2ss_s_r(a, b);
			I_CVTSI2SSQ => asm.cvtsi2ss_s_r(a, b);
			I_CVTSI2SDD => asm.d.cvtsi2sd_s_r(a, b);
			I_CVTSI2SDQ => asm.cvtsi2sd_s_r(a, b);
		}
	}
	def assemble_r_s(opcode: int, a: X86_64Gpr, b: X86_64Xmmr) {
		match (opcode & 0xFF) {
			I_MOVSS => asm.movd_r_s(a, b);
			I_MOVSD => asm.movq_r_s(a, b);
			I_CVTSS2SID => asm.d.cvtss2si_r_s(a, b);
			I_CVTSS2SIQ => asm.cvtss2si_r_s(a, b);
			I_CVTSD2SID => asm.d.cvtsd2si_r_s(a, b);
			I_CVTSD2SIQ => asm.cvtsd2si_r_s(a, b);
		}
	}
	def assemble_r_m(opcode: int, a: X86_64Gpr, b: X86_64Addr) {
		match (opcode & 0xFF) {
			I_ADDD => asm.d.add_r_m(a, b);
			I_ORD =>  asm.d.or_r_m(a, b);
			I_ADCD => asm.d.adc_r_m(a, b);
			I_ANDD => asm.d.and_r_m(a, b);
			I_SUBD => asm.d.sub_r_m(a, b);
			I_XORD => asm.d.xor_r_m(a, b);
			I_CMPD => asm.d.cmp_r_m(a, b);
			I_CMPB => asm.cmpb_r_m(a, b);
			I_TESTD => asm.d.test_r_m(a, b);

			I_ADDQ => asm.add_r_m(a, b);
			I_ORQ =>  asm.or_r_m(a, b);
			I_ADCQ => asm.adc_r_m(a, b);
			I_ANDQ => asm.and_r_m(a, b);
			I_SUBQ => asm.sub_r_m(a, b);
			I_XORQ => asm.xor_r_m(a, b);
			I_CMPQ => asm.cmp_r_m(a, b);
			I_TESTQ => asm.test_r_m(a, b);

			I_MOVD => asm.movd_r_m(a, b);
			I_MOVQ => asm.movq_r_m(a, b);
			I_MOVB => asm.movb_r_m(a, b);
			I_MOVBSX => asm.movbsx_r_m(a, b);
			I_MOVBZX => asm.movbzx_r_m(a, b);
			I_MOVW => asm.movw_r_m(a, b);
			I_MOVWSX => asm.movwsx_r_m(a, b);
			I_MOVWZX => asm.movwzx_r_m(a, b);

			I_MULD => asm.d.imul_r_m(a, b);
			I_MULQ => asm.imul_r_m(a, b);

			I_CVTSS2SID => asm.d.cvtss2si_r_m(a, b);
			I_CVTSS2SIQ => asm.cvtss2si_r_m(a, b);
			I_CVTSD2SID => asm.d.cvtsd2si_r_m(a, b);
			I_CVTSD2SIQ => asm.cvtsd2si_r_m(a, b);
			I_MOVSS => asm.movd_r_m(a, b);
			I_MOVSD => asm.movq_r_m(a, b);
			_ => return invalidOpcode(opcode);
		}
	}
	def assemble_m_i(opcode: int, a: X86_64Addr, val: Val) {
		var b = toB32(val);
		match (opcode & 0xFF) {
			I_ADDD => asm.d.add_m_i(a, b);
			I_ORD =>  asm.d.or_m_i(a, b);
			I_ADCD => asm.d.adc_m_i(a, b);
			I_ANDD => asm.d.and_m_i(a, b);
			I_SUBD => asm.d.sub_m_i(a, b);
			I_XORD => asm.d.xor_m_i(a, b);
			I_CMPD => asm.d.cmp_m_i(a, b);
			I_CMPB => asm.cmpb_m_i(a, b);
			I_MOVD => asm.movd_m_i(a, b);
			I_TESTD => asm.d.test_m_i(a, b);

			I_ADDQ => asm.add_m_i(a, b);
			I_ORQ =>  asm.or_m_i(a, b);
			I_ADCQ => asm.adc_m_i(a, b);
			I_ANDQ => asm.and_m_i(a, b);
			I_SUBQ => asm.sub_m_i(a, b);
			I_XORQ => asm.xor_m_i(a, b);
			I_CMPQ => asm.cmp_m_i(a, b);
			I_MOVQ => asm.movq_m_i(a, b);
			I_TESTQ => asm.test_m_i(a, b);
			I_MOVB => asm.movb_m_i(a, b);
			I_MOVW => asm.movw_m_i(a, b);
			_ => return invalidOpcode(opcode);
		}
		recordAddr(val);
	}
	def assemble_r_i(opcode: int, a: X86_64Gpr, val: Val) {
		var b = toB32(val);
		match (opcode & 0xFF) {
			I_ADDD => asm.d.add_r_i(a, b);
			I_ORD =>  asm.d.or_r_i(a, b);
			I_ADCD => asm.d.adc_r_i(a, b);
			I_ANDD => asm.d.and_r_i(a, b);
			I_SUBD => asm.d.sub_r_i(a, b);
			I_XORD => asm.d.xor_r_i(a, b);
			I_CMPD => asm.d.cmp_r_i(a, b);
			I_CMPB => asm.cmpb_r_i(a, b);
			I_MULD => asm.d.imul_r_i(a, b);
			I_SHLD => asm.d.shl_r_i(a, u5.view(b));
			I_SARD => asm.d.sar_r_i(a, u5.view(b));
			I_SHRD => asm.d.shr_r_i(a, u5.view(b));

			I_ADDQ => asm.add_r_i(a, b);
			I_ORQ =>  asm.or_r_i(a, b);
			I_ADCQ => asm.adc_r_i(a, b);
			I_ANDQ => asm.and_r_i(a, b);
			I_SUBQ => asm.sub_r_i(a, b);
			I_XORQ => asm.xor_r_i(a, b);
			I_CMPQ => asm.cmp_r_i(a, b);
			I_MULQ => asm.imul_r_i(a, b);
			I_MOVD => asm.movd_r_i(a, b);
			I_MOVQ => asm.movq_r_i(a, b);
			I_SHLQ => asm.shl_r_i(a, u6.view(b));
			I_SARQ => asm.sar_r_i(a, u6.view(b));
			I_SHRQ => asm.shr_r_i(a, u6.view(b));
			_ => return invalidOpcode(opcode);
		}
		recordAddr(val);
	}
	def assemble_none(opcode: int, a: Array<Operand>) {
		match (opcode & 0xFF) {
			I_DIVQ, I_IDIVQ, I_DIVD, I_IDIVD => {
				var loc = toLoc(a[4]), reg = Regs.toGpr(loc);
				if (reg != null) assemble_r(opcode, reg);
				else assemble_m(opcode, loc_m(loc));
			}
			I_JMP => {
				var label = toLabel(a[0]);
				if (label.pos >= 0) asm.jmp_rel(label.pos - asm.pos());
				else asm.jmp_rel_addr(X86_64LabelRef.new(label));
			}
			I_JC => {
				var label = toLabel(a[0]), cond = decodeCond(opcode);
				if (label.pos >= 0) asm.jc_rel(cond, label.pos - asm.pos());
				else asm.jc_rel_addr(cond, X86_64LabelRef.new(label));
			}
			I_SETC => {
				var reg = toGpr(a[0]), cond = decodeCond(opcode);
				asm.set_r(cond, reg);
			}
			I_THROW => {
				var exSource = toExSource(a[0]);
				var addr = mach.runtime.getExceptionDest(asm.pos(), exSource.0, exSource.1);
				asm.jmp_rel_addr(X86_64AddrRef.new(null, null, 1, addr, true));
			}
			I_THROWC => {
				var exSource = toExSource(a[0]), cond = decodeCond(opcode);
				var addr = mach.runtime.getExceptionDest(asm.pos(), exSource.0, exSource.1);
				asm.jc_rel_addr(cond, X86_64AddrRef.new(null, null, 1, addr, true));
			}
			I_SWITCHD, I_SWITCHQ => {
				var is64 = (opcode & 0xFF) == I_SWITCHQ;
				var size = a.length - 2;
				var key = toGpr(a[0]);
				if (is64) asm.cmp_r_i(key, size - 1);
				else asm.d.cmp_r_i(key, size - 1);
				asm.jc_rel_addr(X86_64Conds.A, X86_64LabelRef.new(toLabel(a[a.length - 1])));
				// load from the jump table to follow
				var jtAddr = Addr.new(mach.codeRegion, null, 0);
				var scratch = Regs.toGpr(Regs.SCRATCH_GPR);
				asm.movd_r_m(scratch, X86_64AddrRef.new(null, key, 4, jtAddr, false));
				asm.ijmp_r(scratch);
				// align and emit the jump table
				w.align(4);
				jtAddr.absolute = w.endAddr();
				// emit (32-bit) jump table
				for (i = 2; i < a.length; i++) {
					w.zeroN(4);
					asm.patcherImpl.recordAbs32(asm.pos() - 4, X86_64LabelRef.new(toLabel(a[i])));
				}
			}
			I_CALL => {
				var target: bool, outgoing: MachCallConv, livepoint = -1;
				for (o in a) {
					match (o) {
						Immediate(val) => {
							asm.callr_v3(Addr.!(val));
							target = true;
							break;
						}
						Use(vreg, assignment) => {
							asm.icall_r(loc_r(assignment));
							target = true;
							break;
						}
						RefMap(lp, o) => {
							livepoint = lp;
							outgoing = o;
						}
						_ => ;
					}
				}
				if (!target) context.fail("no target for call");
				if (livepoint >= 0 && mach.runtime.gc != null) {
					var off = asm.mw.offset();
					var entry = buildStackMap(off, outgoing, livepoint);
					if (entry >= 0) mach.runtime.gc.recordStackRefMap(off, getSource(a), entry);
				}
				return recordReturnSource(a);
			}
			I_CALLER_IP => {
				asm.movq_r_m(toGpr(a[0]), X86_64Regs.RSP.plus(frameAdjust()));
			}
			I_CALLER_SP => {
				asm.lea(toGpr(a[0]), X86_64Regs.RSP.plus(frame.size()));
			}
			I_TRUNCS_U64,
			I_TRUNCD_U64 => {
				// XXX: move this MachLowering
				var is64 = (opcode & 0xFF) == I_TRUNCD_U64;
				var it = Int.getType(false, 63);
				var xmm = toXmmr(a[1]), gpr = toGpr(a[0]);
				var max: X86_64Addr, two: X86_64Addr;
				if (is64) {
					max = toLongAddr(Float64Val.!(Float.FLOAT64.maxplus1(it)).bits);
					two = toLongAddr(Float.F64_TWO.bits);
				} else {
					max = toLongAddr(Float32Val.!(Float.FLOAT32.maxplus1(it)).bits);
					two = toLongAddr(Float.F32_TWO.bits);
				}
				var inrange = asm.newLabel(), done = asm.newLabel();
				if (is64) asm.ucomisd_s_m(xmm, max);
				else asm.ucomiss_s_m(xmm, max);
				asm.jc_rel_near(X86_64Conds.C, inrange);
				// > max/2 => divide, convert, mul
				if (is64) asm.divsd_s_m(xmm, two);
				else asm.divss_s_m(xmm, two);
				if (is64) asm.cvtsd2si_r_s(gpr, xmm);
				else asm.cvtss2si_r_s(gpr, xmm);
				asm.shl_r_i(gpr, 1); // lower order bit is 0 because 53 bit precision
				asm.jmp_rel_near(done);
				// in range => convert
				asm.bind(inrange);
				if (is64) asm.cvtsd2si_r_s(gpr, xmm);
				else asm.cvtss2si_r_s(gpr, xmm);
				asm.bind(done);
			}
			I_SYSCALL => {
				asm.syscall();
			}
			_ => return invalidOpcode(opcode);
		}
	}
	def recordReturnSource(a: Array<Operand>) {
		if (rtsrc == null) return;
		match (a[a.length - 1]) {
			ExSource(ex, src) => rtsrc.recordReturnSource(asm.mw.offset(), src);
			_ => ;
		}
	}
	def recordExSource(a: Array<Operand>) {
		if (rtsrc == null) return;
		if (a.length == 0) return;
		match (a[a.length - 1]) {
			ExSource(ex, src) => if (ex != null) rtsrc.recordSource(asm.mw.offset(), src);
			_ => ;
		}
	}
	def getSource(a: Array<Operand>) -> Source {
		if (a.length == 0) return null;
		match (a[a.length - 1]) {
			ExSource(ex, src) => return src;
			_ => return null;
		}
	}
	def toLongAddr(bits: u64) -> X86_64Addr {
		return X86_64AddrRef.new(null, null, 1, mach.getLongConstAddr(long.view(bits)), false);
	}
	def toB32(val: Val) -> int {
		var addr: Addr, b: int;
		match (val) {
			x: Box<int> 		=> b = x.val;
			x: Box<long> 		=> b = int.view(x.val);
			x: Addr 		=> b = ABS_MARKER;
			x: Box<bool> 		=> b = if(x.val, 1);
			x: Float32Val 		=> b = int.view(x.bits);
			x: Float64Val 		=> b = int.view(x.bits);
			x: ArrayRangeStart 	=> b = x.start;
			null 			=> b = 0;
			_ => ;
		}
		return b;
	}
	def recordAddr(val: Val) {
		match (val) {
			x: Addr => asm.recordAbs32(w.pos - 4, X86_64AddrRef.new(null, null, 0, x, false));
			x: Record => asm.recordAbs32(w.pos - 4, X86_64AddrRef.new(null, null, 0, mach.addrOfRecord(x), false));
		}
	}
	def invalidOpcode(opcode: int) {
		var cond = byte.view(0xF & (opcode >> COND_SHIFT));
		var am = byte.view(0x1F & (opcode >> AM_SHIFT));
		var code = byte.view(opcode);
		context.fail(Strings.format3("invalid opcode cond=%x am=%x opcode=%x", cond, am, code));
	}
	def toMrrsd(a: Array<Operand>, start: int) -> X86_64Addr {
		var base: X86_64Gpr, index: X86_64Gpr, b = a[start + 0], i = a[start + 1];
		if (Operand.Use.?(b)) base = toGpr(b);
		if (Operand.Use.?(i)) index = toGpr(i);
		var scale = byte.view(toInt(a[start + 2]));
		var val = toImm(a[start + 3]);
		match (val) {
			x: Addr => return X86_64AddrRef.new(base, index, scale, x, false);
			x: Record => return X86_64AddrRef.new(base, index, scale, mach.addrOfRecord(x), false);
			_ => return X86_64Addr.new(base, index, scale, toB32(val));
		}
	}

	def frameAdjust() -> int {
		// assumes return address already pushed
		return frame.size() - mach.code.addressSize;
	}
	def toGpr(o: Operand) -> X86_64Gpr {
		return loc_r(toLoc(o));
	}
	def toXmmr(o: Operand) -> X86_64Xmmr {
		return loc_s(toLoc(o));
	}
	def toLoc(o: Operand) -> int {
		match (o) {
			Overwrite(dst, src, assignment) => return assignment;
			Def(vreg, assignment) => return assignment;
			Use(vreg, assignment) => return assignment;
			_ => return V3.fail("expected operand with assignment");
		}
	}
	def getLoc(o: Operand) -> (X86_64Reg, int) {
		var loc = toLoc(o);
		var reg: X86_64Reg = Regs.toGpr(loc);
		reg = if(reg != null, reg, Regs.toXmmr(loc));
		var disp = if (reg == null, loc_m(loc).disp);
		return (reg, disp);
	}

	def encodeCond(cond: X86_64Cond) -> int {
		return cond.index << COND_SHIFT;
	}
	def decodeCond(opcode: int) -> X86_64Cond {
		return Conds.all[(opcode >> COND_SHIFT) & 0xF];
	}
	def decodeRounding(opcode: int) -> X86_64Rounding {
		match ((opcode >> ROUNDING_SHIFT) & 0x3) {
			X86_64Rounding.TO_NEAREST.tag => return X86_64Rounding.TO_NEAREST;
			X86_64Rounding.TO_POS_INF.tag => return X86_64Rounding.TO_POS_INF;
			X86_64Rounding.TO_NEG_INF.tag => return X86_64Rounding.TO_NEG_INF;
			X86_64Rounding.TO_ZERO.tag => return X86_64Rounding.TO_ZERO;
		}
		return X86_64Rounding.TO_NEAREST;
	}

	def loc_r(loc: int) -> X86_64Gpr {
		var gpr = Regs.toGpr(loc);
		if (gpr == null) return V3.fail1("expected GPR, got %s", regSet.identify(loc));
		return gpr;
	}
	def loc_s(loc: int) -> X86_64Xmmr {
		var xmm = Regs.toXmmr(loc);
		if (xmm == null) return V3.fail1("expected XMM, got %s", regSet.identify(loc));
		return xmm;
	}
	def loc_m(loc: int) -> X86_64Addr {
		loc = frame.un64(loc);
		var wordSize = mach.data.addressSize, offset = 0;
		if (loc >= regSet.calleeStart) {
			offset = wordSize * (loc - regSet.calleeStart);
		} else if (loc >= regSet.callerStart) {
			offset = frame.size() + (wordSize * (loc - regSet.callerStart));
		} else if (loc >= regSet.spillStart) {
			offset = wordSize * (loc - regSet.spillStart + frame.spillArgs);
		} else {
			return V3.fail1("invalid spill location %s", regSet.identify(loc));
		}
		return X86_64Regs.RSP.plus(offset);
	}
	def getOutput() -> ArchInstrBuffer {
		if (out != null) return out;
		return out = X86InstrBuffer.new(this, context.prog, regSet);
	}
}
class X86InstrBuffer extends ArchInstrBuffer {
	def x86codegen: SsaX86_64Gen;
	new(x86codegen, prog: Program, regSet: MachRegSet) super(x86codegen, prog, regSet) { }
	def putArchInstr(indent: int, i: ArchInstr) -> int {
		var opcode = int.view(i.opcode()), a = i.operands;
		var name: string, cond: X86_64Cond, rounding = false;

		match (opcode & 0xFF) {
			I_SWITCHD, I_SWITCHQ => name = ".switch";
			I_ADDD => name = "addd";
			I_ORD =>  name = "ord";
			I_ADCD => name = "adcd";
			I_ANDD => name = "andd";
			I_SUBD => name = "subd";
			I_XORD => name = "xord";
			I_CMPD => name = "cmpd";
			I_CMPB => name = "cmpb";
			I_MULD => name = "imuld";
			I_DIVD => name = "divd";
			I_IDIVD => name = "idivd";
			I_NEGD => name = "negd";
			I_NOTD => name = "notd";
			I_TESTD => name = "testd";
			I_CDQ => name = "cdq";
			I_ADDQ => name = "addq";
			I_ORQ =>  name = "orq";
			I_ADCQ => name = "adcq";
			I_ANDQ => name = "andq";
			I_SUBQ => name = "subq";
			I_XORQ => name = "xorq";
			I_CMPQ => name = "cmpq";
			I_MULQ => name = "imulq";
			I_DIVQ => name = "divq";
			I_IDIVQ => name = "idivq";
			I_NEGQ => name = "negq";
			I_NOTQ => name = "notq";
			I_LEAQ => name = "leaq";
			I_TESTQ => name = "testq";
			I_CQO => name = "cqo";
			I_MOVD => name = "movd";
			I_MOVQ => name = "movq";
			I_MOVSS => name = "movss";
			I_MOVSD => name = "movsd";
			I_CVTSS2SD => name = "cvtss2sd";
			I_CVTSD2SS => name = "cvtsd2ss";
			I_CVTSS2SID, I_CVTSS2SIQ => name = "cvtss2si";
			I_CVTSD2SID, I_CVTSD2SIQ => name = "cvtsd2si";
			I_CVTSI2SSD, I_CVTSI2SSQ => name = "cvtsi2ss";
			I_CVTSI2SDD, I_CVTSI2SDQ => name = "cvtsi2sd";
			I_PCMPEQD => name = "pcmpeqd";
			I_PCMPEQQ => name = "pcmpeqq";
			I_UCOMISS => name = "ucomiss";
			I_UCOMISD => name = "ucomisd";
			I_PSLLD => name = "pslld";
			I_PSRLD => name = "psrld";
			I_PSLLQ => name = "psllq";
			I_PSRLQ => name = "psrlq";
			I_ROUNDSS => {
				name = "roundss";
				rounding = true;
			}
			I_ROUNDSD => {
				name = "roundsd";
				rounding = true;
			}
			I_MOVB => name = "movb";
			I_MOVBSX => name = "movbsx";
			I_MOVBZX => name = "movbzx";
			I_MOVW => name = "movw";
			I_MOVWSX => name = "movwsx";
			I_MOVWZX => name = "movwzx";
			I_CMPXCHG8 => name = "cmpxchgb";
			I_CMPXCHG16 => name = "cmpxchgw";
			I_CMPXCHG32 => name = "cmpxchgd";
			I_CMPXCHG64 => name = "cmpxchgq";
			I_INCQ => name = "incq";
			I_DECQ => name = "decq";
			I_SHLQ => name = "shlq";
			I_SARQ => name = "sarq";
			I_SHRQ => name = "shrq";
			I_SHLD => name = "shld";
			I_SARD => name = "sard";
			I_SHRD => name = "shrd";
			I_CALL => name = "call";
			I_ADDSS => name = "addss";
			I_SUBSS => name = "subss";
			I_MULSS => name = "mulss";
			I_DIVSS => name = "divss";
			I_SQRTSS => name = "sqrtss";
			I_ADDSD => name = "addsd";
			I_SUBSD => name = "subsd";
			I_MULSD => name = "mulsd";
			I_DIVSD => name = "divsd";
			I_SQRTSD => name = "sqrtsd";
			I_CALLER_IP => name = ".caller_ip";
			I_CALLER_SP => name = ".caller_sp";
			I_TRUNCS_U64 => name = ".truncs_u64";
			I_TRUNCD_U64 => name = ".truncd_u64";
			I_JMP => name = "j";
			I_SETC => {
				name = "set";
				cond = x86codegen.decodeCond(opcode);
			}
			I_JC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			ArchInstrs.ARCH_RET => {
				putIndent(indent);
				puts(name);
				sp();
				if (codegen.frame.frameSize < 0) puts("?");
				else putd(x86codegen.frameAdjust());
				sp();
				putOperands(a);
				return indent;
			}
			I_THROW => {
				name = "j";
			}
			I_THROWC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			I_TEST_ALLOC => {
				putIndent(indent);
				puts("xaddq [").green().puts("CiRuntime.heapCurLoc").end().puts("], ");
				putOperand(a[0]);
				putOperands2(a, 1);
				return indent;
			}
			_ => {
				return putSimpleInstr(indent, i);
			}
		}
		match ((opcode >> AM_SHIFT & 0x1F)) {
			AM_MRRSD_XMM,
			AM_MRRSD_REG,
			AM_MRRSD_IMM => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				var offset = renderMrrsd(a, 0);
				csp();
				putOperand(a[offset]);
				putOperands2(a, offset + 1);
			}
			AM_XMM_MRRSD,
			AM_REG_MRRSD => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperand(a[0]);
				csp();
				var offset = renderMrrsd(a, 1);
				putOperands2(a, offset);
			}
			_ => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperands(a);
			}
		}
		if (rounding) {
			var r = x86codegen.decodeRounding(opcode);
			sp();
			puts(r.name);
		}
		return indent;
	}
	def putOperands2(a: Array<Operand>, start: int) {
		for (i = start; i < a.length; i++) {
			csp();
			putOperand(a[i]);
		}
	}
	def renderMrrsd(a: Array<Operand>, start: int) -> int {
		var x = a[start], y = a[start + 1], scale = codegen.toInt(a[start + 2]), disp = codegen.toImm(a[start + 3]);
		putc('[');
		var prev = false;
		if (Operand.Use.?(x)) {
			putOperand(x);
			prev = true;
		}
		if (Operand.Use.?(y)) {
			if (prev) puts(" + ");
			putOperand(y);
			prev = true;
		}
		if (scale > 1) {
			puts(" * ");
			putd(scale);
		}
		if (disp != null) {
			if (prev) puts(" + ");
			green().putcv(disp, null).end();
		}
		putc(']');
		return start + 4;
	}
}
// Result of matching and emitting a compare: one or two conditions.
class X86_64CondPair(cond1: X86_64Cond, and: bool, cond2: X86_64Cond) {
	def negate() -> X86_64CondPair {
		if (cond2 != null) return X86_64CondPair.new(cond1.negate, !and, cond2.negate);
		return X86_64CondPair.new(cond1.negate, false, null);
	}
}
