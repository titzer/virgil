// Copyright 2021 Ben L. Titzer. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def Regs: X86_64RegSet;

component X86_64Common {
	def LOWERING = makeMachLoweringConfig();
	def makeMachLoweringConfig() -> MachLoweringConfig {
		var config = MachLoweringConfig.new();
		config.Int32Arith = true;
		config.Int64Arith = true;
		config.ExplicitDivChecks = true;
		config.ExplicitModChecks = true;
		config.IntConvertFUnsigned = false; // no unsigned conversions until AVX?
		config.IntConvertFMapsNanToZero = false; // cvts{s,d}2si maps NaN to int.min
		config.IntConvertFPosSaturates = false; // cvts{s,d}2si returns int.min
		config.FloatConvertIUnsigned = false; // cvts{s,d}2si returns int.min
		return config;
	}
	def KERNEL_PARAM_REGS = [
		X86_64RegSet.RAX,
		X86_64RegSet.RDI,
		X86_64RegSet.RSI,
		X86_64RegSet.RDX,
		X86_64RegSet.R10,
		X86_64RegSet.R8,
		X86_64RegSet.R9
	];
	def KERNEL_RETURN_REGS = [
		X86_64RegSet.RAX,
		X86_64RegSet.RDX // TODO
	];
}

class X86_64Backend extends MachBackend {
	var asm: X86_64MacroAssembler;
	def test: bool;
	var codegen: SsaX86_64Gen;
	var allocateRegs: void -> void;
	var allocateRegsGlobal: void -> void;

	// memory allocator configuration
	var objReg: X86_64Gpr;
	var sizeReg: X86_64Gpr;
	var ipReg: X86_64Gpr;
	var spReg: X86_64Gpr;
	var allocStubAddr: Addr;

	new(compiler: Compiler, prog: Program, mach: MachProgram, w: MachDataWriter, dwarf: Dwarf, test)
		super(compiler, prog, mach, w) {

		asm = X86_64MacroAssembler.new(w);
		if (ri_gc != null) {
			// call the RiRuntime.gc() method
			var frame = computeFrameSize(getFrame(ri_gc.ssa)), c = frame.conv;
			objReg = Regs.toGpr(c.calleeRet(0));
			sizeReg = Regs.toGpr(c.calleeParam(1));
			ipReg = Regs.toGpr(c.calleeParam(2));
			spReg = Regs.toGpr(c.calleeParam(3));
		} else {
			// there is no appropriate RiRuntime.gc() method
			objReg = X86_64Regs.RAX;
			sizeReg = X86_64Regs.RAX;
		}
		codegen = SsaX86_64Gen.new(context, mach, asm, w, dwarf);
		if (compiler.useGlobalRegAllocMatcher != VstMatcher.None) allocateRegsGlobal = GlobalRegAlloc.new(X86_64RegSet.SET, codegen).allocate;
		if (compiler.LocalRegAlloc) allocateRegs = LocalRegAlloc.new(X86_64RegSet.SET, codegen).allocate;
		else allocateRegs = SimpleRegAlloc.new(X86_64RegSet.SET, codegen).allocate;
	}
	def genEntryStub() {
		var main = prog.getMain().asMethod();
		var frame = computeFrameSize(getFrame(main.ssa));
		// initialize runtime if necessary
		genMainInit(frame);
		if (test) genTestInputs(main, frame);
		// switch to compiler-provided stack before initializing the runtime
		if (CLOptions.STACK_SIZE.get() > 0) {
			var stackEnd = ref(CiRuntimeModule.STACK_END);
			asm.q.lea(X86_64Regs.RSP, stackEnd);
		}
		// call main
		asm.callr_v3(mach.addrOfMethod(main));
		// write return value to stdout if this is a test
		if (test) genTestOutput(main, frame);
		// exit with the return value of main
		if (main.sig.returnTypes.length > 0) asm_exit_r(loc_gpr(frame, frame.conv.callerRet(0)));
		else return asm_exit_code(0);
	}
	def genAllocStub() {
		if (alwaysGc) return jumpRiGc(); // just call the GC directly

		// %sizeReg = alloc(%sizeReg = size)
		var scratchReg = Regs.toGpr(Regs.SCRATCH_GPR);
		asm.movq_r_r(scratchReg, sizeReg);
		// add size = size + [heapCurLoc]
		var heapCurLocAddr = ref(CiRuntimeModule.HEAP_CUR_LOC);
		asm.add_r_m(sizeReg, heapCurLocAddr);
		// check for addition overflow
		var callrt = X86_64Label.new();
		asm.jc_rel_near(X86_64Conds.C, callrt);
		// compare with [heapEndLoc]
		var heapEndLocAddr = ref(CiRuntimeModule.HEAP_END_LOC);
		asm.cmp_r_m(sizeReg, heapEndLocAddr);

		if (ri_gc == null) {
			// branch to the fatal address
			var fatal = ref(mach.runtime.getFatalAddress(V3Exception.HeapOverflow));
			asm.jc_rel_addr(X86_64Conds.A, fatal);
		} else {
			// branch forward to the slow path
			asm.jc_rel_near(X86_64Conds.A, callrt);
		}

		asm.movq_m_r(heapCurLocAddr, sizeReg);
		asm.sub_r_r(sizeReg, scratchReg);
		if (sizeReg != objReg) asm.movq_r_r(objReg, sizeReg);
		asm.ret();

		if (ri_gc == null) return;
		asm.bind(callrt);
		// slow path: call RiRuntime.gc
		// callerParam(0) = this
		// callerParam(1) = size
		asm.movq_r_r(sizeReg, scratchReg); // reload saved size
		jumpRiGc(); // load other args and tail call runtime
	}
	def jumpRiGc() {
		// callerParam(2) = ip
		asm.movq_r_m(ipReg, X86_64Regs.RSP.indirect());
		// callerParam(3) = sp
		asm.lea(spReg, X86_64Addr.new(null, X86_64Regs.RSP, 1, mach.data.addressSize));
		var addr = X86_64AddrRef.new(null, null, 1, mach.addrOfMethod(ri_gc), true);
		asm.jmp_rel_addr(addr);
		// no need to return, this is a tail call
	}
	def genCodeFromSsa() {
		var frame = getFrame(context.method.ssa);
		var rtsrc = mach.runtime.src;
		if (rtsrc != null) rtsrc.curFrame = frame;
		codegen.generate(context.method, frame);
		if (context.shouldUseGlobalRegAlloc()) allocateRegsGlobal();
		else allocateRegs();
		computeFrameSize(frame);
		if (rtsrc != null) rtsrc.recordMethodStart(w.endOffset(), context.method.source, frame);
		codegen.assembleInstrs();
		asm.patcherImpl.patchLabels();
		if (rtsrc != null) rtsrc.recordFrameEnd(w.endOffset());
	}
	def genTestInputs(main: IrMethod, frame: MachFrame) {
		// "argc" is on the top of the stack on x86-64-linux
		asm.movq_r_m(X86_64Regs.RBX, X86_64Regs.RSP.indirect()); // load "argc"
		var params = main.ssa.params, conv = frame.conv;
		if (conv.overflow > 0) {
			// allocate space for overflow arguments
			asm.sub_r_i(X86_64Regs.RSP, conv.overflow * mach.data.addressSize);
		}

		var vals = mach.runtime.parseTestInputs(prog, prog.ERROR, params.length - 1);
		var w = MachDataWriter.!(asm.w);

		for (i < conv.paramLocs.length) {
			var loc = conv.callerParam(i), dest = Regs.toGpr(loc);
			if (i == 0) {
				// load the component "this" pointer with NULL
				asm.movq_r_i(dest, 0);
				continue;
			}
			// load the register value from the table to follow
			var tableAddr = Addr.new(mach.codeRegion, null, 0);
			var taRef = X86_64AddrRef.new(null, X86_64Regs.RBX, 4, tableAddr, false);
			if (dest == null) {
				// destination is a stack location
				var scratchReg = Regs.toGpr(Regs.SCRATCH_GPR);
				var offset = mach.data.addressSize * (loc - frame.conv.regSet.callerStart);
				asm.movd_r_m(scratchReg, taRef);
				asm.movq_m_r(X86_64Regs.RSP.plus(offset), scratchReg);
			} else {
				asm.movd_r_m(dest, taRef);
			}

			var endAddr = Addr.new(mach.codeRegion, null, 0);
			var refEnd = X86_64AddrRef.new(null, null, 1, endAddr, true);
			asm.jmp_rel_addr(refEnd); // jump over the table
			tableAddr.absolute = w.posAddr() - 4; // argc = 1 adjustment
			// emit table of argument values
			for (v in vals) w.put_b32(V3.unboxI32(v[i - 1]));
			endAddr.absolute = w.endAddr();
		}
	}
	def genSigInstalls() {
		genSigHandlerInstall(8, mach.runtime.getFatalAddress(V3Exception.DivideByZero));
		genSigHandlerInstall(10, mach.runtime.getFatalAddress(V3Exception.NullCheck));
		genSigHandlerInstall(11, mach.runtime.getFatalAddress(V3Exception.NullCheck));
	}
	def genMainInit(frame: MachFrame) {
		// call RiRuntime.init() if it exists
		if (mach.runtime.ri_init >= 0) return genRiInit(frame);
		// if this is a test, install custom signal handlers
		if (test) return genSigInstalls();
		// TODO: remove compiler-generated initialization of args
		if (frame.conv.paramTypes.length <= 1) return; // don't bother, main doesn't use it
		// initialize arg array from OS-supplied argv and envp
		var argc = X86_64Regs.RBX, argp = X86_64Regs.RBX, argArray = X86_64Regs.RDX;
		var arrayType = V3Array.newType(mach.machType(V3.stringType));

		asm.movq_r_m(argc, X86_64Regs.RSP.indirect());
		asm.sub_r_i(argc, 1); // adjust for first arg
		asmArrayAlloc(arrayType, argArray, argc);
		asm.pushq_r(argArray); // save array on stack

		asm.lea(argArray, argArray.plus(mach.getArrayElemOffset(arrayType, 0)));
		asm.lea(argp, X86_64Regs.RSP.plus(3 * mach.data.addressSize));

		// loop over individual arguments
		var loopStart = asm.pos();
		var str = X86_64Regs.RDI, strlen = X86_64Regs.RCX;

		// load the argument and test against null
		asm.movq_r_m(str, argp.indirect());
		asm.cmp_r_i(str, 0);
		asm.jc_rel(X86_64Conds.Z, 0);
		var cmpPatch = asm.pos() - 1;

		// compute length of string by finding null byte
		asm.movq_r_i(strlen, -1);
		asm.repne().scasb(); // updates RDI and RCX, kills RAX
		asm.not_r(strlen);
		asm.dec_r(strlen);

		// allocate a byte array of length strlen
		var argString = X86_64Regs.RDI;
		asmArrayAlloc(V3.stringType, argString, strlen);
		asm.movq_m_r(argArray.indirect(), argString); // write into arg array

		// copy data into array
		asm.lea(X86_64Regs.RDI, argString.plus(mach.getArrayElemOffset(V3.stringType, 0)));
		asm.movq_r_m(X86_64Regs.RSI, argp.indirect());
		asm.repne().movsb();

		// increment argp and argarray position
		asm.add_r_i(argArray, 4);
		asm.add_r_i(argp, 4);
		var offset = loopStart - asm.pos();
		asm.jmp_rel(offset); // loop back to start

		var offset2 = asm.pos() - cmpPatch - 1;
		asm.w.at(cmpPatch).putb(offset2); // patch the branch that skips the loop
		asm.w.atEnd();
		asm.popq_r(loc_gpr(frame, frame.conv.callerParam(1)));
	}
	def genRiInit(frame: MachFrame) {
		// generate a call to the RiRuntime.init() method
		var init_meth = mach.runtime.getRiInit();
		var frame = getFrame(init_meth.ssa), conv = frame.conv;
		// arg 0 = "this" = null
		var thisReg = Regs.toGpr(conv.calleeParam(0));
		asm.movq_r_i(thisReg, 0); // "this" = null
		// arg 1 = argc @ [esp]
		var argcReg = Regs.toGpr(conv.calleeParam(1));
		asm.movq_r_m(argcReg, X86_64Regs.RSP.indirect());
		var argvReg = Regs.toGpr(conv.calleeParam(2));
		if (test) {
			// if testing, arg 2 = argv = NULL
			asm.movq_r_i(argvReg, 0);
		} else {
			// otherwise arg 2 = argv @ [rsp + 8]
			asm.lea(argvReg, X86_64Regs.RSP.plus(mach.data.addressSize));
		}
		// arg 3 = envp @ [esp + 8] = null
		var envpReg = Regs.toGpr(conv.calleeParam(3));
		asm.movq_r_i(envpReg, 0);
		// call RiRuntime.init(argc: int, argv: Pointer, envp: Pointer) -> Array<string>
		asm.callr_v3(mach.addrOfMethod(init_meth));
		// ret -> dest
		if (!test) {
			var retReg = Regs.toGpr(conv.calleeRet(0));
			var dest = Regs.toGpr(conv.callerParam(1));
			asm.movd_r_r(dest, retReg);
		}
	}
	def asmArrayAlloc(arrayType: Type, dest: X86_64Gpr, len: X86_64Gpr) {
		var scale = mach.getArrayElemScale(arrayType), align = (scale % mach.data.addressSize) != 0;
		var adjust = if(align, mach.data.addressSize - 1, 0);
		asm.lea(dest, X86_64Addr.new(null, len, byte.!(scale), mach.getArrayElemOffset(arrayType, 0) + adjust));
		if (align) asm.and_r_i(dest, -1 ^ adjust);
		asmAlloc(dest);
		asm.movq_m_i(dest.indirect(), Int.unbox(mach.objectTag(arrayType)));
		asm.movq_m_r(dest.plus(mach.getArrayLengthOffset(arrayType)), len);
	}
	def asmAlloc(reg: X86_64Gpr) {
		// exchange-add with heap current pointer
		var addr = X86_64AddrRef.new(null, null, 1, CiRuntimeModule.HEAP_CUR_LOC, false);
		asm.xadd_m_r(addr, reg);
	}
	def getFrame(ssa: SsaGraph) -> MachFrame {
		return MachFrame.new(X86_64VirgilCallConv.getForGraph(mach, ssa), mach.data.addrAlign, mach.refSize);
	}
	def computeFrameSize(frame: MachFrame) -> MachFrame {
		frame.frameSize = mach.alignTo(frame.slots() * mach.refSize + mach.code.addressSize, mach.stackAlign);
		return frame;
	}
	def patchCodeAddr(w: DataWriter, a: Addr, posAddr: int) {
		var abs = mach.absolute(a);
		if (CLOptions.PRINT_PATCH.val) {
			TerminalBuffer.new()
				.puts("patch-code @0x")
				.putx(posAddr)
				.puts(" <- ")
				.putcv(a, null)
				.puts(" = 0x")
				.putx(abs)
				.outln();
		}
		asm.patcherImpl.patch(a, posAddr, abs);
	}
	def loc_gpr(frame: MachFrame, loc: int) -> X86_64Gpr {
		var r = Regs.toGpr(loc);
		if (r == null) return V3.fail(Strings.format1("expected GPR, but got %s", frame.conv.regSet.identify(loc)));
		return r;
	}
	def ref(addr: Addr) -> X86_64AddrRef {
		return X86_64AddrRef.new(null, null, 1, addr, false);
	}
	// Methods overridden for each OS target
	def genSigHandlerInstall(signo: int, handler: Addr);
	def asm_exit_r(r: X86_64Gpr);
	def asm_exit_code(code: int);
	def genTestOutput(main: IrMethod, frame: MachFrame);
}
