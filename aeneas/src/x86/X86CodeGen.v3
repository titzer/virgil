// Copyright 2011 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Utilities for X86 addresses
component X86Addrs {
	// absolute patch constant
	def ABS_CONST  = 0x44332211;
	def ABS_CONST0 = 0x11;
	def ABS_CONST1 = 0x22;
	def ABS_CONST2 = 0x33;
	def ABS_CONST3 = 0x44;

	// relative patch constant
	def REL_CONST = 0x44332205;

	def ABS_PATCH = X86Addr.new(null, null, 1, 0x44332211);
	def ABS_PATCH_SSE = ABS_PATCH.toSSEAddr();
}
// Pattern match of a comparison between two vars or a var and a constant
class X86CmpMatch {
	def cond: X86Cond;
	def x: SsaInstr;
	def y: SsaInstr;
	def val: Val;
	new(cond, x, y, val) { }
	def negate() -> X86CmpMatch { return X86CmpMatch.new(cond.negate, x, y, val); }
}
// Represents an X86 instruction for register allocation and assembling
class X86Instr<T> extends MachInstr {
	def emitFunc: T -> void;
	def params: T;
	new(name: string, emitFunc, params) super(name) { }
	def emit() { emitFunc(params); }
}
def SHR_shifter = X86Shifter.new(Opcode.IntShr);
def SAR_shifter = X86Shifter.new(Opcode.IntSar);
def SHL_shifter = X86Shifter.new(Opcode.IntShl);

def Regs: X86RegSet;

// Parameter registers for x86-linux
def X86_LINUX_KERNEL_PARAM_REGS = [X86RegSet.EBX, X86RegSet.ECX, X86RegSet.EDX, X86RegSet.ESI, X86RegSet.EDI];

// Generates X86 code from SSA-M32 form.
// XXX: emit short branch instructions where possible
// XXX: align loop headers to cache boundaries
// XXX: schedule loads of constants for calls, phis, etc
// XXX: remove use of scratch register
class X86CodeGen extends OldCodeGen {
	def scratchReg = Regs.SCRATCH;
	var branches: List<(int, SsaBlock)>; // XXX: pull branches up to MachCodeGen?
	var jumpTables: List<(int, Array<SsaCfEdge>)>;
	var asm: X86MacroAssembler;
	var kernel: Kernel;

	new(mach: MachProgram, context: SsaContext) super(mach, context) {
		frame = MachFrame.new(X86VirgilCallConv.getForGraph(mach, context.graph), mach.data.addrAlign, mach.refSize);
	}
	def genCode(asm: X86MacroAssembler) {
		this.asm = asm;
		if (CLOptions.PRINT_CFG.get()) {
			var buf = TerminalBuffer.new();
			context.method.renderLong(buf);
			buf.puts(": ").putd(blocks.order.length);
			buf.outln();
			blocks.print();
		}
		gen_entry();
		var blocks = blocks.order;
		for (i < blocks.length) {
			// emit all blocks in order
			var info = blocks[i];
			info.start = code.length;
			genBlock(info.block);
			info.end = code.length;
		}
		markAllLiveVars();
		genPhis();
		lsra = LinearScanRegAlloc.new(this, Regs.regs);
		lsra.assignRegs();
		frame.frameSize = mach.alignTo(frame.slots() * mach.refSize + mach.code.addressSize, mach.stackAlign);
		if (context.shouldPrintMach()) print();
		emitInstrs();
	}
	def genBlock(b: SsaBlock) {
		// generate code for each instruction in a block
		context.block = b;
		curBlockStart = code.length;
		var next = b.next;
		if (SsaPhi.?(next)) next = gen_phi_dfn(SsaPhi.!(next));
		for (i = next; SsaInstr.?(i); i = i.next) {
			var v = makeVar(SsaInstr.!(i));
			v.start = code.length;
			var root = true;
			match (i) {
				x: SsaApplyOp => root = genApply(x, v);
				x: SsaReturn => genReturn(x);
				x: SsaIf => genIf(x);
				x: SsaGoto => genGoto(x.target());
				x: SsaSwitch => genSwitch(x);
				x: SsaThrow => genThrow(x);
				x: SsaNewVar => ; // do nothing
				x: SsaDeleteVar => ; // do nothing
				x: SsaProbe => ; // do nothing
			}
			v.end = code.length;
			if (v.varSize > 0) {
				for (i = v.varNum; i < v.varNum + v.varSize; i++) {
					var ov = vars[i];
					ov.start = v.start;
					ov.end = v.end;
				}
			}
			if (root) markUsesLive(v);
		}
	}
	def emitInstrs() {
		if (rtsrc != null) rtsrc.recordMethodStart(asm.codeOffset(), context.method.source, frame);
		var w = asm.w;
		var offsets = Array<int>.new(code.length);
		// assemble the code for each instruction
		for (i < code.length) {
			var instr = code[i];
			offsets[i] = w.pos;
			if (!instr.live) continue;
			var m = instr.moves;
			if (m != null) { // emit any spills / constants before instruction
				emitMoves(m.before);
				emitVarMoves(m.varMoves);
				emitValMoves(m.valMoves);
			}
			instr.emit();
			if (m != null) { // emit any restores after instruction
				emitMoves(m.after);
			}
		}
		// patch any branch instructions with the target address
		for (l = branches; l != null; l = l.tail) {
			var p = l.head.0;
			var t = offsets[blocks.order[l.head.1.mark].start];
			w.at(p).put_b32(t - p - 4); // encode pc-relative address
		}
		// patch any jump tables
		for (l = jumpTables; l != null; l = l.tail) {
			var p = l.head.0, succ = l.head.1;
			w.at(p);
			for (i < succ.length) {
				var t = offsets[blocks.order[succ[i].dest.mark].start];
				w.put_b32(t + asm.machBuffer.startAddr); // encode absolute address
			}
		}
		w.atEnd();
		if (rtsrc != null) rtsrc.recordFrameEnd(asm.codeOffset());
	}
	def genApply(i: SsaApplyOp, v: VReg) -> bool {
		var root = false;
		match (i.op.opcode) {
			IntAdd =>		genAdd(i, v);
			IntMul =>		genMul(i, v);
			IntDiv =>		root = genDiv(i, v, true);
			IntMod =>		root = genDiv(i, v, false);
			IntAnd,			// fall through
			BoolAnd =>		genOp2("and", i, v, X86Assembler.and);
			IntOr,			// fall through
			BoolOr =>		genOp2("or", i, v, X86Assembler.or);
			IntSub =>		genOp2("sub", i, v, X86Assembler.sub);
			IntXor =>		genOp2("xor", i, v, X86Assembler.xor);
			IntShl =>		genShift("shl", i, v, SHL_shifter);
			IntSar =>		genShift("sar", i, v, SAR_shifter);
			IntShr =>		genShift("shr", i, v, SHR_shifter);
			IntWide(op) =>		root = genWide(i, op, v);
			BoolEq,			// fall through
			IntEq,			// fall through
			RefEq,			// fall through
			IntLt,			// fall through
			IntLteq,		// fall through
			PtrLt,			// fall through
			PtrLteq =>		genCmpSet(matchCmp(i), v);
			IntViewI =>		genIntViewI(i, v);
			BoolNot =>		gen_op2_i("xor", X86Assembler.xor, v, i.input0(), Int.box(1));
			IntCastF(isDouble) => 	genIntCastF(i, v, isDouble);
			IntViewF =>		gen_sse_view_int("int_viewf", X86Assembler.movd_rm_s, v, i.input0());
			IntTruncF(isDouble) => 	genIntTruncF(i, v, isDouble);
			FloatPromoteI(isDouble) => genFloatRoundI(i, v, isDouble);
			FloatPromoteF => 	gen_sse_cvt("float_promote", X86Assembler.cvtss2sd, v, i.input0());
			FloatViewI(isDouble) => {
				if (isDouble) gen_sse_view_double("movq", X86Assembler.movq_s_sm, v, i.input0(), i.input1());
				else gen_sse_view_float("movd", X86Assembler.movd_s_rm, v, i.input0());
			}
			FloatRoundI(isDouble) => genFloatRoundI(i, v, isDouble);
			FloatRound(isDouble) => {
				if (isDouble) gen_sse_round("roundsd", X86Assembler.roundsd, v, i.input0(), RoundingMode.TO_NEAREST);
				else gen_sse_round("roundss", X86Assembler.roundss, v, i.input0(), RoundingMode.TO_NEAREST);
			}
			FloatRoundD => 		gen_sse_cvt("cvt_d2f", X86Assembler.cvtsd2ss, v, i.input0());
			FloatAdd(isDouble) => {
				if (isDouble) gen_sse_op2("addsd", X86Assembler.addsd, v, i.input0(), i.input1());
				else gen_sse_op2("addss", X86Assembler.addss, v, i.input0(), i.input1());
			}
			FloatSub(isDouble) => {
				if (isDouble) gen_sse_op2("subsd", X86Assembler.subsd, v, i.input0(), i.input1());
				else gen_sse_op2("subss", X86Assembler.subss, v, i.input0(), i.input1());
			}
			FloatMul(isDouble) => {
				if (isDouble) gen_sse_op2("mulsd", X86Assembler.mulsd, v, i.input0(), i.input1());
				else gen_sse_op2("mulss", X86Assembler.mulss, v, i.input0(), i.input1());
			}
			FloatDiv(isDouble) => {
				if (isDouble) gen_sse_op2("divsd", X86Assembler.divsd, v, i.input0(), i.input1());
				else gen_sse_op2("divss", X86Assembler.divss, v, i.input0(), i.input1());
			}
			FloatBitEq(isDouble) =>	gen_sse_bit_eq(v, i.input0(), i.input1(), isDouble);
			FloatEq(isDouble) => 	gen_sse_cmp(X86Conds.Z, v, i.input0(), i.input1(), isDouble);
			FloatNe(isDouble) => 	gen_sse_cmp(X86Conds.NZ, v, i.input0(), i.input1(), isDouble);
			FloatLt(isDouble) => 	gen_sse_cmp(X86Conds.C, v, i.input0(), i.input1(), isDouble);
			FloatLteq(isDouble) => 	gen_sse_cmp(X86Conds.NA, v, i.input0(), i.input1(), isDouble);
			FloatAbs(isDouble) => 	gen_sse_fabs(v, i.input0(), isDouble);
			FloatCeil(isDouble) => {
				if (isDouble) gen_sse_round("roundsd_ceil", X86Assembler.roundsd, v, i.input0(), RoundingMode.TO_POS_INF);
				else gen_sse_round("roundss_ceil", X86Assembler.roundss, v, i.input0(), RoundingMode.TO_POS_INF);
			}
			FloatFloor(isDouble) => {
				if (isDouble) gen_sse_round("roundsd_floor", X86Assembler.roundsd, v, i.input0(), RoundingMode.TO_NEG_INF);
				else gen_sse_round("roundss_floor", X86Assembler.roundss, v, i.input0(), RoundingMode.TO_NEG_INF);
			}
			FloatSqrt(isDouble) => {
				if (isDouble) gen_sse_sqrt("sqrtsd", X86Assembler.sqrtsd, v, i.input0());
				else gen_sse_sqrt("sqrtss", X86Assembler.sqrtss, v, i.input0());
			}
			TupleGetElem(index) => {
				// make tuple element refer to appropriate variable
				i.mark = context.graph.markGen + makeVar(i.input0()).varNum + index;
			}
			ConditionalThrow(exception) =>	root = genCondThrow(i, v, exception);
			PtrLoad =>		root = genLoad(i, v);
			PtrStore =>		root = genStore(i, v);
			PtrCmpSwp =>		root = genCmpSwp(i, v);
			PtrAdd =>		genAdd(i, v);
			PtrSub =>		genOp2("sub", i, v, X86Assembler.sub);
			Alloc => 		root = genAlloc(i, v);
			CallAddress(funcRep) =>		root = genCall(i, v, funcRep);
			CallerIp =>		gen("caller-ip", asm_caller_ip, dfngpr(v));
			CallerSp =>		gen("caller-sp", asm_caller_sp, dfn(v));
			CallKernel(kernel) =>	root = genCallKernel(i, v, kernel);
		} else {
			context.fail1("Unexpected opcode %s", i.op.opcode.name);
		}
		return root;
	}
	def genWide(i: SsaApplyOp, orig: Operator, v: VReg) -> bool {
		match (orig.opcode) {
			IntShl => return genWideShift(i, "wshl", SHL_shifter, v);
			IntSar => return genWideShift(i, "wsar", SAR_shifter, v);
			IntShr => return genWideShift(i, "wshr", SHR_shifter, v);
			IntAdd => return genWideArith(i, "wadd", asm_wadd, v);
			IntSub => return genWideArith(i, "wsub", asm_wsub, v);
			IntMul => return genWideMul(i, v);
			IntDiv => return genWideDiv(i, orig, v, false);
			IntMod => return genWideDiv(i, orig, v, true);
			IntCastF(isDouble) => return genWideIntCastF(i, orig, v, isDouble);
			IntViewF => return genWideIntViewF(i, v);
			IntTruncF(isDouble) => return genWideIntTruncF(i, orig, v, isDouble);
			FloatPromoteI(isDouble) => return genWideFloatRoundI(i, orig, v, isDouble);
			FloatRoundI(isDouble) => return genWideFloatRoundI(i, orig, v, isDouble);
			FloatViewI(isDouble) => {
				gen_sse_view_double("movq", X86Assembler.movq_s_sm, v, i.input0(), i.input1());
				return false;
			}
			_ => ;
		}
		context.fail1("unexpected wide operator: %s", orig.opcode.name);
		return false;
	}
	def genWideMul(i: SsaApplyOp, v: VReg) -> bool {
		// TODO: loosen register requirements for 64-bit multiply
		gen("wmul", asm_wmul, (
			dfnAt(vars[v.varNum], Regs.EAX),
			dfnAt(vars[v.varNum + 1], Regs.EDX),
			kill(Regs.EDI),
			useFixed(i.input0(), Regs.EAX),
			useFixed(i.input1(), Regs.EDX),
			useFixed(i.inputs[2].dest, Regs.ESI),
			useFixed(i.inputs[3].dest, Regs.EDI)));
		return false;
	}
	def genWideDiv(i: SsaApplyOp, orig: Operator, v: VReg, mod: bool) -> bool {
		var stuff = (
			dfnAt(vars[v.varNum], Regs.EAX),
			dfnAt(vars[v.varNum + 1], Regs.EDX),
			kill(Regs.ALL),	// wide division uses all the x86 regs.
			useFixed(i.input0(), Regs.ESI),
			useFixed(i.input1(), Regs.EAX),
			useFixed(i.inputs[2].dest, Regs.EDI),
			useFixed(i.inputs[3].dest, Regs.EDX));
		var upper_divisor = i.inputs[3].dest;
		var signed = IntType.!(orig.sig.returnType()).signed;
		var zeroCheck = !i.facts.O_NO_ZERO_CHECK && rtsrc != null;
		var small_divisor = upper_divisor.facts.V_ZERO;
		var large_divisor = !signed && upper_divisor.facts.V_NON_ZERO;
		var op = WideDivision.new(mod, signed, large_divisor, small_divisor, zeroCheck);
		gen(op.name, asm_wdiv, (op, i.source));
		return !i.facts.O_NO_ZERO_CHECK;
	}
	def genWideArith(i: SsaApplyOp, name: string, asm: (int, int, int, int, int, int) -> (), v: VReg) -> bool {
		// TODO: loosen register requirements for 64-bit add/sub
		gen(name, asm, (
			dfnAt(vars[v.varNum], Regs.EAX),
			dfnAt(vars[v.varNum + 1], Regs.EDX),
			useFixed(i.input0(), Regs.EAX),
			useFixed(i.input1(), Regs.EDX),
			useFixed(i.inputs[2].dest, Regs.ECX),
			useFixed(i.inputs[3].dest, Regs.EBX)));
		return false;
	}
	def genWideShift(i: SsaApplyOp, name: string, shifter: X86Shifter, v: VReg) -> bool {
		// TODO: loosen register requirements for 64-bit shifts
		// TODO: reduce checks required in 64-bit shifts
		gen(name, asm_wsh, (shifter,
			dfnAt(vars[v.varNum], Regs.EAX),
			dfnAt(vars[v.varNum + 1], Regs.EDX),
			useFixed(i.input0(), Regs.EAX),
			useFixed(i.input1(), Regs.EDX),
			useFixed(i.inputs[2].dest, Regs.ECX),
			if (i.inputs.length > 3, useFixed(i.inputs[3].dest, Regs.EBX), -1)));
		return false;
	}
	def genWideIntCastF(i: SsaApplyOp, orig: Operator, v: VReg, isDouble: bool) -> bool {
		var name: string, m: (X86MacroAssembler, X86Rm, X86Rm, SSERm, bool) -> void;
		var signed = IntType.!(orig.sig.returnType()).signed;
		if (isDouble) {
			name = if(signed, "cvt_d2l", "cvt_d2ul");
			m = if(signed, X86MacroAssembler.cvt_fp2l, X86MacroAssembler.cvt_fp2ul);
		} else {
			name = if(signed, "cvt_f2l", "cvt_f2ul");
			m = if(signed, X86MacroAssembler.cvt_fp2l, X86MacroAssembler.cvt_fp2ul);
		}
		gen(name, asm_sse_f2l, (m,
			dfn(vars[v.varNum]),
			dfn(vars[v.varNum + 1]),
			use(i.input0()),
			isDouble));
		return false;
	}
	def genWideIntViewF(i: SsaApplyOp, v: VReg) -> bool {
		var spill = frame.allocSpill(RegClass.I64) & ~(frame.IS_64);
		gen("long_viewd", asm_sse_view_long, (
			X86Assembler.movq_sm_s,
			dfnAt(vars[v.varNum], spill),
			dfnAt(vars[v.varNum + 1], spill + 1),
			use(i.input0())));
		return false;
	}
	def genWideIntTruncF(i: SsaApplyOp, orig: Operator, v: VReg, isDouble: bool) -> bool {
		var name: string, m: (X86MacroAssembler, X86Rm, X86Rm, SSERm, bool) -> void;
		var signed = IntType.!(orig.sig.returnType()).signed;
		if (isDouble) {
			name = if(signed, "long_truncd", "ulong_truncd");
			m = if(signed, X86MacroAssembler.long_truncfp, X86MacroAssembler.ulong_truncfp);
		} else {
			name = if(signed, "long_truncf", "ulong_truncf");
			m = if(signed, X86MacroAssembler.long_truncfp, X86MacroAssembler.ulong_truncfp);
		}
		gen(name, asm_sse_f2l, (m,
			dfn(vars[v.varNum]),
			dfn(vars[v.varNum + 1]),
			use(i.input0()),
			isDouble));
		return false;
	}
	def genWideFloatRoundI(i: SsaApplyOp, orig: Operator, v: VReg, isDouble: bool) -> bool {
		var name: string, m: (X86MacroAssembler, SSEReg, X86Rm, X86Rm, bool) -> void;
		var signed = IntType.!(orig.typeArgs[0]).signed;
		if (isDouble) {
			name = if(signed, "round_l2d", "round_ul2d");
			m = if(signed, X86MacroAssembler.cvt_l2fp, X86MacroAssembler.cvt_ul2fp);
		} else {
			name = if(signed, "round_l2f", "round_ul2f");
			m = if(signed, X86MacroAssembler.cvt_l2fp, X86MacroAssembler.cvt_ul2fp);
		}
		gen(name, asm_sse_l2f, (
			m, dfn(v),
			use(i.input0()),
			use(i.input1()),
			isDouble));
		return false;
	}
	def genIntViewI(i: SsaApplyOp, v: VReg) {
		var tt = IntType.!(i.op.sig.returnType());
		if (tt.width == 32) {
			id(i, makeVar(i.input0()));
		} else if (!tt.signed) {
			var a = i.input0();
			gen("zext", asm_op2_i, (X86Assembler.and, dfn(v), use(a), Int.unbox(tt.max), hint(a, v)));
		} else {
			gen("sext", asm_sext, (dfn(v), use(i.input0()), tt.width));
		}
	}
	def genAdd(i: SsaApplyOp, v: VReg) {
		var xe = i.inputs[0], y = i.input1(), x = xe.dest;
		if (SsaConst.?(y)) {
			// match x + K
			var disp = y.unbox<int>(), xscale = matchScale(xe);
			if (xscale.1 > 1) return gen_lea(v, xscale.0, xscale.1, disp);
			else return gen_add_i(v, x, disp);
		}
		// XXX: match K + b for absolute (mtable) addresses?
		// XXX: match a + (b * K) -> lea [a + b * K]
		// XXX: match a + (b * K) + K -> lea [a + b * K + K]
		gen_op2("add", true, X86Assembler.add, v, x, y);
	}
	def genMul(i: SsaApplyOp, v: VReg) {
		// XXX: strength reduce here also, or just rely on local optimizer?
		var y = i.input1();
		if (SsaConst.?(y)) gen_imul_i(v, i.input0(), y.unbox<int>());
		else gen_imul(v, i.input0(), y);
	}
	def genOp2(name: string, i: SsaApplyOp, v: VReg, m: X86Assembler -> X86Op2) {
		var x = i.input0(), y = i.input1(), optag = i.op.opcode.tag;
		if (SsaConst.?(y)) {
			// X <op> K
			var val = SsaConst.!(y).val;
			if (optag == Opcode.IntXor.tag) {
				if (Int.MINUS_1.equals(val)) return gen_not(v, x);
			}
			return gen_op2_i(name, m, v, x, val);
		}
		if (SsaConst.?(x)) {
			// K <op> Y
			var val = SsaConst.!(x).val;
			if (optag == Opcode.IntSub.tag || optag == Opcode.PtrSub.tag) {
				if (val == null || val.equals(null)) return gen_neg(v, y); // generate negation
			}
			if (optag == Opcode.IntXor.tag) {
				if (Int.MINUS_1.equals(val)) return gen_not(v, y);
			}
			if (i.facts.O_COMMUTATIVE || optag == Opcode.PtrAdd.tag) {
				return gen_op2_i(name, m, v, y, val);
			}
		}
		// gen a basic two-address operation
		var commutative = i.facts.O_COMMUTATIVE || optag == Opcode.PtrAdd.tag;
		gen_op2(name, commutative, m, v, x, y);
	}
	def genCmp(cmp: X86CmpMatch) {
		if (cmp.y == null) {
			match (cmp.val) {
				x: Addr => gen_cmp_a(cmp.x, x);
				x: Record => gen_cmp_a(cmp.x, mach.addrOfRecord(x));
				_ => gen_cmp_i(cmp.x, V3.unboxI32(cmp.val));
			}
		} else {
			gen_cmp(cmp.x, cmp.y);
		}
	}
	def genCmpBr(cmp: X86CmpMatch, target: SsaBlock) {
		genCmp(cmp);
		gen_br(cmp.cond, target);
	}
	def genCmpSet(cmp: X86CmpMatch, v: VReg) {
		genCmp(cmp);
		gen_set(cmp.cond, v);
	}
	def genLoad(i: SsaApplyOp, v: VReg) -> bool {
		var ptr = i.inputs[0];
		var t = i.op.typeArgs[1];
		if (SsaConst.?(ptr.dest)) {
			// an absolute address
			var addr = Addr.!(val(ptr.dest));
			if (mach.isValid(addr)) { // is valid?
				if (FloatType.?(t)) gen_loada_sse(v, t, addr, FloatType.!(t).is64);
				else gen_loada(v, t, addr);
				return false;
			}
			// invalid address, generate a throw
			gen("throw_null", asm_throw, (i.source, V3Exception.NullCheck));
			return true;
		}
		if (matchEdge(ptr, Opcode.PtrAdd.tag)) {
			// load(x + y)
			var add = SsaApplyOp.!(ptr.dest);
			var y = add.input1();
			var x = add.inputs[0];
			if (SsaConst.?(y)) {
				var xscale = matchScale(x);
				return genLoadAddr(i, v, xscale.0, xscale.1, y.unbox<int>());
			}
			if (SsaConst.?(x.dest)) {
				// load(<addr> + y)
				var addr = Addr.!(val(x.dest));
				if (FloatType.?(t)) gen_loada_disp_sse(v, t, addr, y, FloatType.!(t).is64);
				else gen_loada_disp(v, t, addr, y);
				return !i.facts.O_NO_NULL_CHECK;
			}
		}
		return genLoadAddr(i, v, ptr.dest, 1, 0);
	}
	def genLoadAddr(i: SsaApplyOp, v: VReg, x: SsaInstr, scale: byte, disp: int) -> bool {
		var t = i.op.typeArgs[1];
		var nullCheck = V3Op.needsNullCheck(i, x);
		if (FloatType.?(t)) gen_loadx_sse(v, t, x, scale, disp, FloatType.!(t).is64, nullCheck, i.source);
		else gen_loadx(v, t, x, scale, disp, nullCheck, i.source);
		return nullCheck;
	}
	def genStore(i: SsaApplyOp, v: VReg) -> bool {
		var ptr = i.inputs[0], ssaval = i.input1();
		if (SsaConst.?(ptr.dest)) {
			// store to absolute address
			var t = i.op.typeArgs[1];
			var addr = Addr.!(val(ptr.dest));
			if (FloatType.?(t)) gen_storex_abs_sse(addr, ssaval, FloatType.!(t).is64);
			else gen_storex_abs(mach.sizeOf(t), addr, ssaval);
			// XXX: store immediate constants to absolute addresses
			return true;
		}
		if (matchEdge(ptr, Opcode.PtrAdd.tag)) {
			var add = SsaApplyOp.!(ptr.dest);
			var y = add.input1();
			if (SsaConst.?(y)) {
				var xscale = matchScale(add.inputs[0]);
				return genStoreAddr(i, v, xscale.0, xscale.1, y.unbox<int>(), ssaval);
			}
		}
		return genStoreAddr(i, v, ptr.dest, 1, 0, ssaval);
	}
	def genCmpSwp(i: SsaApplyOp, v: VReg) -> bool {
		var size = mach.sizeOf(i.op.typeArgs[1]);
		var ptr = i.inputs[0], val = i.inputs[2].dest;
		if (size == 1 && SsaApplyOp.?(val)) {
			// eliminate int -> byte conversions in cmpswp
			var vop = SsaApplyOp.!(val);
			if (Opcode.IntViewI.?(vop.op.opcode)) {
				var width = IntType.!(vop.op.sig.returnType()).width;
				if (width == 8) val = vop.input0();
			}
		}
		var dest = dfnAt(v, Regs.BYTE);
		var expect = useFixed(i.input1(), Regs.EAX);
		if (SsaConst.?(ptr.dest)) {
			// cmpswp to absolute address
			var addr = Addr.!(this.val(ptr.dest));
			if (size == 4) gen("stored_abs", asm_cmpswpx_abs, (X86Assembler.cmpxchngd, dest, addr, gpr(val)));
			else if (size == 1) gen("storeb_abs", asm_cmpswpx_abs, (X86Assembler.cmpxchngb, dest, addr, byt(val)));
			else if (size == 2) gen("storew_abs", asm_cmpswpx_abs, (X86Assembler.cmpxchngw, dest, addr, gpr(val)));
			else context.fail("invalid size for cmpswp");
			return true;
		}
		var base = ptr.dest;
		var scale: byte = 1, disp = 0;
		if (matchEdge(ptr, Opcode.PtrAdd.tag)) {
			var add = SsaApplyOp.!(ptr.dest);
			var y = add.input1();
			if (SsaConst.?(y)) {
				var xscale = matchScale(add.inputs[0]);
				base = xscale.0;
				scale = xscale.1;
				disp = y.unbox<int>();
			}
		}
		var canTrap = V3Op.needsNullCheck(i, base);
		if (size == 4) gen("cmpswpd", asm_cmpswpx, (X86Assembler.cmpxchngd, dest, gpr(base), scale, disp, gpr(val), canTrap, i.source));
		else if (size == 1) gen("cmpswpb", asm_cmpswpx, (X86Assembler.cmpxchngb, dest, gpr(base), scale, disp, byt(val), canTrap, i.source));
		else if (size == 2) gen("cmpswpw", asm_cmpswpx, (X86Assembler.cmpxchngw, dest, gpr(base), scale, disp, gpr(val), canTrap, i.source));
		else context.fail("invalid size for cmpswp");
		return true;
	}
	def genStoreAddr(i: SsaApplyOp, v: VReg, x: SsaInstr, scale: byte, disp: int, val: SsaInstr) -> bool {
		var t = i.op.typeArgs[1];
		var size = mach.sizeOf(t);
		var nullCheck = V3Op.needsNullCheck(i, x);
		if (SsaConst.?(val)) {
			// match stores of immediates
			match (SsaConst.!(val).val) {
				null => return gen_storex_i(size, x, scale, disp, 0, nullCheck, i.source);
				v: Box<int> => return gen_storex_i(size, x, scale, disp, v.val, nullCheck, i.source);
				v: Box<byte> => return gen_storex_i(size, x, scale, disp, v.val, nullCheck, i.source);
				v: Box<bool> => return gen_storex_i(size, x, scale, disp, Bool.toInt(v.val), nullCheck, i.source);
				// XXX: store of immediate addresses
			}
		}
		if (FloatType.?(t)) gen_storex_sse(x, scale, disp, val, FloatType.!(t).is64, nullCheck, i.source);
		else gen_storex(size, x, scale, disp, val, nullCheck, i.source);
		return true;
	}
	def genDiv(i: SsaApplyOp, v: VReg, div: bool) -> bool {
		dfnAt(v, if(div, Regs.EAX, Regs.EDX));
		kill(if(div, Regs.EDX, Regs.EAX));
		useFixed(i.input0(), Regs.EAX);
		var bu = useFixed(i.input1(), Regs.NOT_EDX);
		var zeroCheck = !i.facts.O_NO_ZERO_CHECK;
		if (IntType.!(i.op.sig.returnType()).signed) {
			var negCheck = !i.facts.O_NO_DIV_CHECK;
			gen(if(div, "idiv", "imod"), asm_idiv, (i.source, zeroCheck, negCheck, bu, div));
		} else {
			gen(if(div, "udiv", "umod"), asm_udiv, (i.source, zeroCheck, bu));
		}
		return zeroCheck; // pure if no zero check
	}
	def genCondThrow(i: SsaApplyOp, v: VReg, exception: string) -> bool {
		// generate a branch to the throw function
		var cmp = matchCmp(i.input0());
		genCmp(cmp);
		gen("br", asm_brthrow, (cmp.cond, exception, i.source));
		return true;
	}
	def genAlloc(i: SsaApplyOp, v: VReg) -> bool {
		var size = i.input0();
		def objLoc = Regs.EAX; // TODO: hard-coded calling conv
		def sizeLoc = Regs.EAX; // TODO: hard-coded calling conv
		if (mach.allocStub != null) {
			// generate a call to the allocation stub
			dfnAt(v, objLoc);
			var lp = if(mach.runtime.gc != null, livePoint());
			kill(Regs.ALL);
			useAt(makeVar(size), sizeLoc);
			gen("alloc", asm_alloc_rt, (lp, i.source));
		} else {
			// generate inlined (test) allocation
			hint(size, v);
			gen("alloc", asm_alloc_test, (dfngpr(v), gpr(size), i.source));
		}
		return true;
	}
	def genCall(call: SsaApplyOp, rv: VReg, funcRep: Mach_FuncRep) -> bool {
		var func = call.input0(), mi: MachInstr;
		var conv = frame.allocCallerSpace(X86VirgilCallConv.getForFunc(mach, funcRep));

		// define the return value(s) of the call
		for (i < rv.varSize) {
			dfnAt(vars[rv.varNum + i], conv.calleeRet(i));
		}
		var lp = if(rtgc != null, livePoint());
		kill(Regs.ALL);
		var inputs = call.inputs;
		if (SsaConst.?(func)) {
			// match CallAddress(#func, ...)
			var target = Address<IrMethod>.!(val(func));
			// don't emit the (constant) receiver for a direct component call
			var skip = if(target != null && V3.isComponent(target.val.receiver), 1, 0);
			for (i = 1 + skip; i < inputs.length; i++) {  // input[0] == func
				useFixed(inputs[i].dest, conv.calleeParam(i - 1));
			}
			// generate a direct call
			mi = gen("call", asm_call, (conv, lp, target, call.source));
		} else {
			for (i = 1; i < inputs.length; i++) {  // input[0] == func
				useFixed(inputs[i].dest, conv.calleeParam(i - 1));
			}
			// generate an indirect call, {func} as last input
			mi = gen("icall", asm_icall, (conv, lp, gpr(func), call.source));
		}
		return true;
	}
	def genCallKernel(sys: SsaApplyOp, rv: VReg, kernel: Kernel) -> bool {
		match (kernel) {
			LINUX => {
				// polymorphic system call: define EAX and EDX
				dfnAt(rv, Regs.EAX);
				// define second result in EDX
				dfnAt(vars[rv.varNum + 1], Regs.EDX);
				// accept system call number in EAX
				useFixed(sys.input0(), Regs.EAX);
				// accept arguments in registers according to kernel calling convention
				for (i = 1; i < sys.inputs.length; i++) {
					useFixed(sys.inputs[i].dest, X86_LINUX_KERNEL_PARAM_REGS[i - 1]);
				}
				gen("linux:int80", asm_int80, ());
			}
			DARWIN => {
				var inputs = sys.inputs;
				frame.allocOverflow((1 + inputs.length) * mach.data.addressSize);
				// polymorphic system call: define EAX, EDX, and ECX = eflags
				dfnAt(rv, Regs.EAX);
				// define second result in EDX
				dfnAt(vars[rv.varNum + 1], Regs.EDX);
				// define third result in ECX
				var flags = vars[rv.varNum + 2];
				dfnAt(flags, Regs.ECX);
				// accept system call number in EAX
				useFixed(inputs[0].dest, Regs.EAX);
				// accept arguments on callee's portion of stack of current frame
				var start = frame.conv.regSet.calleeStart;
				for (i = 1; i < inputs.length; i++) {
					useFixed(inputs[i].dest, start + i);
				}
				gen("darwin:int80", asm_int80_flags, flags);
			}
			_ => context.fail1("unsupported kernel on x86: %s", kernel.name);
		}
		return true;
	}
	def genShift(name: string, i: SsaApplyOp, v: VReg, shifter: X86Shifter) {
		var x = i.input0(), y = i.input1();
		if (SsaConst.?(y)) {
			// shift by constant
			gen(name, asm_sh_i, (shifter, dfn(v), use(x), y.unbox<int>(), hint(x, v)));
		} else {
			// gen shift by cl
			gen(name, asm_sh, (shifter, dfnAt(v, Regs.NOT_ECX), useFixed(y, Regs.ECX), useFixed(x, Regs.NOT_ECX), hint(x, v)));
		}
	}
	def genReturn(i: SsaReturn) {
		for (j < i.inputs.length) {
			useFixed(i.inputs[j].dest, frame.conv.callerRet(j));
		}
		gen("ret", asm_ret, ());
	}
	def genGoto(target: SsaBlock) {
		if (target.hasPhis()) gen("phi_resolve", asm_phi_resolve, code.length);
		if (blocks.isImmediatelyAfter(context.block, target)) gen("nop", asm_nop, ());
		else gen_br(null, target);
	}
	def genJump(target: SsaBlock) {
		if (!blocks.isImmediatelyAfter(context.block, target)) gen_br(null, target);
	}
	def genThrow(i: SsaThrow) {
		gen("throw", asm_throw, (i.source, i.exception));
	}
	def genIf(i: SsaIf) {
		var key = i.input0(), cmp = matchCmp(key), succ = context.block.succs();
		var s0 = succ[0].dest, s1 = succ[1].dest;
		if (blocks.isImmediatelyAfter(context.block, s0)) {
			// can fall through to first successor
			genCmpBr(cmp.negate(), s1);
		} else {
			// branch to first successor
			genCmpBr(cmp, s0);
			genJump(s1);
		}
	}
	def genSwitch(i: SsaSwitch) {
		gen("sw", asm_tableswitch, (i, context.block.succs(), use(i.input0())));
	}
	// pattern-match a comparison
	def matchCmp(i: SsaInstr) -> X86CmpMatch {
		if (!inSameBlock(i) || !SsaApplyOp.?(i)) return X86CmpMatch.new(X86Conds.NZ, i, null, null);
		match (SsaApplyOp.!(i).op.opcode) {
			BoolEq,
			IntEq,
			RefEq =>		return newCmp2(i, X86Conds.Z);
			IntLt =>		return newCmp2(i, signedCmp(i, X86Conds.L, X86Conds.C));
			IntLteq =>		return newCmp2(i, signedCmp(i, X86Conds.LE, X86Conds.NA));
			PtrLt =>		return newCmp2(i, X86Conds.C);
			PtrLteq =>		return newCmp2(i, X86Conds.NA);
			BoolNot => {
				var cmp = matchCmp(i.input0());
				return X86CmpMatch.new(cmp.cond.negate, cmp.x, cmp.y, cmp.val);
			}
			_ => ;
		}
		return X86CmpMatch.new(X86Conds.NZ, i, null, null);
	}
	def signedCmp(i: SsaInstr, signed: X86Cond, unsigned: X86Cond) -> X86Cond {
		var t = SsaApplyOp.!(i).op.sig.paramTypes[0];
		match (t.typeCon.kind) {
			V3Kind.POINTER => return unsigned;
			V3Kind.INT => return if (IntType.!(t).signed, signed, unsigned);
		}
		return signed;
	}
	def matchScale(e: SsaDfEdge) -> (SsaInstr, byte) {
		var i = e.dest;
		if (!soleEdge(e)) return (i, 1);
		var optag = i.optag();
		if (optag == Opcode.IntMul.tag) {
			var y = i.input1();
			if (SsaConst.?(y)) { // match i = x * K
				var scale = byte.view(y.unbox<int>());
				match (scale) {
					1, 2, 4, 8 => return (i.input0(), scale);
				}
			}
		} else if (optag == Opcode.IntShl.tag) {
			var y = i.input1();
			if (SsaConst.?(y)) { // match i = x << K
				var shift = y.unbox<int>();
				match (shift) {
					0, 1, 2, 3 => return (i.input0(), byte.view(1 << u5.view(shift)));
				}
			}
		} else if (optag == Opcode.IntAdd.tag) {
			var x = i.input0();
			if (i.input1() == x) return (x, 2); // i = x + x
		}
		return (i, 1);
	}
	def isSigned(i: SsaApplyOp, isReturn: bool) -> bool {
		if (isReturn) return IntType.!(i.getType()).signed;
		return IntType.!(i.op.typeArgs[0]).signed;
	}
	def newCmp2(i: SsaInstr, cond: X86Cond) -> X86CmpMatch {
		var x = i.input0(), y = i.input1();
		if (SsaConst.?(y)) return X86CmpMatch.new(cond, x, null, val(y));
		if (SsaConst.?(x) && cond.commute != null) return X86CmpMatch.new(cond.commute, y, null, val(x));
		return X86CmpMatch.new(cond, x, y, null);
	}
	def genIntCastF(i: SsaApplyOp, v: VReg, isDouble: bool) {
	   var name: string, m: (X86MacroAssembler, X86Reg, SSERm) -> void;
	   var signed = isSigned(i, true);
	   if (isDouble) {
	      name = if(signed, "cvt_d2i", "cvt_f2ui");
	      m = if(signed, X86Assembler.cvtsd2si, X86MacroAssembler.cvt_d2ui);
	   } else {
	      name = if(signed, "cvt_f2i", "cvt_f2ui");
	      m = if(signed, X86Assembler.cvtss2si, X86MacroAssembler.cvt_f2ui);
	   }
	   gen_sse_f2i(name, m, v, i.input0());
	}
	def genIntTruncF(i: SsaApplyOp, v: VReg, isDouble: bool) {
	   var name: string, m: (X86MacroAssembler, X86Reg, SSERm) -> void;
	   var signed = isSigned(i, true);
	   if (isDouble) {
	      name = if(signed, "int_truncd", "uint_truncd");
	      m = if(signed, X86MacroAssembler.int_truncd, X86MacroAssembler.uint_truncd);
	   } else {
	      name = if(signed, "int_truncf", "uint_truncf");
	      m = if(signed, X86MacroAssembler.int_truncf, X86MacroAssembler.uint_truncf);
	   }
	   gen_sse_trunc(name, m, v, i.input0());
	}
	def genFloatRoundI(i: SsaApplyOp, v: VReg, isDouble: bool) {
		var name: string, m: (X86MacroAssembler, SSEReg, X86Rm) -> void;
		var signed = isSigned(i, false);
		if (isDouble) {
			name = if(signed, "round_i2d", "round_ui2d");
			m = if(signed, X86Assembler.cvtsi2sd, X86MacroAssembler.cvt_ui2d);
		} else {
			name = if(signed, "round_i2f", "round_ui2f");
			m = if(signed, X86Assembler.cvtsi2ss, X86MacroAssembler.cvt_ui2f);
		}
		gen_sse_i2f(name, m, v, i.input0());
	}

	def gen_entry() {
		// add defs of each parameter as the first instruction
		var p = context.graph.params, max = p.length;
		var defs = Array<int>.new(max);
		for (i < max) {
			var pvar = makeVar(p[i]), loc = frame.conv.callerParam(i);
			if (frame.conv.regSet.isReg(loc)) {
				defs[i] = dfnAt(pvar, loc);
				pvar.hint = byte.view(loc); // register hint
			} else {
				defs[i] = dfnAt(pvar, loc);
				pvar.spill = loc; // reuse spill slot in caller frame
			}
		}
		gen("entry", asm_entry, defs).live = true;
	}
	def asm_entry(defs: Array<int>) {
		var adjust = frameAdjust();
		if (adjust > 0) asm.sub.rm_i(X86Regs.ESP, adjust); // allocate frame
	}

	// ---------- Loads ----------------------------------------------
	def gen_loadx(dest: VReg, t: Type, base: SsaInstr, scale: byte, disp: int, canTrap: bool, source: Source) {
		var size = mach.sizeOf(t);
		if (size == 0) {
			// load produces no value; only needed for possible nullcheck
			gen("test_i", asm_test_i, (gpr(base), scale, disp, 0, canTrap, source));
			return;
		}
		var lt = matchLoad(t, size);
		gen(lt.0, asm_loadx, (lt.1, dfngpr(dest), gpr(base), scale, disp, canTrap, source));
	}
	def gen_loada(dest: VReg, t: Type, addr: Addr) {
		var lt = matchLoad(t, mach.sizeOf(t));
		gen(lt.0, asm_loada, (lt.1, dfngpr(dest), addr));
	}
	def gen_loada_disp(dest: VReg, t: Type, addr: Addr, disp: SsaInstr) {
		var lt = matchLoad(t, mach.sizeOf(t));
		gen(lt.0, asm_loada_disp, (lt.1, dfngpr(dest), addr, gpr(disp)));
	}
	def gen_loadx_sse(dest: VReg, t: Type, base: SsaInstr, scale: byte, disp: int, isDouble: bool, canTrap: bool, source: Source) {
		if (isDouble) {
			gen("movsd", asm_loadx_sse, (X86Assembler.movsd_s_sm, dfnsse(dest), gpr(base), scale, disp, canTrap, source));
		} else {
			gen("movss", asm_loadx_sse, (X86Assembler.movss_s_sm, dfnsse(dest), gpr(base), scale, disp, canTrap, source));
		}
	}
	def gen_loada_sse(dest: VReg, t: Type, addr: Addr, isDouble: bool) {
		if (isDouble) gen("movsd", asm_loada_sse, (X86Assembler.movsd_s_sm, dfnsse(dest), addr));
		else gen("movss", asm_loada_sse, (X86Assembler.movss_s_sm, dfnsse(dest), addr));
	}
	def gen_loada_disp_sse(dest: VReg, t: Type, addr: Addr, disp: SsaInstr, isDouble: bool) {
		if (isDouble) gen("movsd", asm_loada_disp_sse, (X86Assembler.movsd_s_sm, dfnsse(dest), addr, gpr(disp)));
		else gen("movss", asm_loada_disp_sse, (X86Assembler.movss_s_sm, dfnsse(dest), addr, gpr(disp)));
	}
	def matchLoad(t: Type, size: int) -> (string, (X86Assembler, X86Reg, X86Rm) -> void) {
		// choose appropriate X86 load based on size and signedness
		if (size == 4) return ("loadd", X86Assembler.movd_r_rm);
		if (size == 1) return if(V3.isSigned(t),
				("loadsb", X86Assembler.movbsx),
				("loadub", X86Assembler.movbzx));
		if (size == 2) return if(V3.isSigned(t),
				("loadsw", X86Assembler.movwsx),
				("loaduw", X86Assembler.movwzx));
		context.fail("invalid size for load");
		return ("invalid", X86Assembler.movd_r_rm);
	}
	def asm_loadx(m: (X86Assembler, X86Reg, X86Rm) -> void, dest: int, base: int, scale: byte, disp: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, r(dest), X86Addr.new(null, r(base), scale, disp));
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def asm_loada(m: (X86Assembler, X86Reg, X86Rm) -> void, dest: int, addr: Addr) {
		var pos = asm.pos();
		m(asm, r(dest), X86Addrs.ABS_PATCH);
		recordPatch(pos, addr);
	}
	def asm_loada_disp(m: (X86Assembler, X86Reg, X86Rm) -> void, dest: int, addr: Addr, disp: int) {
		var pos = asm.pos();
		m(asm, r(dest), r(disp).plus(X86Addrs.ABS_CONST));
		recordPatch(pos, addr);
	}
	def asm_loadx_sse(m: (X86Assembler, SSEReg, SSERm) -> void, dest: int, base: int, scale: byte, disp: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, s(dest), SSEAddr.new(null, r(base), scale, disp));
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def asm_loada_sse(m: (X86Assembler, SSEReg, SSERm) -> void, dest: int, addr: Addr) {
		var pos = asm.pos();
		m(asm, s(dest), X86Addrs.ABS_PATCH_SSE);
		recordPatch(pos, addr);
	}
	def asm_loada_disp_sse(m: (X86Assembler, SSEReg, SSERm) -> void, dest: int, addr: Addr, disp: int) {
		var pos = asm.pos();
		m(asm, s(dest), r(disp).plusSSE(X86Addrs.ABS_CONST));
		recordPatch(pos, addr);
	}
	// ---------- Stores ----------------------------------------------
	def gen_storex(size: int, base: SsaInstr, scale: byte, disp: int, val: SsaInstr, canTrap: bool, source: Source) {
		if (size == 4) gen("stored", asm_storex, (X86Assembler.movd_rm_r, gpr(base), scale, disp, gpr(val), canTrap, source));
		else if (size == 1) gen("storeb", asm_storex, (X86Assembler.movb_rm_r, gpr(base), scale, disp, byt(val), canTrap, source));
		else if (size == 0) gen("test_i", asm_test_i, (gpr(base), scale, disp, 0, canTrap, source));
		else if (size == 2) gen("storew", asm_storex, (X86Assembler.movw_rm_r, gpr(base), scale, disp, gpr(val), canTrap, source));
		else context.fail("invalid size for store");
	}
	def gen_storex_sse(base: SsaInstr, scale: byte, disp: int, val: SsaInstr, isDouble: bool, canTrap: bool, source: Source) {
		if (isDouble) gen("storeq_sse", asm_storex_sse, (X86Assembler.movsd_sm_s, gpr(base), scale, disp, sse(val), canTrap, source));
		else gen("stored_sse", asm_storex_sse, (X86Assembler.movss_sm_s, gpr(base), scale, disp, sse(val), canTrap, source));
	}
	def gen_storex_i(size: int, base: SsaInstr, scale: byte, disp: int, val: int, canTrap: bool, source: Source) -> bool {
		if (size == 4) gen("stored_i", asm_storex_i, (X86Assembler.movd_rm_i, gpr(base), scale, disp, val, canTrap, source));
		else if (size == 1) gen("storeb_i", asm_storex_i, (X86Assembler.movb_rm_i, gpr(base), scale, disp, val, canTrap, source));
		else if (size == 0) gen("test_i", asm_test_i, (gpr(base), scale, disp, 0, canTrap, source));
		else if (size == 2) gen("storew_i", asm_storex_i, (X86Assembler.movw_rm_i, gpr(base), scale, disp, val, canTrap, source));
		else context.fail("invalid size for store");
		return true;
	}
	def asm_storex(m: (X86Assembler, X86Rm, X86Reg) -> void, base: int, scale: byte, disp: int, val: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, X86Addr.new(null, r(base), scale, disp), r(val));
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def asm_storex_sse(m: (X86Assembler, SSERm, SSEReg) -> void, base: int, scale: byte, disp: int, val: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, SSEAddr.new(null, r(base), scale, disp), s(val));
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def asm_cmpswpx(m: (X86Assembler, X86Rm, X86Reg) -> void, dest: int, base: int, scale: byte, disp: int, val: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, X86Addr.new(null, r(base), scale, disp), r(val));
		var reg = r(dest);
		asm.setz(reg);
		asm.movbzx(reg, reg);  // XXX: make zero extend unnecessary
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def gen_storex_abs(size: int, addr: Addr, val: SsaInstr) {
		if (size == 4) gen("stored_abs", asm_storex_abs, (X86Assembler.movd_rm_r, addr, gpr(val)));
		else if (size == 1) gen("storeb_abs", asm_storex_abs, (X86Assembler.movb_rm_r, addr, byt(val)));
		else if (size == 2) gen("storew_abs", asm_storex_abs, (X86Assembler.movw_rm_r, addr, gpr(val)));
		else context.fail("invalid size for store");
	}
	def gen_storex_abs_sse(addr: Addr, val: SsaInstr, isDouble: bool) {
		if (isDouble) gen("storeq_abs_sse", asm_storex_abs_sse, (X86Assembler.movsd_sm_s, addr, sse(val)));
		else gen("stored_abs_sse", asm_storex_abs_sse, (X86Assembler.movss_sm_s, addr, sse(val)));
	}
	def asm_storex_abs(m: (X86Assembler, X86Rm, X86Reg) -> void, addr: Addr, val: int) {
		var pos = asm.pos();
		m(asm, X86Addrs.ABS_PATCH, r(val));
		recordPatch(pos, addr);
	}
	def asm_storex_abs_sse(m: (X86Assembler, SSERm, SSEReg) -> void, addr: Addr, val: int) {
		var pos = asm.pos();
		m(asm, X86Addrs.ABS_PATCH_SSE, s(val));
		recordPatch(pos, addr);
	}
	def asm_cmpswpx_abs(m: (X86Assembler, X86Rm, X86Reg) -> void, dest: int, addr: Addr, val: int) {
		var pos = asm.pos();
		m(asm, X86Addrs.ABS_PATCH, r(val));
		var reg = r(dest);
		asm.setz(reg);
		asm.movbzx(reg, reg);  // XXX: make zero extend unnecessary
		recordPatch(pos, addr);
	}
	def asm_storex_i(m: (X86Assembler, X86Rm, int) -> void, base: int, scale: byte, disp: int, imm: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, X86Addr.new(null, r(base), scale, disp), imm);
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	// ---------- Lea and test --------------------------------------------------
	def gen_lea(dest: VReg, base: SsaInstr, scale: byte, disp: int) {
		gen("lea", asm_lea, (dfngpr(dest), gpr(base), scale, disp));
	}
	def asm_lea(dest: int, base: int, scale: byte, disp: int) {
		asm.lea(r(dest), X86Addr.new(null, r(base), scale, disp));
	}
	def asm_test_i(base: int, scale: byte, disp: int, imm: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		asm.test_rm_i(X86Addr.new(null, r(base), scale, disp), imm);
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	// ---------- Basic operations ----------------------------------------------
	def gen_op2(name: string, commutative: bool, m: X86Assembler -> X86Op2, dest: VReg, a: SsaInstr, b: SsaInstr) {
		var f = if(commutative, asm_op2_comm, asm_op2);
		gen(name, f, (m, dfngpr(dest), use(a), use(b), hint(a, dest)));
	}
	// ---------- Basic operations (SSE) ----------------------------------------
	def gen_sse_op2(name: string, m: (X86Assembler, SSEReg, SSERm) -> void, dest: VReg, a: SsaInstr, b: SsaInstr) {
		gen(name, asm_sse_op2, (m, dfnsse(dest), use(a), use(b), hint(a, dest)));
	}
	def asm_sse_op2(m: (X86Assembler, SSEReg, SSERm) -> void, dest: int, a: int, b: int, v: void) {
		dest = loc(dest); a = loc(a); b = loc(b);
		if (dest != a) {
			if (dest == b) {
				var scratch = frame.conv.regSet.scratch[RegClass.F32.tag];
				asm_movd_l_l(b, scratch);
				b = scratch;
			}
			asm_movd_l_l(a, dest);
		}
		m(asm, loc_s(dest), loc_sm(b));
	}
	// ---------- Conversion and viewing operations -----------------------------
	def gen_sse_f2i(name: string, m: (X86MacroAssembler, X86Reg, SSERm) -> void, dest: VReg, a: SsaInstr) {
		gen(name, asm_sse_f2i, (m, dfn(dest), use(a)));
	}
	def gen_sse_i2f(name: string, m: (X86MacroAssembler, SSEReg, X86Rm) -> void, dest: VReg, a: SsaInstr) {
		gen(name, asm_sse_i2f, (m, dfnsse(dest), use(a)));
	}
	def gen_sse_cvt(name: string, m: (X86Assembler, SSEReg, SSERm) -> void, dest: VReg, a: SsaInstr) {
		gen(name, asm_sse_cvt, (m, dfnsse(dest), use(a)));
	}
	def gen_sse_view_int(name: string, m: (X86Assembler, X86Rm, SSEReg) -> void, dest: VReg, a: SsaInstr) {
		gen(name, asm_sse_view_int, (m, dfn(dest), use(a)));
	}
	def gen_sse_view_float(name: string, m: (X86Assembler, SSEReg, X86Rm) -> void, dest: VReg, a: SsaInstr) {
		gen(name, asm_sse_view_float, (m, dfnsse(dest), use(a)));
	}
	def gen_sse_view_double(name: string, m: (X86Assembler, SSEReg, SSERm) -> void, dest: VReg, a: SsaInstr, b: SsaInstr) {
		var spill = frame.allocSpill(RegClass.I64) & ~(frame.IS_64);
		gen(name, asm_sse_view_double, (m, dfnsse(dest), useFixed(a, spill), useFixed(b, spill + 1)));
	}
	def asm_sse_f2i(m: (X86MacroAssembler, X86Reg, SSERm) -> void, dest: int, a: int) {
		dest = loc(dest); a = loc(a);
		m(asm, loc_r(dest), loc_sm(a));
	}
	def asm_sse_i2f(m: (X86MacroAssembler, SSEReg, X86Rm) -> void, dest: int, a: int) {
		dest = loc(dest); a = loc(a);
		m(asm, loc_s(dest), loc_rm(a));
	}
	def asm_sse_f2l(m: (X86MacroAssembler, X86Rm, X86Rm, SSERm, bool) -> void, il: int, ih: int, a: int, isDouble: bool) {
		il = loc(il); ih = loc(ih); a = loc(a);
		m(asm, loc_rm(il), loc_rm(ih), loc_sm(a), isDouble);
	}
	def asm_sse_l2f(m: (X86MacroAssembler, SSEReg, X86Rm, X86Rm, bool) -> void, dest: int, a: int, b: int, isDouble: bool) {
		dest = loc(dest); a = loc(a); b = loc(b);
		m(asm, loc_s(dest), loc_rm(a), loc_rm(b), isDouble);
	}
	def asm_sse_cvt(m: (X86Assembler, SSEReg, SSERm) -> void, dest: int, a: int) {
		dest = loc(dest); a = loc(a);
		m(asm, loc_s(dest), loc_sm(a));
	}
	def asm_sse_view_int(m: (X86Assembler, X86Rm, SSEReg) -> void, dest: int, a: int) {
		dest = loc(dest); a = loc(a);
		m(asm, loc_rm(dest), loc_s(a));
	}
	def asm_sse_view_float(m: (X86Assembler, SSEReg, X86Rm) -> void, dest: int, a: int) {
		dest = loc(dest); a = loc(a);
		m(asm, loc_s(dest), loc_rm(a));
	}
	def asm_sse_view_double(m: (X86Assembler, SSEReg, SSERm) -> void, dest: int, il: int, ih: int) {
		dest = loc(dest); il = loc(il); // ih is ignored as movq assumes the address points to a 64-bit value
		m(asm, loc_s(dest), loc_sm(il));
	}
	def asm_sse_view_long(m: (X86Assembler, SSERm, SSEReg) -> void, il: int, ih: int, a: int) {
		il = loc(il); a = loc(a); // ih is ignored as movq assumes the address points to a 64-bit value
		m(asm, loc_sm(il), loc_s(a));
	}
	// ---------- Miscellaneous maths operations --------------------------------
	def gen_sse_sqrt(name: string, m: (X86Assembler, SSEReg, SSERm) -> void, dest: VReg, a: SsaInstr) {
		gen(name, asm_sse_sqrt, (m, dfnsse(dest), use(a)));
	}
	def gen_sse_fabs(dest: VReg, a: SsaInstr, isDouble: bool) {
		gen("fabs", asm_sse_fabs, (dfnsse(dest), use(a), isDouble, hint(a, dest)));
	}
	def asm_sse_sqrt(m: (X86Assembler, SSEReg, SSERm) -> void, dest: int, a: int) {
		dest = loc(dest); a = loc(a);
		if (dest != a) asm_movd_l_l(a, dest);
		m(asm, loc_s(dest), loc_sm(a));
	}
	def asm_sse_fabs(dest: int, a: int, isDouble: bool, v: void) {
		dest = loc(dest); a = loc(a);
		asm.fabs(loc_s(dest), loc_sm(a), isDouble);
	}
	// ---------- Rounding operations -------------------------------------------
	def gen_sse_round(name: string, m: (X86Assembler, SSEReg, SSERm, RoundingMode) -> void,
			dest: VReg, a: SsaInstr, mode: RoundingMode) {
		gen(name, asm_sse_round, (m, dfnsse(dest), use(a), mode, hint(a, dest)));
	}
	def gen_sse_trunc(name: string, m: (X86MacroAssembler, X86Reg, SSERm) -> void, dest: VReg, a: SsaInstr) {
		gen(name, asm_sse_trunc, (m, dfn(dest), use(a)));
	}
	def asm_sse_round(m: (X86Assembler, SSEReg, SSERm, RoundingMode) -> void, dest: int, a: int,
			mode: RoundingMode, v: void) {
		dest = loc(dest); a = loc(a);
		if (dest != a) asm_movd_l_l(a, dest);
		m(asm, loc_s(dest), loc_sm(a), mode);
	}
	def asm_sse_trunc(m: (X86MacroAssembler, X86Reg, SSERm) -> void, dest: int, a: int) {
		dest = loc(dest); a = loc(a);
		m(asm, loc_r(dest), loc_sm(a));
	}
	// ---------- Basic operations (immediate) ---------------------------------
	def gen_op2_i(name: string, m: X86Assembler -> X86Op2, dest: VReg, a: SsaInstr, imm: Val) {
		match (imm) {
			null => gen(name, asm_op2_i, (m, dfn(dest), use(a), 0, hint(a, dest)));
			x: Box<int> => gen(name, asm_op2_i, (m, dfn(dest), use(a), x.val, hint(a, dest)));
			x: Addr => gen(name, asm_op2_a, (m, dfn(dest), use(a), x, hint(a, dest)));
			x: Box<bool> => gen(name, asm_op2_i, (m, dfn(dest), use(a), Bool.toInt(x.val), hint(a, dest)));
			_ => context.fail1("not a valid immediate: %s", V3.renderResult(imm, null, _));
		}
	}
	def gen_add_i(dest: VReg, a: SsaInstr, imm: int) {
		gen("add_i", asm_add_i, (dfn(dest), use(a), imm, hint(a, dest)));
	}
	def gen_neg(dest: VReg, a: SsaInstr) {
		gen("neg", asm_neg, (dfn(dest), use(a), hint(a, dest)));
	}
	def gen_not(dest: VReg, a: SsaInstr) {
		gen("not", asm_not, (dfn(dest), use(a), hint(a, dest)));
	}
	def asm_op2(m: X86Assembler -> X86Op2, dest: int, a: int, b: int, v: void) {
		dest = loc(dest); a = loc(a); b = loc(b);
		if (dest != a) {
			if (dest == b) {
				var scratch = frame.conv.regSet.scratch[RegClass.I32.tag];
				asm_movd_l_l(b, scratch);
				b = scratch;
			}
			asm_movd_l_l(a, dest);
		}
		m(asm).r_rm(loc_r(dest), loc_rm(b));
	}
	def asm_op2_comm(m: X86Assembler -> X86Op2, dest: int, a: int, b: int, v: void) {
		dest = loc(dest); a = loc(a); b = loc(b);
		if (dest == a) return m(asm).r_rm(loc_r(dest), loc_rm(b)); // op(dest = a, b)
		if (dest == b) return m(asm).r_rm(loc_r(dest), loc_rm(a)); // op(dest = b, a)
		if (dest != a) asm_movd_l_l(a, dest);
		m(asm).r_rm(loc_r(dest), loc_rm(b));
	}
	def asm_op2_i(m: X86Assembler -> X86Op2, dest: int, a: int, imm: int, v: void) {
		m(asm).rm_i(resolveOp1(dest, a), imm);
	}
	def asm_add_i(dest: int, a: int, imm: int, v: void) {
		dest = loc(dest); a = loc(a);
		if (dest != a) {
			if (frame.conv.regSet.isReg(dest) && frame.conv.regSet.isReg(a)) return asm.lea(loc_r(dest), loc_r(a).plus(imm));
			asm_movd_l_l(a, dest);
		}
		asm.add.rm_i(loc_rm(dest), imm);
	}
	def asm_op2_a(m: X86Assembler -> X86Op2, dest: int, a: int, addr: Addr, v: void) {
		var drm = resolveOp1(dest, a), pos = asm.pos();
		m(asm).rm_i(drm, X86Addrs.ABS_CONST);
		recordPatch(pos, addr);
	}
	def gen_cmp(a: SsaInstr, b: SsaInstr) {
		gen("cmp", asm_cmp, (gpr(a), use(b)));
	}
	def gen_cmp_i(a: SsaInstr, imm: int) {
		gen("cmp_i", asm_cmp_i, (use(a), imm));
	}
	def gen_cmp_a(a: SsaInstr, addr: Addr) {
		gen("cmp_a", asm_cmp_a, (use(a), addr));
	}
	def gen_sse_cmp(cond: X86Cond, dest: VReg, a: SsaInstr, b: SsaInstr, isDouble: bool) {
		gen("cmp_fp", asm_sse_cmp, (cond, dfnAt(dest, Regs.BYTE), use(a), use(b), isDouble));
	}
	def gen_sse_bit_eq(dest: VReg, a: SsaInstr, b: SsaInstr, isDouble: bool) {
		gen("float_bit_eq", asm_sse_bit_eq, (dfn(dest), use(a), use(b), isDouble));
	}
	def asm_sse_cmp(cond: X86Cond, dest: int, a: int, b: int, isDouble: bool) {
		dest = loc(dest); a = loc(a); b = loc(b);
		asm.cmp_fp(cond, loc_r(dest), loc_s(a), loc_sm(b), isDouble);
	}
	def asm_sse_bit_eq(dest: int, a: int, b: int, isDouble: bool) {
		dest = loc(dest); a = loc(a); b = loc(b);
		asm.fp_bit_eq(loc_r(dest), loc_s(a), loc_sm(b), isDouble);
	}
	def asm_cmp(a: int, b: int) {
		asm.cmp.r_rm(r(a), rm(b));
	}
	def asm_cmp_i(a: int, imm: int) {
		asm.cmp.rm_i(rm(a), imm);
	}
	def asm_cmp_a(a: int, addr: Addr) {
		var pos = asm.pos();
		asm.cmp.rm_i(rm(a), X86Addrs.ABS_CONST);
		recordPatch(pos, addr);
	}
	def asm_not(dest: int, a: int, v: void) {
		asm.not(resolveOp1(dest, a));
	}
	def asm_neg(dest: int, a: int, v: void) {
		asm.neg(resolveOp1(dest, a));
	}
	def resolveOp1(dest: int, a: int) -> X86Rm {
		dest = loc(dest); a = loc(a);
		if (dest != a) asm_movd_l_l(a, dest);
		return loc_rm(dest);
	}
	// ---------- Shift operations ---------------------------------------------
	def asm_sh(shifter: X86Shifter, dest: int, cl: int, a: int, v: void) {
		shifter.sh_cl(asm, resolveOp1(dest, a));
	}
	def asm_sh_i(shifter: X86Shifter, dest: int, a: int, imm: int, v: void) {
		shifter.sh_i(asm, resolveOp1(dest, a), imm);
	}
	def asm_sext(dest: int, a: int, size: int) {
		var d = resolveOp1(dest, a);
		asm.shl_i(d, 32 - size);
		asm.sar_i(d, 32 - size);
	}
	// ---------- Multiply / Divide -------------------------------------------
	def gen_imul (dest: VReg, a: SsaInstr, b: SsaInstr) {
		gen("imul", asm_imul, (dfngpr(dest), use(a), use(b), hint(a, dest)));
	}
	def asm_imul(dest: int, a: int, b: int, v: void) {
		dest = loc(dest); a = loc(a); b = loc(b);
		if (dest == a) return asm.imul_r_rm(loc_r(dest), loc_rm(b)); // op(dest = a, b)
		if (dest == b) return asm.imul_r_rm(loc_r(dest), loc_rm(a)); // op(dest = b, a)
		if (dest != a) asm_movd_l_l(a, dest);
		asm.imul_r_rm(loc_r(dest), loc_rm(b));
	}
	def gen_imul_i(dest: VReg, a: SsaInstr, imm: int) {
		gen("imul_i", asm_imul_i, (dfngpr(dest), use(a), imm, hint(a, dest)));
	}
	def asm_imul_i(dest: int, a: int, imm: int, v: void) {
		dest = loc(dest); a = loc(a);
		if (dest != a) asm_movd_l_l(a, dest);
		asm.imul_r_i(loc_r(dest), imm);
	}
	def asm_idiv(source: Source, zeroCheck: bool, negCheck: bool, a: int, div: bool) {
		asm.idivmod_checked(source, zeroCheck && rtsrc != null, negCheck, rm(a), div);
	}
	def asm_udiv(source: Source, zeroCheck: bool, a: int) {
		asm.udivmod_checked(source, zeroCheck && rtsrc != null, rm(a));
	}
	def gen_set(cond: X86Cond, a: VReg) {
		gen("set", asm_set, (cond, dfnAt(a, Regs.BYTE)));
	}
	def asm_set(cond: X86Cond, a: int) {
		var reg = r(a);
		asm.setx(cond, reg);
		asm.movbzx(reg, reg); // XXX: make movzbx unnecessary
	}
	def gen_br(cond: X86Cond, target: SsaBlock) {
		gen(if(cond == null, "jmp", "br"), asm_br, (cond, target));
	}
	def asm_br(cond: X86Cond, target: SsaBlock) {
		asm.jmpx(cond, X86Addrs.REL_CONST);
		recordBranch(asm.pos() - 4, target);
	}
	def asm_brthrow(cond: X86Cond, ex: string, source: Source) {
		asm.jmpx_addr(cond, rt.getExceptionDest(asm.codeOffset(), ex, source));
	}
	def gen_phi_dfn(phi: SsaPhi) -> SsaLink {
		phiList = List.new(phi, phiList);
		gen("phi_dfn", asm_nop, ()).live = true;
		var next = phi.next;
		while (SsaPhi.?(next)) next = next.next;
		return next;
	}
	def asm_nop() {
		// do nothing.
	}
	def asm_wsh(shifter: X86Shifter, dl: int, dh: int, al: int, ah: int, sl: int, sh: int) {
		// Assumes sl == ECX
		if (sh == -1) shifter.wsh(asm, r(dl), r(dh), null);
		else shifter.wsh(asm, r(dl), r(dh), r(sh));
	}
	def asm_wadd(dl: int, dh: int, al: int, ah: int, bl: int, bh: int) {
		// Assumes dl == al, dh == ah
		asm.add.rm_r(r(dl), r(bl));
		asm.adc.rm_r(r(dh), r(bh));
	}
	def asm_wsub(dl: int, dh: int, al: int, ah: int, bl: int, bh: int) {
		// Assumes dl == al, dh == ah
		asm.sub.rm_r(r(dl), r(bl));
		asm.sbb.rm_r(r(dh), r(bh));
	}
	def asm_wmul(dl: int, dh: int, k: int, al: int, ah: int, bl: int, bh: int) {
		// Assumes dl == al, dh == ah, al = %eax, ah = %edx
		var alr = r(dl), ahr = r(dh), blr = r(bl), bhr = r(bh);
		asm.imul_r_rm(bhr, alr);
		asm.imul_r_rm(ahr, blr);
		asm.add.r_rm(bhr, ahr);
		asm.mul(blr);
		asm.add.r_rm(ahr, bhr);
	}
	def asm_wdiv(op: WideDivision, source: Source) {
		asm.wdiv(op, source);
	}
	def asm_alloc_test(dest: int, sz: int, source: Source) {
		// exchange-add [CiRuntime.heapCurLoc] with size
		var rd = r(dest), ra = r(sz);
		if (rd != ra) asm.movd_rm_r(rd, ra);
		var pos = asm.pos();
		asm.xadd(X86Addrs.ABS_PATCH, rd);
		recordPatch(pos, CiRuntimeModule.HEAP_CUR_LOC);
	}
	def asm_alloc_rt(lp: int, source: Source) {
		// call a shared allocation stub routine
		asm.call_addr(mach.allocStub);
		var off = asm.codeOffset();
		if (rtgc != null) rtgc.recordStackRefMap(off, source, buildStackMap(off, null, lp));
		if (rtsrc != null) rtsrc.recordReturnSource(off, source);
	}
	def asm_int80() {
		asm.intK(0x80);
	}
	def asm_int80_flags(flags: VReg) {
		asm.intK(0x80);
		if (flags.live) {
			asm.pushfd();         // push flags
			asm.pop(X86Regs.ECX); // pop flags into ECX
		}
	}
	def asm_caller_ip(dst: int) {
		var d = rm(dst);
		if (X86Reg.?(d)) asm.movd_r_rm(X86Reg.!(d), X86Regs.ESP.plus(frameAdjust()));
		else {
			asm.movd_r_rm(scratchReg, X86Regs.ESP.plus(frame.size()));
			asm.movd_rm_r(d, scratchReg);
		}
	}
	def asm_caller_sp(dst: int) {
		var d = rm(dst);
		if (X86Reg.?(d)) asm.lea(X86Reg.!(d), X86Regs.ESP.plus(frame.size()));
		else {
			asm.lea(scratchReg, X86Regs.ESP.plus(frame.size()));
			asm.movd_rm_r(d, scratchReg);
		}
	}
	def asm_phi_resolve(pos: int) {
		// do nothing. all moves should be inserted by main emit() loop
	}
	def asm_ret() {
		var adjust = frameAdjust();
		if (adjust > 0) asm.add.rm_i(X86Regs.ESP, adjust); // deallocate frame
		asm.ret();
	}
	def asm_call(conv: MachCallConv, lp: int, target: Addr, source: Source) {
		asm.call_addr(target);
		var off = asm.codeOffset();
		if (rtgc != null) rtgc.recordStackRefMap(off, source, buildStackMap(off, conv, lp));
		if (rtsrc != null) rtsrc.recordReturnSource(off, source);
	}
	def asm_icall(conv: MachCallConv, lp: int, addr: int, source: Source) {
		// TODO: record canTrap for indirect calls
		asm.icall(rm(addr));
		var off = asm.codeOffset();
		if (rtgc != null) rtgc.recordStackRefMap(off, source, buildStackMap(off, conv, lp));
		if (rtsrc != null) rtsrc.recordReturnSource(off, source);
	}
	def buildStackMap(off: int, conv: MachCallConv, lp: int) -> int {
		var builder = rtgc.beginRefMap(frame.slots(), 20);
		if (conv != null && conv.overflow > 0) {
			// record any outgoing overflow parameters that are references
			var pt = conv.paramTypes, rs = conv.regSet;
			for (i < pt.length) {
				var ploc = conv.calleeParam(i);
				if (rs.isStack(ploc) && mach.isRefType(pt[i])) {
					builder.setRefMap(ploc - rs.calleeStart);
				}
			}
		}
		// lazily compute the reference slot index for each live VReg
		if (varRefSlotIndex == null) computeVarRefSlotIndex();
		lsra.livemap.row(lp).apply(setRefMap, builder);
		return builder.finishRefMap();
	}
	def setRefMap(vnum: int, builder: MachRefMapBuilder) {
		var refSlot = varRefSlotIndex[vnum];
		if (refSlot > 0) builder.setRefMap(refSlot - 1 + frame.spillArgs);
	}
	// compute the reference slot index for each (live) VReg
	def computeVarRefSlotIndex() {
		// XXX: pull this code up to MachCodeGen--currently depends on frame layout
		varRefSlotIndex = Array<int>.new(vars.length);
		var regSet = frame.conv.regSet;
		for (i = 0; i < vars.length; ()) {
			var vreg = vars[i];
			if (vreg.varSize > 1) {
				// set reference slots for all sub-variables
				var types = Tuple.toTypeArray(vreg.ssa.getType());
				for (j < vreg.varSize) {
					var subVar = vars[i + j];
					if (subVar.live) setVarRefSlot(subVar, types[j]);
				}
				i = i + vreg.varSize;
			} else if (vreg.live && vreg.ssa != null) {
				// set a reference slot for this live mach var
				setVarRefSlot(vreg, vreg.ssa.getType());
				i++;
			} else {
				i++;
			}
		}
	}
	def setVarRefSlot(vreg: VReg, t: Type) {
		if (vreg.live && mach.isRefType(t)) {
			var spill = vreg.spill, regSet = frame.conv.regSet;
			if (spill >= regSet.spillStart && spill < regSet.callerStart) {
				varRefSlotIndex[vreg.varNum] = 1 + spill - regSet.spillStart;
			}
		}
	}
	def asm_throw(source: Source, ex: string) {
		asm.jmpx_addr(null, rt.getExceptionDest(asm.codeOffset(), ex, source));
	}
	def asm_tableswitch(sw: SsaSwitch, succ: Array<SsaCfEdge>, val: int) {
		var rm = rm(val), size = sw.size();
		asm.movd_r_rm(scratchReg, rm);
		asm.cmp.rm_i(scratchReg, size - 1);
		asm.ja(X86Addrs.REL_CONST);
		recordBranch(asm.pos() - 4, sw.default().dest);
		// load from the jump table to follow
		var start = asm.pos();
		asm.movd_r_rm(scratchReg, X86Addr.new(null, scratchReg, 4, X86Addrs.ABS_CONST));
		var jtaddrpos = asm.findAbsConst(start);
		asm.ijmp(scratchReg);
		// align and emit the jump table
		asm.w.align(4);
		var jumpTableAddr = asm.machBuffer.posAddr();
		asm.w.at(jtaddrpos).put_b32(jumpTableAddr);
		asm.w.atEnd();
		// emit jump table
		jumpTables = List.new((asm.pos(), succ), jumpTables);
		for (i < size) {
			asm.w.put_b32(0);
		}
	}

	// generates a single X86Instr and adds it to the "code" sequence
	def gen<T>(name: string, f: T -> void, params: T) -> MachInstr {
		return addInstr(X86Instr.new(name, f, params));
	}

	def gpr(a: SsaInstr) -> int {
		return useAt(makeVar(a), Regs.GPR);
	}
	def sse(a: SsaInstr) -> int {
		return useAt(makeVar(a), Regs.SSE_CLASS);
	}
	def dfngpr(a: VReg) -> int {
		return dfnAt(a, Regs.GPR);
	}
	def dfnsse(a: VReg) -> int {
		return dfnAt(a, Regs.SSE_CLASS);
	}
	def byt(a: SsaInstr) -> int {
		return useAt(makeVar(a), Regs.BYTE);
	}
	def use(a: SsaInstr) -> int {
		return useAt(makeVar(a), 0);
	}
	def useFixed(a: SsaInstr, fixed: int) -> int {
		return useAt(makeVar(a), fixed);
	}
	def dfn(a: VReg) -> int {
		return dfnAt(a, 0);
	}
	// get the location assignment for a use position
	def loc(usepos: int) -> int {
		return lsra.getAssignment(usepos);
	}
	// convert a use into an x86 register
	def r(usepos: int) -> X86Reg {
		return loc_r(loc(usepos));
	}
	// convert a use into an x86 register/memory
	def rm(usepos: int) -> X86Rm {
		return loc_rm(loc(usepos));
	}
	// convert a use into an SSE register
	def s(usepos: int) -> SSEReg {
		return loc_s(loc(usepos));
	}
	// convert a use into an SSE register/memory
	def sm(usepos: int) -> SSERm {
		return loc_sm(loc(usepos));
	}
	def recordBranch(pos: int, target: SsaBlock) {
		branches = List.new((pos, target), branches);
	}
	def print() {
		if (lsra != null) lsra.print();
	}
	// amount to adjust the frame at the beginning and end of invocation
	def frameAdjust() -> int {
		return frame.size() - mach.code.addressSize; // assumes return address already pushed
	}
	def allocMoveTmp(loc: int) -> int {
		if (Regs.isSSEReg(loc)) return frame.conv.regSet.scratch[RegClass.F32.tag];
		return frame.conv.regSet.scratch[RegClass.I32.tag];
	}
	def loc_r(loc: int) -> X86Reg {
		return asm.loc_r(frame, loc);
	}
	def loc_rm(loc: int) -> X86Rm {
		return asm.loc_rm(frame, loc);
	}
	def loc_s(loc: int) -> SSEReg {
		return asm.loc_s(frame, loc);
	}
	def loc_sm(loc: int) -> SSERm {
		return asm.loc_sm(frame, loc);
	}
	def asm_movd_l_l(src: int, dst: int) {
		if (src <= 0) return; // nothing to do
		if (src == dst) return; // nothing to do
		if (Regs.isSSEReg(src)) {
			if (is64(src, dst) || Regs.isSSEReg(dst)) { // check if spill slot is for a 64-bit value
				asm.movsd_sm_sm(loc_sm(dst), loc_sm(src));
			} else {
				asm.movss_sm_sm(loc_sm(dst), loc_sm(src));
			}
		} else if (Regs.isSSEReg(dst)) {
			if (is64(src, dst)) { // check if spill slot is for a 64-bit value
				asm.movsd_sm_sm(loc_sm(dst), loc_sm(src));
			} else {
				asm.movss_sm_sm(loc_sm(dst), loc_sm(src));
			}
		} else { // either src, dest are GPRs or memory-to-memory move
			if (is64(src, dst)) { // check if spill slot is for a 64-bit value
				asm.movq_m_m(loc_sm(dst), loc_sm(src));		// 64-bit
			} else {
				asm.movd_rm_rm(loc_rm(dst), loc_rm(src), null);	// 32-bit
			}
		}
	}
	def is64(src: int, dst: int) -> bool {
		return frame.is64(src | dst);
	}
	def recordPatch(start: int, addr: Addr) {
		asm.recordPatch(start, addr);
	}
	def emitMoves(mr: MoveResolver) {
		if (mr != null) mr.genMoves(allocMoveTmp, asm_movd_l_l);
	}
	def emitVarMoves(list: List<(int, int)>) {
		var mr = MoveResolver.new(context.prog.ERROR);
		for (l = list; l != null; l = l.tail) mr.addMove(loc(l.head.0), loc(l.head.1));
		mr.genMoves(allocMoveTmp, asm_movd_l_l);
	}
	def emitValMoves(list: List<(Val, int)>) {
		for (l = list; l != null; l = l.tail) asm.movd_l_val(frame, loc(l.head.1), l.head.0);
	}
}
