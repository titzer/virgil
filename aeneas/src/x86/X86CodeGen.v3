// Copyright 2011 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Utilities for X86 addresses
component X86Addrs {
	// absolute patch constant
	def ABS_CONST  = 0x44332211;
	def ABS_CONST0 = 0x11;
	def ABS_CONST1 = 0x22;
	def ABS_CONST2 = 0x33;
	def ABS_CONST3 = 0x44;

	// relative patch constant
	def REL_CONST = 0x44332205;

	def ABS_PATCH = X86Addr.new(null, 1, 0x44332211);
}
// Pattern match of a comparison between two vars or a var and a constant
class X86CmpMatch {
	def cond: X86Cond;
	def x: SsaInstr;
	def y: SsaInstr;
	def val: Val;
	new(cond, x, y, val) { }
	def negate() -> X86CmpMatch { return X86CmpMatch.new(cond.negate, x, y, val); }
}
// Represents an X86 instruction for register allocation and assembling
class X86Instr<T> extends MachInstr {
	def emitFunc: T -> void;
	def params: T;
	new(name: string, emitFunc, params) super(name) { }
	def emit() { emitFunc(params); }
}
def SHR_shifter = X86Shifter.new(Opcode.IntShr);
def SZR_shifter = X86Shifter.new(Opcode.IntSzr);
def SAR_shifter = X86Shifter.new(Opcode.IntSar);
def SHL_shifter = X86Shifter.new(Opcode.IntShl);
// Generates X86 code from SSA-M32 form.
// XXX: emit short branch instructions where possible
// XXX: align loop headers to cache boundaries
// XXX: schedule loads of constants for calls, phis, etc
// XXX: remove use of scratch register
class X86CodeGen extends MachCodeGen {
	def scratchReg = X86MachRegs.SCRATCH;
	var branches: List<(int, SsaBlock)>; // XXX: pull branches up to MachCodeGen?
	var jumpTables: List<(int, SsaSwitch)>;
	var asm: X86MacroAssembler;

	new(mach: MachProgram, context: SsaContext) super(mach, context) {
		frame = MachFrame.new(X86VirgilCallConv.getForGraph(context.graph));
	}
	def genCode(asm: X86MacroAssembler) {
		this.asm = asm;
		blocks.computeOrder();
		gen_entry();
		var blocks = blocks.order;
		for (i < blocks.length) {
			// emit all blocks in order
			var info = blocks.get(i);
			info.start = code.length;
			genBlock(info.block);
			info.end = code.length;
		}
		markAllLiveVars();
		genPhis();
		lsra = LinearScanRegAlloc.new(this, X86MachRegs.regs);
		lsra.assignRegs();
		frame.frameSize = mach.alignTo(frame.slots() * mach.refSize + mach.code.addressSize, mach.stackAlign);
		if (Aeneas.PRINT_MACH.get()) print();
		emitInstrs();
	}
	def genBlock(b: SsaBlock) {
		// generate code for each instruction in a block
		context.block = b;
		curBlockStart = code.length;
		if (b.phis != null) gen_phi_dfn(b.phis); // special instruction defines phis
		for (i = b.next; SsaInstr.?(i); i = i.next) {
			var mv = makeVar(SsaInstr.!(i));
			mv.start = code.length;
			var root = true;
			if (SsaApplyOp.?(i)) root = genApply(SsaApplyOp.!(i), mv);
			else if (SsaReturn.?(i)) genReturn(SsaReturn.!(i));
			else if (SsaIf.?(i)) genIf(SsaIf.!(i));
			else if (SsaGoto.?(i)) genGoto(SsaGoto.!(i).target());
			else if (SsaSwitch.?(i)) genSwitch(SsaSwitch.!(i));
			else if (SsaThrow.?(i)) genThrow(SsaThrow.!(i));
			mv.end = code.length;
			if (mv.varSize > 0) {
				for (i = mv.varNum; i < mv.varNum + mv.varSize; i++) {
					var ov = vars.get(i);
					ov.start = mv.start;
					ov.end = mv.end;
				}
			}
			if (root) markUsesLive(mv);
		}
	}
	def emitInstrs() {
		if (rtsrc != null) rtsrc.recordMethodStart(asm.codeOffset(), context.method.source, frame);
		var encoder = asm.encoder;
		var offsets = Array<int>.new(code.length);
		// assemble the code for each instruction
		for (i < code.length) {
			var instr = code.get(i);
			offsets(i) = encoder.pos;
			if (!instr.live) continue;
			var m = instr.moves;
			if (m != null) { // emit any spills / constants before instruction
				emitMoves(m.before);
				emitVarMoves(m.varMoves);
				emitValMoves(m.valMoves);
			}
			instr.emit();
			if (m != null) { // emit any restores after instruction
				emitMoves(m.after);
			}
		}
		// patch any branch instructions with the target address
		for (l = branches; l != null; l = l.tail) {
			var p = l.head.0;
			var t = offsets(blocks.order.get(l.head.1.mark).start);
			encoder.at(p).i4le(t - p - 4); // encode pc-relative address
		}
		// patch any jump tables
		for (l = jumpTables; l != null; l = l.tail) {
			var p = l.head.0, sw = l.head.1;
			encoder.at(p);
			var succ = sw.block.succ;
			for (i < succ.length) {
				var t = offsets(blocks.order.get(succ(i).dest.mark).start);
				encoder.i4le(t + asm.machEncoder.startAddr); // encode absolute address
			}
		}
		encoder.atEnd();
		if (rtsrc != null) rtsrc.recordFrameEnd(asm.codeOffset());
	}
	def genApply(i: SsaApplyOp, mv: MachVar) -> bool {
		var root = false;
		match (i.op.opcode) {
			IntAdd:			genAdd(i, mv);
			IntMul:			genMul(i, mv);
			IntDiv:			root = genDiv(i, mv, true);
			IntMod:			root = genDiv(i, mv, false);
			IntAnd,			// fall through
			BoolAnd:		genOp2("and", i, mv, X86Assembler.and);
			IntOr,			// fall through
			BoolOr:			genOp2("or", i, mv, X86Assembler.or);
			IntSub:			genOp2("sub", i, mv, X86Assembler.sub);
			IntXor:			genOp2("xor", i, mv, X86Assembler.xor);
			IntShl:			genShift("shl", i, mv, SHL_shifter);
			IntSar:			genShift("sar", i, mv, SAR_shifter);
			IntShr:			genShift("shr", i, mv, SHR_shifter);
			IntSzr:			genShift("szr", i, mv, SZR_shifter);
			IntWide:		root = genWide(i, i.op.attr<Operator>(), mv);
			Equal,			// fall through
			NotEqual,		// fall through
			IntLt,			// fall through
			IntGt,			// fall through
			IntLteq,		// fall through
			IntGteq,		// fall through
			UnsignedLt,		// fall through
			UnsignedLteq,		// fall through
			UnsignedGt,		// fall through
			UnsignedGteq:		genCmpSet(matchCmp(i), mv);
			IntConvert:		genIntConvert(i, mv);
			BoolNot:		gen_op2_i("xor", X86Assembler.xor, mv, i.input0(), Int.box(1));
			TupleGetElem: {
				// make tuple element refer to appropriate variable
				i.mark = makeVar(i.input0()).varNum + i.op.attr<int>();
			}
			ConditionalThrow:	root = genCondThrow(i, mv);
			PtrLoad:		root = genLoad(i, mv);
			PtrStore:		root = genStore(i, mv);
			PtrCmpSwp:		root = genCmpSwp(i, mv);
			PtrAdd:			genAdd(i, mv);
			PtrSub:			genOp2("sub", i, mv, X86Assembler.sub);
			Alloc: {
				rt.genAlloc(this, i, mv);
				root = true;
			}
			CallAddress:		root = genCall(i, mv);
			CallerIp:		gen("caller-ip", asm_caller_ip, dfngpr(mv));
			CallerSp:		gen("caller-sp", asm_caller_sp, dfn(mv));
			MachSystemOp: {
				rt.genSystemOp(this, i, mv);
				root = true;
			}
		} else {
			context.fail1("Unexpected opcode %1", i.op.opcode);
		}
		return root;
	}
	def genWide(i: SsaApplyOp, orig: Operator, mv: MachVar) -> bool {
		match (orig.opcode) {
			IntShl: return genWideShift(i, "wshl", SHL_shifter, mv);
			IntSar: return genWideShift(i, "wsar", SAR_shifter, mv);
			IntShr: return genWideShift(i, "wshr", SHR_shifter, mv);
			IntSzr: return genWideShift(i, "wszr", SZR_shifter, mv);
			IntAdd: return genWideArith(i, "wadd", asm_wadd, mv);
			IntSub: return genWideArith(i, "wsub", asm_wsub, mv);
			IntMul: return genWideMul(i, mv);
			IntDiv: return genWideDiv(i, orig, mv, false);
			IntMod: return genWideDiv(i, orig, mv, true);
			_: ;
		}
		// TODO: generate code for the wide operators
		for (j < mv.varSize) {
			dfn(vars.get(mv.varNum + j));
		}
		gen("wide", asm_nop, ()); // TODO
		return false;
	}
	def genWideMul(i: SsaApplyOp, mv: MachVar) -> bool {
		// TODO: loosen register requirements for 64-bit multiply
		gen("wmul", asm_wmul, (
			dfnAt(vars.get(mv.varNum), X86MachRegs.EAX),
			dfnAt(vars.get(mv.varNum + 1), X86MachRegs.EDX),
			kill(X86MachRegs.EDI),
			useFixed(i.input0(), X86MachRegs.EAX),
			useFixed(i.input1(), X86MachRegs.EDX),
			useFixed(i.inputs(2).dest, X86MachRegs.ESI),
			useFixed(i.inputs(3).dest, X86MachRegs.EDI)));
		return false;
	}
	def genWideDiv(i: SsaApplyOp, orig: Operator, mv: MachVar, mod: bool) -> bool {
		var stuff = (
			dfnAt(vars.get(mv.varNum), X86MachRegs.EAX),
			dfnAt(vars.get(mv.varNum + 1), X86MachRegs.EDX),
			kill(X86MachRegs.ALL),  // XXX: kill fewer regs with divide
			useFixed(i.input0(), X86MachRegs.ESI),
			useFixed(i.input1(), X86MachRegs.EAX),
			useFixed(i.inputs(2).dest, X86MachRegs.EDI),
			useFixed(i.inputs(3).dest, X86MachRegs.EDX));
		var upper_divisor = i.inputs(3).dest;
		var signed = IntType.!(orig.resultType).signed;
		var zeroCheck = !i.checkFact(Facts.O_NO_ZERO_CHECK) && rtsrc != null;
		var small_divisor = upper_divisor.checkFact(Facts.V_ZERO);
		var large_divisor = !signed && upper_divisor.checkFact(Facts.V_NON_ZERO);
		var op = WideDivision.new(mod, signed, large_divisor, small_divisor, zeroCheck);
		gen(op.name, asm_wdiv, (op, i.source));
		return !i.checkFact(Facts.O_NO_ZERO_CHECK);
	}
	def genWideArith(i: SsaApplyOp, name: string, asm: (int, int, int, int, int, int) -> (), mv: MachVar) -> bool {
		// TODO: loosen register requirements for 64-bit add/sub
		gen(name, asm, (
			dfnAt(vars.get(mv.varNum), X86MachRegs.EAX),
			dfnAt(vars.get(mv.varNum + 1), X86MachRegs.EDX),
			useFixed(i.input0(), X86MachRegs.EAX),
			useFixed(i.input1(), X86MachRegs.EDX),
			useFixed(i.inputs(2).dest, X86MachRegs.ECX),
			useFixed(i.inputs(3).dest, X86MachRegs.EBX)));
		return false;
	}
	def genWideShift(i: SsaApplyOp, name: string, shifter: X86Shifter, mv: MachVar) -> bool {
		// TODO: loosen register requirements for 64-bit shifts
		// TODO: reduce checks required in 64-bit shifts
		gen(name, asm_wsh, (shifter,
			dfnAt(vars.get(mv.varNum), X86MachRegs.EAX),
			dfnAt(vars.get(mv.varNum + 1), X86MachRegs.EDX),
			useFixed(i.input0(), X86MachRegs.EAX),
			useFixed(i.input1(), X86MachRegs.EDX),
			useFixed(i.inputs(2).dest, X86MachRegs.ECX),
			if (i.inputs.length > 3, useFixed(i.inputs(3).dest, X86MachRegs.EBX), -1)));
		return false;
	}
	def genIntConvert(i: SsaApplyOp, mv: MachVar) {
		var tt = IntType.!(i.op.resultType);
		if (tt.width == 32) {
			id(i, makeVar(i.input0()));
		} else if (!tt.signed) {
			var a = i.input0();
			gen("zext", asm_op2_i, (X86Assembler.and, dfn(mv), use(a), Int.unbox(tt.max), hint(a, mv)));
		} else {
			gen("sext", asm_sext, (dfn(mv), use(i.input0()), tt.width));
		}
	}
	def genAdd(i: SsaApplyOp, mv: MachVar) {
		var xe = i.inputs(0), y = i.input1(), x = xe.dest;
		if (SsaValue.?(y)) {
			// match x + K
			var disp = y.unbox<int>(), xscale = matchScale(xe);
			if (xscale.1 > 1) return gen_lea(mv, xscale.0, xscale.1, disp);
			else return gen_add_i(mv, x, disp);
		}
		// XXX: match K + b for absolute (mtable) addresses?
		// XXX: match a + (b * K) -> lea [a + b * K]
		// XXX: match a + (b * K) + K -> lea [a + b * K + K]
		gen_op2("add", true, X86Assembler.add, mv, x, y);
	}
	def genMul(i: SsaApplyOp, mv: MachVar) {
		// XXX: strength reduce here also, or just rely on local optimizer?
		var y = i.input1();
		if (SsaValue.?(y)) gen_imul_i(mv, i.input0(), y.unbox<int>());
		else gen_imul(mv, i.input0(), y);
	}
	def genOp2(name: string, i: SsaApplyOp, mv: MachVar, m: X86Assembler -> X86Op2) {
		var x = i.input0(), y = i.input1();
		if (SsaValue.?(y)) {
			// X <op> K
			var val = SsaValue.!(y).val;
			if (i.op.opcode == Opcode.IntXor) {
				if (Int.MINUS_1.equals(val)) gen_not(mv, x);
				else gen_op2_i(name, m, mv, x, val);
			} else {
				gen_op2_i(name, m, mv, x, val);
			}
		} else if (SsaValue.?(x)) {
			// K <op> Y
			var val = SsaValue.!(x).val;
			if (i.op.opcode == Opcode.IntSub) {
				if (val == null || val.equals(null)) gen_neg(mv, y);	// generate negation
				else gen_op2(name, false, m, mv, x, y); // not commutative
			} else if (i.op.opcode == Opcode.IntXor) {
				if (Int.MINUS_1.equals(val)) gen_not(mv, y);
				else gen_op2_i(name, m, mv, y, val);
			} else {
				gen_op2_i(name, m, mv, y, val);
			}
		} else {
			// gen a basic two-address operation
			var commutative = i.checkFact(Facts.O_COMMUTATIVE) || i.op.opcode == Opcode.PtrAdd;
			gen_op2(name, commutative, m, mv, x, y);
		}
	}
	def genCmp(cmp: X86CmpMatch) {
		if (cmp.y == null) {
			if (Addr.?(cmp.val)) gen_cmp_a(cmp.x, Addr.!(cmp.val));
			else gen_cmp_i(cmp.x, V3.unboxIntegral(cmp.val));
		} else {
			gen_cmp(cmp.x, cmp.y);
		}
	}
	def genCmpBr(cmp: X86CmpMatch, target: SsaBlock) {
		genCmp(cmp);
		gen_br(cmp.cond, target);
	}
	def genCmpSet(cmp: X86CmpMatch, mv: MachVar) {
		genCmp(cmp);
		gen_set(cmp.cond, mv);
	}
	def genLoad(i: SsaApplyOp, mv: MachVar) -> bool {
		var ptr = i.inputs(0);
		if (SsaValue.?(ptr.dest)) {
			// an absolute address
			var addr = Addr.!(val(ptr.dest));
			if (mach.layout.isValid(addr)) { // is valid?
				gen_loada(mv, i.op.typeArgs(1), addr);
				return false;
			}
			// invalid address, generate a throw
			gen("throw_null", asm_throw, (i.source, V3Exception.NullCheck));
			return true;
		}
		if (matchEdge(ptr, Opcode.PtrAdd)) {
			// load(x + y)
			var add = SsaApplyOp.!(ptr.dest);
			var y = add.input1();
			var x = add.inputs(0);
			if (SsaValue.?(y)) {
				var xscale = matchScale(x);
				return genLoadAddr(i, mv, xscale.0, xscale.1, y.unbox<int>());
			}
			if (SsaValue.?(x.dest)) {
				// load(<addr> + y)
				var addr = Addr.!(val(x.dest));
				gen_loada_disp(mv, i.op.typeArgs(1), addr, y);
				return !i.checkFact(Facts.O_NO_NULL_CHECK);
			}
		}
		return genLoadAddr(i, mv, ptr.dest, 1, 0);
	}
	def genLoadAddr(i: SsaApplyOp, mv: MachVar, x: SsaInstr, scale: int, disp: int) -> bool {
		var nullCheck = V3Op.needsNullCheck(i, x);
		gen_loadx(mv, i.op.typeArgs(1), x, scale, disp, nullCheck, i.source);
		return nullCheck;
	}
	def genStore(i: SsaApplyOp, mv: MachVar) -> bool {
		var ptr = i.inputs(0), v = i.input1();
		if (SsaValue.?(ptr.dest)) {
			// store to absolute address
			var addr = Addr.!(val(ptr.dest));
			gen_storex_abs(mach.sizeOf(i.op.typeArgs(1)), addr, v);
			// XXX: store immediate constants to absolute addresses
			return true;
		}
		if (matchEdge(ptr, Opcode.PtrAdd)) {
			var add = SsaApplyOp.!(ptr.dest);
			var y = add.input1();
			if (SsaValue.?(y)) {
				var xscale = matchScale(add.inputs(0));
				return genStoreAddr(i, mv, xscale.0, xscale.1, y.unbox<int>(), v);
			}
		}
		return genStoreAddr(i, mv, ptr.dest, 1, 0, v);
	}
	def genCmpSwp(i: SsaApplyOp, mv: MachVar) -> bool {
		var size = mach.sizeOf(i.op.typeArgs(1));
		var ptr = i.inputs(0), val = i.inputs(2).dest;
		if (size == 1 && SsaApplyOp.?(val)) {
			// eliminate int -> byte conversions in cmpswp
			var vop = SsaApplyOp.!(val);
			if (vop.op.opcode == Opcode.IntConvert) {
				var width = IntType.!(vop.op.resultType).width;
				if (width == 8) val = vop.input0();
			}
		}
		var dest = dfnAt(mv, X86MachRegs.BYTE);
		var expect = useFixed(i.input1(), X86MachRegs.EAX);
		if (SsaValue.?(ptr.dest)) {
			// cmpswp to absolute address
			var addr = Addr.!(this.val(ptr.dest));
			if (size == 4) gen("stored_abs", asm_cmpswpx_abs, (X86Assembler.cmpxchngd, dest, addr, gpr(val)));
			else if (size == 1) gen("storeb_abs", asm_cmpswpx_abs, (X86Assembler.cmpxchngb, dest, addr, byt(val)));
			else if (size == 2) gen("storew_abs", asm_cmpswpx_abs, (X86Assembler.cmpxchngw, dest, addr, gpr(val)));
			else context.fail("invalid size for cmpswp");
			return true;
		}
		var base = ptr.dest;
		var scale = 1, disp = 0;
		if (matchEdge(ptr, Opcode.PtrAdd)) {
			var add = SsaApplyOp.!(ptr.dest);
			var y = add.input1();
			if (SsaValue.?(y)) {
				var xscale = matchScale(add.inputs(0));
				base = xscale.0;
				scale = xscale.1;
				disp = y.unbox<int>();
			}
		}
		var canTrap = V3Op.needsNullCheck(i, base);
		if (size == 4) gen("cmpswpd", asm_cmpswpx, (X86Assembler.cmpxchngd, dest, gpr(base), scale, disp, gpr(val), canTrap, i.source));
		else if (size == 1) gen("cmpswpb", asm_cmpswpx, (X86Assembler.cmpxchngb, dest, gpr(base), scale, disp, byt(val), canTrap, i.source));
		else if (size == 2) gen("cmpswpw", asm_cmpswpx, (X86Assembler.cmpxchngw, dest, gpr(base), scale, disp, gpr(val), canTrap, i.source));
		else context.fail("invalid size for cmpswp");
		return true;
	}
	def genStoreAddr(i: SsaApplyOp, mv: MachVar, x: SsaInstr, scale: int, disp: int, val: SsaInstr) -> bool {
		var size = mach.sizeOf(i.op.typeArgs(1));
		if (size == 1 && SsaApplyOp.?(val)) {
			// eliminate int -> byte conversions in stores
			var vop = SsaApplyOp.!(val);
			if (vop.op.opcode == Opcode.IntConvert) {
				var width = IntType.!(vop.op.resultType).width;
				if (width == 8) val = vop.input0();
			}
		}
		var nullCheck = V3Op.needsNullCheck(i, x);
		if (SsaValue.?(val)) {
			// match stores of immediates
			var v = SsaValue.!(val).val;
			if (v == null) return gen_storex_i(size, x, scale, disp, 0, nullCheck, i.source);
			if (Box<int>.?(v)) return gen_storex_i(size, x, scale, disp, Int.unbox(v), nullCheck, i.source);
			if (Box<byte>.?(v)) return gen_storex_i(size, x, scale, disp, Byte.unbox(v), nullCheck, i.source);
			if (Box<bool>.?(v)) return gen_storex_i(size, x, scale, disp, Bool.toInt(Bool.unbox(v)), nullCheck, i.source);
			// XXX: store of immediate addresses
		}
		gen_storex(size, x, scale, disp, val, nullCheck, i.source);
		return true;
	}
	def genDiv(i: SsaApplyOp, mv: MachVar, div: bool) -> bool {
		dfnAt(mv, if(div, X86MachRegs.EAX, X86MachRegs.EDX));
		kill(if(div, X86MachRegs.EDX, X86MachRegs.EAX));
		useFixed(i.input0(), X86MachRegs.EAX);
		var bu = useFixed(i.input1(), X86MachRegs.NOT_EDX);
		var zeroCheck = !i.checkFact(Facts.O_NO_ZERO_CHECK);
		if (IntType.!(i.op.resultType).signed) {
			var negCheck = !i.checkFact(Facts.O_NO_DIV_CHECK);
			gen(if(div, "idiv", "imod"), asm_idiv, (i.source, zeroCheck, negCheck, bu, div));
		} else {
			gen(if(div, "udiv", "umod"), asm_udiv, (i.source, zeroCheck, bu));
		}
		return zeroCheck; // pure if no zero check
	}
	def genCondThrow(i: SsaApplyOp, mv: MachVar) -> bool {
		// generate a branch to the throw function
		var cmp = matchCmp(i.input0());
		genCmp(cmp);
		gen("br", asm_brthrow, (cmp.cond, i.op.attr<string>(), i.source));
		return true;
	}
	def genCall(call: SsaApplyOp, rv: MachVar) -> bool {
		var func = call.input0(), mi: MachInstr;
		var funcRep = call.op.attr<Mach_FuncRep>();
		var conv = frame.allocCallerSpace(X86VirgilCallConv.getForFunc(funcRep));

		// define the return value(s) of the call		
		for (i < rv.varSize) {
			dfnAt(vars.get(rv.varNum + i), conv.calleeRet(i));
		}
		var lp = if(rtgc != null, livePoint());
		kill(X86MachRegs.ALL);
		// use the arguments to the call
		var inputs = call.inputs, m = MachMoves.new();
		var adjust = if(funcRep.receiver, -1, 0); // adjust for receiver if necessary
		for (i = 1; i < inputs.length; i++) {
			useFixed(inputs(i).dest, conv.calleeParam(i + adjust));
		}
		if (SsaValue.?(func)) {
			// generate a (patchable) direct call
			mi = gen("call", asm_call, (conv, lp, Addr.!(val(func)), call.source));
		} else {
			// generate an indirect call
			mi = gen("icall", asm_icall, (conv, lp, gpr(func), call.source));
		}
		return true;
	}
	def genShift(name: string, i: SsaApplyOp, mv: MachVar, shifter: X86Shifter) {
		var x = i.input0(), y = i.input1();
		if (SsaValue.?(y)) {
			// shift by constant
			var imm = y.unbox<int>();
			if (imm >= 0 && imm < 32) {
				gen(name, asm_sh_i, (shifter, dfn(mv), use(x), imm, hint(x, mv)));
			} else {
				// XXX: use the valMoves machinery?
				gen_const(mv, null);
			}
		} else {
			// gen shift by cl
			var asm_meth = if(i.checkFact(Facts.O_NO_SHIFT_CHECK), asm_sh, asm_sh_checked);
			gen(name, asm_meth, (shifter, dfn(mv), useFixed(y, X86MachRegs.ECX), use(x), hint(x, mv)));
		}
	}
	def genReturn(i: SsaReturn) {
		for (j < i.inputs.length) {
			useFixed(i.inputs(j).dest, frame.conv.callerRet(j));
		}
		gen("ret", asm_ret, ());
	}
	def genGoto(target: SsaBlock) {
		if (target.phis != null) gen("phi_resolve", asm_phi_resolve, code.length);
		if (blocks.isImmediatelyAfter(context.block, target)) gen("nop", asm_nop, ());
		else gen_br(null, target);
	}
	def genJump(target: SsaBlock) {
		if (!blocks.isImmediatelyAfter(context.block, target)) gen_br(null, target);
	}
	def genThrow(i: SsaThrow) {
		gen("throw", asm_throw, (i.source, i.exception));
	}
	def genIf(i: SsaIf) {
		var key = i.input0(), cmp = matchCmp(key), succ = i.block.succ;
		var s0 = succ(0).dest, s1 = succ(1).dest;
		if (blocks.isImmediatelyAfter(context.block, s0)) {
			// can fall through to first successor
			genCmpBr(cmp.negate(), s1);
		} else {
			// branch to first successor
			genCmpBr(cmp, s0);
			genJump(s1);
		}
	}
	def genSwitch(i: SsaSwitch) {
		gen("sw", asm_tableswitch, (i, use(i.input0())));
	}

	def numVars(i: SsaInstr) -> int {
		// a call could have multiple return values
		return Tuple.length(i.getType());
	}
	// pattern-match a comparison
	def matchCmp(i: SsaInstr) -> X86CmpMatch {
		if (!inSameBlock(i) || !SsaApplyOp.?(i)) return X86CmpMatch.new(X86Conds.NZ, i, null, null);
		match (SsaApplyOp.!(i).op.opcode) {
			Equal:			return newCmp2(i, X86Conds.Z);
			NotEqual:		return newCmp2(i, X86Conds.NZ);
			IntLt:			return newCmp2(i, signedCmp(i, X86Conds.L, X86Conds.C));
			IntGt:			return newCmp2(i, signedCmp(i, X86Conds.G, X86Conds.A));
			IntLteq:		return newCmp2(i, signedCmp(i, X86Conds.LE, X86Conds.NA));
			IntGteq:		return newCmp2(i, signedCmp(i, X86Conds.GE, X86Conds.NC));
			UnsignedLt:		return newCmp2(i, X86Conds.C);
			UnsignedLteq:		return newCmp2(i, X86Conds.NA);
			UnsignedGt:		return newCmp2(i, X86Conds.A);
			UnsignedGteq:		return newCmp2(i, X86Conds.NC);
			BoolNot: {
				var cmp = matchCmp(i.input0());
				return X86CmpMatch.new(cmp.cond.negate, cmp.x, cmp.y, cmp.val);
			}
			_: ;
		}
		return X86CmpMatch.new(X86Conds.NZ, i, null, null);
	}
	def signedCmp(i: SsaInstr, signed: X86Cond, unsigned: X86Cond) -> X86Cond {
		var t = SsaApplyOp.!(i).op.paramTypes(0);
		match (t.typeCon.kind) {
			MachType.MACH_POINTER: return unsigned;
			V3Kind.INT: return if (IntType.!(t).signed, signed, unsigned);
		}
		return signed;
	}
	def matchScale(e: SsaDfEdge) -> (SsaInstr, int) {
		var i = e.dest;
		if (!soleEdge(e)) return (i, 1);
		var opcode = i.opcode();
		if (opcode == Opcode.IntMul.tag) {
			var y = i.input1();
			if (SsaValue.?(y)) { // match i = x * K
				var scale = y.unbox<int>();
				match (scale) {
					1, 2, 4, 8: return (i.input0(), scale);
				}
			}
		}
		else if (opcode == Opcode.IntShl.tag) {
			var y = i.input1();
			if (SsaValue.?(y)) { // match i = x #<< K
				var shift = y.unbox<int>();
				match (shift) {
					0, 1, 2, 3: return (i.input0(), 1 #<< shift);
				}
			}
		} else if (opcode == Opcode.IntAdd.tag) {
			var x = i.input0();
			if (i.input1() == x) return (x, 2); // i = x + x
		}
		return (i, 1);
	}
	def newCmp2(i: SsaInstr, cond: X86Cond) -> X86CmpMatch {
		var x = i.input0(), y = i.input1();
		if (SsaValue.?(y)) return X86CmpMatch.new(cond, x, null, val(y));
		if (SsaValue.?(x) && cond.commute != null) return X86CmpMatch.new(cond.commute, y, null, val(x));
		return X86CmpMatch.new(cond, x, y, null);
	}

	def gen_entry() {
		// add defs of each parameter as the first instruction
		var p = context.graph.params, max = p.length;
		var defs = Array<int>.new(max);
		for (i < max) {
			var pvar = makeVar(p(i)), loc = frame.conv.callerParam(i);
			if (frame.conv.regSet.isReg(loc)) {
				defs(i) = dfnAt(pvar, loc);
				pvar.hint = byte.!(loc); // register hint
			} else {
				defs(i) = dfnAt(pvar, loc);
				pvar.spill = loc; // reuse spill slot in caller frame
			}
		}
		gen("entry", asm_entry, defs).live = true;
	}
	def asm_entry(defs: Array<int>) {
		var adjust = frameAdjust();
		if (adjust > 0) asm.sub.rm_i(X86Regs.ESP, adjust); // allocate frame
	}

	// ---------- Loads ----------------------------------------------
	def gen_const(dest: MachVar, val: Val) {
		gen("const", asm_const, (dfn(dest), val));
	}
	def asm_const(dest: int, val: Val) {
		asm.movd_l_val(frame, loc(dest), val);
	}
	def gen_loadx(dest: MachVar, t: Type, base: SsaInstr, scale: int, disp: int, canTrap: bool, source: Source) {
		var size = mach.sizeOf(t);
		if (size == 0) {
			// load produces no value; only needed for possible nullcheck
			gen("test_i", asm_test_i, (gpr(base), scale, disp, 0, canTrap, source));
			return;
		}
		var lt = matchLoad(t, size);
		gen(lt.0, asm_loadx, (lt.1, dfngpr(dest), gpr(base), scale, disp, canTrap, source));
	}
	def gen_loada(dest: MachVar, t: Type, addr: Addr) {
		var lt = matchLoad(t, mach.sizeOf(t));
		gen(lt.0, asm_loada, (lt.1, dfngpr(dest), addr));
	}
	def gen_loada_disp(dest: MachVar, t: Type, addr: Addr, disp: SsaInstr) {
		var lt = matchLoad(t, mach.sizeOf(t));
		gen(lt.0, asm_loada_disp, (lt.1, dfngpr(dest), addr, gpr(disp)));
	}
	def matchLoad(t: Type, size: int) -> (string, (X86Assembler, X86Reg, X86Rm) -> void) {
		// choose appropriate X86 load based on size and signedness
		if (size == 4) return ("loadd", X86Assembler.movd_r_rm);
		if (size == 1) return if(V3.isSigned(t),
				("loadsb", X86Assembler.movbsx),
				("loadub", X86Assembler.movbzx));
		if (size == 2) return if(V3.isSigned(t),
				("loadsw", X86Assembler.movwsx),
				("loaduw", X86Assembler.movwzx));
		context.fail("invalid size for load");
		return ("invalid", X86Assembler.movd_r_rm);
	}
	def asm_loadx(m: (X86Assembler, X86Reg, X86Rm) -> void, dest: int, base: int, scale: int, disp: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, r(dest), X86Addr.new(r(base), scale, disp));
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def asm_loada(m: (X86Assembler, X86Reg, X86Rm) -> void, dest: int, addr: Addr) {
		var pos = asm.pos();
		m(asm, r(dest), X86Addrs.ABS_PATCH);
		recordPatch(pos, addr);
	}
	def asm_loada_disp(m: (X86Assembler, X86Reg, X86Rm) -> void, dest: int, addr: Addr, disp: int) {
		var pos = asm.pos();
		m(asm, r(dest), r(disp).plus(X86Addrs.ABS_CONST));
		recordPatch(pos, addr);
	}
	// ---------- Stores ----------------------------------------------
	def gen_storex(size: int, base: SsaInstr, scale: int, disp: int, val: SsaInstr, canTrap: bool, source: Source) {
		if (size == 4) gen("stored", asm_storex, (X86Assembler.movd_rm_r, gpr(base), scale, disp, gpr(val), canTrap, source));
		else if (size == 1) gen("storeb", asm_storex, (X86Assembler.movb_rm_r, gpr(base), scale, disp, byt(val), canTrap, source));
		else if (size == 0) gen("test_i", asm_test_i, (gpr(base), scale, disp, 0, canTrap, source));
		else if (size == 2) gen("storew", asm_storex, (X86Assembler.movw_rm_r, gpr(base), scale, disp, gpr(val), canTrap, source));
		else context.fail("invalid size for store");
	}
	def gen_storex_i(size: int, base: SsaInstr, scale: int, disp: int, val: int, canTrap: bool, source: Source) -> bool {
		if (size == 4) gen("stored_i", asm_storex_i, (X86Assembler.movd_rm_i, gpr(base), scale, disp, val, canTrap, source));
		else if (size == 1) gen("storeb_i", asm_storex_i, (X86Assembler.movb_rm_i, gpr(base), scale, disp, val, canTrap, source));
		else if (size == 0) gen("test_i", asm_test_i, (gpr(base), scale, disp, 0, canTrap, source));
		else if (size == 2) gen("storew_i", asm_storex_i, (X86Assembler.movw_rm_i, gpr(base), scale, disp, val, canTrap, source));
		else context.fail("invalid size for store");
		return true;
	}
	def asm_storex(m: (X86Assembler, X86Rm, X86Reg) -> void, base: int, scale: int, disp: int, val: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, X86Addr.new(r(base), scale, disp), r(val));
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def asm_cmpswpx(m: (X86Assembler, X86Rm, X86Reg) -> void, dest: int, base: int, scale: int, disp: int, val: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, X86Addr.new(r(base), scale, disp), r(val));
		var reg = r(dest);
		asm.setz(reg);
		asm.movbzx(reg, reg);  // XXX: make zero extend unnecessary
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	def gen_storex_abs(size: int, addr: Addr, val: SsaInstr) {
		if (size == 4) gen("stored_abs", asm_storex_abs, (X86Assembler.movd_rm_r, addr, gpr(val)));
		else if (size == 1) gen("storeb_abs", asm_storex_abs, (X86Assembler.movb_rm_r, addr, byt(val)));
		else if (size == 2) gen("storew_abs", asm_storex_abs, (X86Assembler.movw_rm_r, addr, gpr(val)));
		else context.fail("invalid size for store");
	}
	def asm_storex_abs(m: (X86Assembler, X86Rm, X86Reg) -> void, addr: Addr, val: int) {
		var pos = asm.pos();
		m(asm, X86Addrs.ABS_PATCH, r(val));
		recordPatch(pos, addr);
	}
	def asm_cmpswpx_abs(m: (X86Assembler, X86Rm, X86Reg) -> void, dest: int, addr: Addr, val: int) {
		var pos = asm.pos();
		m(asm, X86Addrs.ABS_PATCH, r(val));
		var reg = r(dest);
		asm.setz(reg);
		asm.movbzx(reg, reg);  // XXX: make zero extend unnecessary
		recordPatch(pos, addr);
	}
	def asm_storex_i(m: (X86Assembler, X86Rm, int) -> void, base: int, scale: int, disp: int, imm: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		m(asm, X86Addr.new(r(base), scale, disp), imm);
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	// ---------- Lea and test --------------------------------------------------
	def gen_lea(dest: MachVar, base: SsaInstr, scale: int, disp: int) {
		gen("lea", asm_lea, (dfngpr(dest), gpr(base), scale, disp));
	}
	def asm_lea(dest: int, base: int, scale: int, disp: int) {
		asm.lea(r(dest), X86Addr.new(r(base), scale, disp));
	}
	def asm_test_i(base: int, scale: int, disp: int, imm: int, canTrap: bool, source: Source) {
		var off = asm.codeOffset();
		asm.test_rm_i(X86Addr.new(r(base), scale, disp), imm);
		if (canTrap && rtsrc != null) rtsrc.recordSource(off, source);
	}
	// ---------- Basic operations ----------------------------------------------
	def gen_op2(name: string, commutative: bool, m: X86Assembler -> X86Op2, dest: MachVar, a: SsaInstr, b: SsaInstr) {
		var f = if(commutative, asm_op2_comm, asm_op2);
		gen(name, f, (m, dfngpr(dest), use(a), use(b), hint(a, dest)));
	}
	// ---------- Basic operations (immediate) ---------------------------------
	def gen_op2_i(name: string, m: X86Assembler -> X86Op2, dest: MachVar, a: SsaInstr, imm: Val) {
		if (imm == null || Box<int>.?(imm)) gen(name, asm_op2_i, (m, dfn(dest), use(a), Int.unbox(imm), hint(a, dest)));
		else if (Addr.?(imm)) gen(name, asm_op2_a, (m, dfn(dest), use(a), Addr.!(imm), hint(a, dest)));
		else if (Box<bool>.?(imm)) gen(name, asm_op2_i, (m, dfn(dest), use(a), Bool.toInt(Bool.unbox(imm)), hint(a, dest)));
		else context.fail1("not a valid immediate: %1", mach.prog.renderResult(imm, null, _));
	}
	def gen_add_i(dest: MachVar, a: SsaInstr, imm: int) {
		gen("add_i", asm_add_i, (dfn(dest), use(a), imm, hint(a, dest)));
	}
	def gen_neg(dest: MachVar, a: SsaInstr) {
		gen("neg", asm_neg, (dfn(dest), use(a), hint(a, dest)));
	}
	def gen_not(dest: MachVar, a: SsaInstr) {
		gen("not", asm_not, (dfn(dest), use(a), hint(a, dest)));
	}
	def gen_add_a(dest: MachVar, a: SsaInstr, addr: Addr) {
		gen("add_a", asm_op2_a, (X86Assembler.add, dfn(dest), use(a), addr, hint(a, dest)));
	}
	def asm_op2(m: X86Assembler -> X86Op2, dest: int, a: int, b: int, v: void) {
		dest = loc(dest); a = loc(a); b = loc(b);
		if (dest != a) {
			if (dest == b) {
				asm_movd_l_l(b, frame.conv.regSet.scratch);
				b = frame.conv.regSet.scratch;
			}
			asm_movd_l_l(a, dest);
		}
		m(asm).r_rm(loc_r(dest), loc_rm(b));
	}
	def asm_op2_comm(m: X86Assembler -> X86Op2, dest: int, a: int, b: int, v: void) {
		dest = loc(dest); a = loc(a); b = loc(b);
		if (dest == a) return m(asm).r_rm(loc_r(dest), loc_rm(b)); // op(dest = a, b)
		if (dest == b) return m(asm).r_rm(loc_r(dest), loc_rm(a)); // op(dest = b, a)
		if (dest != a) asm_movd_l_l(a, dest);
		m(asm).r_rm(loc_r(dest), loc_rm(b));
	}
	def asm_op2_i(m: X86Assembler -> X86Op2, dest: int, a: int, imm: int, v: void) {
		m(asm).rm_i(resolveOp1(dest, a), imm);
	}
	def asm_add_i(dest: int, a: int, imm: int, v: void) {
		dest = loc(dest); a = loc(a);
		if (dest != a) {
			if (frame.conv.regSet.isReg(dest) && frame.conv.regSet.isReg(a)) return asm.lea(loc_r(dest), loc_r(a).plus(imm));
			asm_movd_l_l(a, dest);
		}
		asm.add.rm_i(loc_rm(dest), imm);
	}
	def asm_op2_a(m: X86Assembler -> X86Op2, dest: int, a: int, addr: Addr, v: void) {
		var drm = resolveOp1(dest, a), pos = asm.pos();
		m(asm).rm_i(drm, X86Addrs.ABS_CONST);
		recordPatch(pos, addr);
	}
	def gen_cmp(a: SsaInstr, b: SsaInstr) {
		gen("cmp", asm_cmp, (gpr(a), use(b)));
	}
	def gen_cmp_i(a: SsaInstr, imm: int) {
		gen("cmp_i", asm_cmp_i, (use(a), imm));
	}
	def gen_cmp_a(a: SsaInstr, addr: Addr) {
		gen("cmp_a", asm_cmp_a, (use(a), addr));
	}
	def asm_cmp(a: int, b: int) {
		asm.cmp.r_rm(r(a), rm(b));
	}
	def asm_cmp_i(a: int, imm: int) {
		asm.cmp.rm_i(rm(a), imm);
	}
	def asm_cmp_a(a: int, addr: Addr) {
		var pos = asm.pos();
		asm.cmp.rm_i(rm(a), X86Addrs.ABS_CONST);
		recordPatch(pos, addr);
	}
	def asm_not(dest: int, a: int, v: void) {
		asm.not(resolveOp1(dest, a));
	}
	def asm_neg(dest: int, a: int, v: void) {
		asm.neg(resolveOp1(dest, a));
	}
	def resolveOp1(dest: int, a: int) -> X86Rm {
		dest = loc(dest); a = loc(a);
		if (dest != a) asm_movd_l_l(a, dest);
		return loc_rm(dest);
	}
	// ---------- Shift operations ---------------------------------------------
	def asm_sh(shifter: X86Shifter, dest: int, cl: int, a: int, v: void) {
		shifter.sh_cl(asm, resolveOp1(dest, a));
	}
	def asm_sh_checked(shifter: X86Shifter, dest: int, cl: int, a: int, v: void) {
		shifter.sh_checked(asm, rm(dest), rm(a), X86MachRegs.SCRATCH);
	}
	def asm_sh_i(shifter: X86Shifter, dest: int, a: int, imm: int, v: void) {
		shifter.sh_i(asm, resolveOp1(dest, a), imm);
	}
	def asm_sext(dest: int, a: int, size: int) {
		var d = resolveOp1(dest, a);
		asm.shl_i(d, 32 - size);
		asm.sar_i(d, 32 - size);
	}
	// ---------- Multiply / Divide -------------------------------------------
	def gen_imul (dest: MachVar, a: SsaInstr, b: SsaInstr) {
		gen("imul", asm_imul, (dfngpr(dest), use(a), use(b), hint(a, dest)));
	}
	def asm_imul(dest: int, a: int, b: int, v: void) {
		dest = loc(dest); a = loc(a); b = loc(b);
		if (dest == a) return asm.imul_r_rm(loc_r(dest), loc_rm(b)); // op(dest = a, b)
		if (dest == b) return asm.imul_r_rm(loc_r(dest), loc_rm(a)); // op(dest = b, a)
		if (dest != a) asm_movd_l_l(a, dest);
		asm.imul_r_rm(loc_r(dest), loc_rm(b));
	}
	def gen_imul_i(dest: MachVar, a: SsaInstr, imm: int) {
		gen("imul_i", asm_imul_i, (dfngpr(dest), use(a), imm, hint(a, dest)));
	}
	def asm_imul_i(dest: int, a: int, imm: int, v: void) {
		dest = loc(dest); a = loc(a);
		if (dest != a) asm_movd_l_l(a, dest);
		asm.imul_r_i(loc_r(dest), imm);
	}
	def asm_idiv(source: Source, zeroCheck: bool, negCheck: bool, a: int, div: bool) {
		asm.idivmod_checked(source, zeroCheck && rtsrc != null, negCheck, rm(a), div);
	}
	def asm_udiv(source: Source, zeroCheck: bool, a: int) {
		asm.udivmod_checked(source, zeroCheck && rtsrc != null, rm(a));
	}
	def gen_set(cond: X86Cond, a: MachVar) {
		gen("set", asm_set, (cond, dfnAt(a, X86MachRegs.BYTE)));
	}
	def asm_set(cond: X86Cond, a: int) {
		var reg = r(a);
		asm.setx(cond, reg);
		asm.movbzx(reg, reg); // XXX: make movzbx unnecessary
	}
	def gen_br(cond: X86Cond, target: SsaBlock) {
		gen(if(cond == null, "jmp", "br"), asm_br, (cond, target));
	}
	def asm_br(cond: X86Cond, target: SsaBlock) {
		asm.jmpx(cond, X86Addrs.REL_CONST);
		recordBranch(asm.pos() - 4, target);
	}
	def asm_brthrow(cond: X86Cond, ex: string, source: Source) {
		asm.jmpx_addr(cond, rt.getExceptionDest(asm.codeOffset(), ex, source));
	}
	def gen_phi_dfn(phis: SsaPhis) {
		phiList = List.new(phis, phiList);
		gen("phi_dfn", asm_nop, ()).live = true;
	}
	def asm_nop() {
		// do nothing.
	}
	def asm_wsh(shifter: X86Shifter, dl: int, dh: int, al: int, ah: int, sl: int, sh: int) {
		// Assumes sl == ECX
		if (sh == -1)  shifter.wsh_checked(asm, r(dl), r(dh), null);
		else shifter.wsh_checked(asm, r(dl), r(dh), r(sh));
	}
	def asm_wadd(dl: int, dh: int, al: int, ah: int, bl: int, bh: int) {
		// Assumes dl == al, dh == ah
		asm.add.rm_r(r(dl), r(bl));
		asm.adc.rm_r(r(dh), r(bh));
	}
	def asm_wsub(dl: int, dh: int, al: int, ah: int, bl: int, bh: int) {
		// Assumes dl == al, dh == ah
		asm.sub.rm_r(r(dl), r(bl));
		asm.sbb.rm_r(r(dh), r(bh));
	}
	def asm_wmul(dl: int, dh: int, k: int, al: int, ah: int, bl: int, bh: int) {
		// Assumes dl == al, dh == ah, al = %eax, ah = %edx
		var alr = r(dl), ahr = r(dh), blr = r(bl), bhr = r(bh);
		asm.imul_r_rm(bhr, alr);
		asm.imul_r_rm(ahr, blr);
		asm.add.r_rm(bhr, ahr);
		asm.mul(blr);
		asm.add.r_rm(ahr, bhr);
	}
	def asm_wdiv(op: WideDivision, source: Source) {
		asm.wdiv(op, source);
	}
	def asm_caller_ip(dst: int) {
		var d = rm(dst);
		if (X86Reg.?(d)) asm.movd_r_rm(X86Reg.!(d), X86Regs.ESP.plus(frameAdjust()));
		else {
			asm.movd_r_rm(scratchReg, X86Regs.ESP.plus(frame.size()));
			asm.movd_rm_r(d, scratchReg);
		}
	}
	def asm_caller_sp(dst: int) {
		var d = rm(dst);
		if (X86Reg.?(d)) asm.lea(X86Reg.!(d), X86Regs.ESP.plus(frame.size()));
		else {
			asm.lea(scratchReg, X86Regs.ESP.plus(frame.size()));
			asm.movd_rm_r(d, scratchReg);
		}
	}
	def asm_phi_resolve(pos: int) {
		// do nothing. all moves should be inserted by main emit() loop
	}
	def asm_ret() {
		var adjust = frameAdjust();
		if (adjust > 0) asm.add.rm_i(X86Regs.ESP, adjust); // deallocate frame
		asm.ret();
	}
	def asm_call(conv: MachCallConv, lp: int, target: Addr, source: Source) {
		asm.call_addr(target);
		var off = asm.codeOffset();
		if (rtgc != null) rtgc.recordStackRefMap(off, source, buildStackMap(off, conv, lp));
		if (rtsrc != null) rtsrc.recordReturnSource(off, source);
	}
	def asm_icall(conv: MachCallConv, lp: int, addr: int, source: Source) {
		// TODO: record canTrap for indirect calls
		asm.icall(rm(addr));
		var off = asm.codeOffset();
		if (rtgc != null) rtgc.recordStackRefMap(off, source, buildStackMap(off, conv, lp));
		if (rtsrc != null) rtsrc.recordReturnSource(off, source);
	}
	def buildStackMap(off: int, conv: MachCallConv, lp: int) -> int {
		var builder = rtgc.beginRefMap(frame.slots(), 20);
		if (conv != null && conv.overflow > 0) {
			// record any outgoing overflow parameters that are references
			var pt = conv.paramTypes, rs = conv.regSet;
			for (i < pt.length) {
				var ploc = conv.calleeParam(i);
				if (rs.isStack(ploc) && mach.isRefType(pt(i))) {
					builder.setRefMap(ploc - rs.calleeStart);
				}
			}
		}
		// lazily compute the reference slot index for each live MachVar
		if (varRefSlotIndex == null) computeVarRefSlotIndex();
		var width = lsra.livemap.width, start = width * lp;
		var bits = lsra.livemap.bits;
		// for each live variable, set the appropriate bit (if any) in the stackmap
		for (i < width) {
			var vnum = i * 32;
			for (b = bits(i + start); b != 0; b = b #>> 1) {
				if ((b & 1) != 0) { // variable is live
					var refSlot = varRefSlotIndex(vnum);
					if (refSlot > 0) builder.setRefMap(refSlot - 1 + frame.spillArgs);
				}
				vnum++;
			}
		}
		return builder.finishRefMap();
	}
	// compute the reference slot index for each (live) MachVar
	def computeVarRefSlotIndex() {
		// XXX: pull this code up to MachCodeGen--currently depends on frame layout
		varRefSlotIndex = Array<int>.new(vars.length);
		var regSet = frame.conv.regSet;
		for (i = 0; i < vars.length; ()) {
			var machVar = vars.get(i);
			if (machVar.varSize > 1) {
				// set reference slots for all sub-variables
				var types = Tuple.toTypeArray(machVar.ssa.getType());
				for (j < machVar.varSize) {
					var subVar = vars.get(i + j);
					if (subVar.live) setVarRefSlot(subVar, types(j));
				}
				i = i + machVar.varSize;
			} else if (machVar.live && machVar.ssa != null) {
				// set a reference slot for this live mach var
				setVarRefSlot(machVar, machVar.ssa.getType());
				i++;
			} else {
				i++;
			}
		}
	}
	def setVarRefSlot(machVar: MachVar, t: Type) {
		if (machVar.live && mach.isRefType(t)) {
			var spill = machVar.spill, regSet = frame.conv.regSet;
			if (spill >= regSet.spillStart && spill < regSet.callerStart) {
				varRefSlotIndex(machVar.varNum) = 1 + spill - regSet.spillStart;
			}
		}
	}
	def asm_throw(source: Source, ex: string) {
		asm.jmpx_addr(null, rt.getExceptionDest(asm.codeOffset(), ex, source));
	}
	def asm_tableswitch(sw: SsaSwitch, val: int) {
		var rm = rm(val), size = sw.size();
		asm.movd_r_rm(scratchReg, rm);
		if (sw.minValue != 0) asm.sub.rm_i(scratchReg, sw.minValue);
		asm.cmp.rm_i(scratchReg, size - 1);
		asm.ja(X86Addrs.REL_CONST);
		recordBranch(asm.pos() - 4, sw.default().dest);
		// load from the jump table to follow
		var start = asm.pos();
		asm.movd_r_rm(scratchReg, X86Addr.new(scratchReg, 4, X86Addrs.ABS_CONST));
		var jtaddrpos = asm.findAbsConst(start);
		asm.ijmp(scratchReg);
		// align and emit the jump table
		asm.encoder.align(4);
		var jumpTableAddr = asm.machEncoder.posAddr();
		asm.encoder.at(jtaddrpos).i4(jumpTableAddr);
		asm.encoder.atEnd();
		// emit jump table
		jumpTables = List.new((asm.pos(), sw), jumpTables);
		for (i < size) {
			asm.encoder.i4(0);
		}
	}

	// generates a single X86Instr and adds it to the "code" sequence
	def gen<T>(name: string, f: T -> void, params: T) -> MachInstr {
		return addInstr(X86Instr.new(name, f, params));
	}

	def gpr(a: SsaInstr) -> int {
		return useAt(makeVar(a), X86MachRegs.GPR);
	}
	def dfngpr(a: MachVar) -> int {
		return dfnAt(a, X86MachRegs.GPR);
	}
	def byt(a: SsaInstr) -> int {
		return useAt(makeVar(a), X86MachRegs.BYTE);
	}
	def use(a: SsaInstr) -> int {
		return useAt(makeVar(a), 0);
	}
	def useFixed(a: SsaInstr, fixed: int) -> int {
		return useAt(makeVar(a), fixed);
	}
	def dfn(a: MachVar) -> int {
		return dfnAt(a, 0);
	}
	// get the location assignment for a use position
	def loc(usepos: int) -> int {
		return lsra.getAssignment(usepos);
	}
	// convert a use into an x86 register
	def r(usepos: int) -> X86Reg {
		return loc_r(loc(usepos));
	}
	// convert a use into an x86 register/memory
	def rm(usepos: int) -> X86Rm {
		return loc_rm(loc(usepos));
	}
	def recordBranch(pos: int, target: SsaBlock) {
		branches = List.new((pos, target), branches);
	}
	def print() {
		if (lsra != null) lsra.print();
	}
	// amount to adjust the frame at the beginning and end of invocation
	def frameAdjust() -> int {
		return frame.size() - mach.code.addressSize; // assumes return address already pushed
	}
	def allocMoveTmp() -> int {
		return frame.conv.regSet.scratch;
	}
	def loc_r(loc: int) -> X86Reg {
		return asm.loc_r(frame, loc); // XXX: inline
	}
	def loc_rm(loc: int) -> X86Rm {
		return asm.loc_rm(frame, loc); // XXX: inline?
	}
	def asm_movd_l_l_print(src: int, dst: int) {
		var pos = asm.pos();
		asm_movd_l_l(src, dst);
		var size = asm.pos() - pos;
	}
	def asm_movd_l_l(src: int, dst: int) {
		if (src <= 0) return; // nothing to do
		if (src == dst) return; // nothing to do
		asm.movd_rm_rm(asm.loc_rm(frame, dst), asm.loc_rm(frame, src), null);
	}
	def recordPatch(start: int, addr: Addr) {
		asm.recordPatch(start, addr);
	}
	def emitMoves(mr: MoveResolver) {
		if (mr != null) mr.genMoves(allocMoveTmp, asm_movd_l_l);
	}
	def emitVarMoves(list: List<(int, int)>) {
		var mr = MoveResolver.new(context.ERROR);
		for (l = list; l != null; l = l.tail) mr.addMove(loc(l.head.0), loc(l.head.1));
		mr.genMoves(allocMoveTmp, asm_movd_l_l);
	}
	def emitValMoves(list: List<(Val, int)>) {
		for (l = list; l != null; l = l.tail) asm.movd_l_val(frame, loc(l.head.1), l.head.0);
	}
}
