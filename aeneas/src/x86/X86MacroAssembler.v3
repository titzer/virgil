// Copyright 2014 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def eax = X86Regs.EAX;
def ebx = X86Regs.EBX;
def ecx = X86Regs.ECX;
def edx = X86Regs.EDX;
def edi = X86Regs.EDI;
def esi = X86Regs.ESI;
def ebp = X86Regs.EBP;
def esp = X86Regs.ESP;
def xmm0 = X86Regs.XMM0;
def xmm1 = X86Regs.XMM1;
def xmm2 = X86Regs.XMM2;
def xmm3 = X86Regs.XMM3;
def xmm4 = X86Regs.XMM4;
def xmm5 = X86Regs.XMM5;
def xmm6 = X86Regs.XMM6;
def xmm7 = X86Regs.XMM7;

def Regs: X86RegSet;

def gen_i64_divmod_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int, op: WideDivision) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.wdiv_full(op, null, true);
	asm.ret();
}
def gen_f2i_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.f2i_fixup();
	asm.ret();
}
def gen_d2i_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.d2i_fixup();
	asm.ret();
}
def gen_d2l_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.d2l_fixup();
	asm.ret();
}
def gen_ui2f_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.ui2f_fixup();
	asm.ret();
}
def gen_ui2d_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.ui2d_fixup();
	asm.ret();
}
def gen_f2ui_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.f2ui_fixup();
	asm.ret();
}
def gen_d2ui_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.d2ui_fixup();
	asm.ret();
}
def gen_d2ul_fixup_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.d2ul_fixup();
	asm.ret();
}
def gen_d2ul_stub(mach: MachProgram, addr: Addr, w: MachDataWriter, codeStartOffset: int) {
	var asm = X86MacroAssembler.new(mach, w, codeStartOffset);
	asm.d2ul();
	asm.ret();
}

// An extended X86Assembler that has additional machine-level utilities, such as
// recording patch locations and translating between regset locations and x86
// registers/memory.
class X86MacroAssembler extends X86Assembler {
	def mach: MachProgram;		  // machine program
	def machBuffer: MachDataWriter; // machine-level buffer
	def codeStartOffset: int;

	new(mach, machBuffer, codeStartOffset) super(machBuffer) { }
	def codeOffset() -> int {
		return w.pos - codeStartOffset;
	}
	// call an absolute address and record the patch location
	def call_addr(addr: Addr) {
		call(X86Addrs.REL_CONST);
		machBuffer.recordPatch(addr, machBuffer.pos - 4);
	}
	// jump (conditionally) to an absolute address and record the patch location
	def jmpx_addr(cond: X86Cond, addr: Addr) {
		jmpx(cond, X86Addrs.REL_CONST);
		machBuffer.recordPatch(addr, machBuffer.pos - 4);
	}
	// a macro to move between any two register/memory operands
	def movd_rm_rm(d: X86Rm, s: X86Rm, scratch: X86Reg) {
		var sse_scratch = Regs.SSE_SCRATCH;
		if (s == d) return;
		if (X86Reg.?(d)) return movd_r_rm(X86Reg.!(d), s);
		if (X86Reg.?(s)) return movd_rm_r(d, X86Reg.!(s));
		if (scratch != null) {
			movd_r_rm(scratch, s);
			movd_rm_r(d, scratch);
		}
		movd_s_rm(sse_scratch, s);
		movd_rm_s(d, sse_scratch);
	}
	// a macro to move between two 64-bit memory operands
	def movq_m_m(d: SSERm, s: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		if (s == d) return;
		if (SSEReg.?(s)) mach.fail(Strings.format1("src: %q is not an SSEAddr", s.render));
		else if (SSEReg.?(d)) mach.fail(Strings.format1("dst: %q is not an SSEAddr", d.render));
		movq_s_sm(sse_scratch, s);
		movq_sm_s(d, sse_scratch);
	}
	// a macro to move between any two double-precision SSE register/memory operands
	def movsd_sm_sm(d: SSERm, s: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		if (s == d) return;
		if (SSEReg.?(d)) return movsd_s_sm(SSEReg.!(d), s);
		if (SSEReg.?(s)) return movsd_sm_s(d, SSEReg.!(s));
		movsd_s_sm(sse_scratch, s);
		movsd_sm_s(d, sse_scratch);
	}
	// a macro to move between any two single-precision SSE register/memory operands
	def movss_sm_sm(d: SSERm, s: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		if (s == d) return;
		if (SSEReg.?(d)) return movss_s_sm(SSEReg.!(d), s);
		if (SSEReg.?(s)) return movss_sm_s(d, SSEReg.!(s));
		movss_s_sm(sse_scratch, s);
		movss_sm_s(d, sse_scratch);
	}
	// a macro to move a value into a location
	def movd_l_val(frame: MachFrame, loc: int, val: Val) {
		if (Regs.isSSEReg(loc)) {
			movd_l_fval(frame, loc, val);
		} else {
			var d = loc_rm(frame, loc);
			match (val) {
				x: Addr => {
					var pos = w.pos;
					movd_rm_i(d, X86Addrs.ABS_CONST);
					recordPatch(pos, x);
				}
				x: Record => {
					var pos = w.pos;
					movd_rm_i(d, X86Addrs.ABS_CONST);
					recordPatch(pos, mach.addrOfRecord(x));
				}
				x: Float32Val => movd_l_fval(frame, loc, x);
				x: Float64Val => movd_l_fval(frame, loc, x);
				_ => movd_rm_i(d, V3.unboxI32(val));
			}
		}
	}
	// a macro to move a floating point value into an SSE location
	def movd_l_fval(frame: MachFrame, loc: int, val: Val) {
		var d = loc_sm(frame, loc);
		if (val != null) {
			match (val) {
				x: Float32Val => {
					var f = int.view(x.bits);
					movd_rm_i(ebp, f);
					if (SSEReg.?(d)) movd_s_rm(SSEReg.!(d), ebp);
					else movd_rm_r(SSEAddr.!(d).toX86Addr(), ebp);
				}
				x: Float64Val => { // XXX: use MachProgram.longMap
					var fl = int.view(x.bits);
					var fh = int.view(x.bits >> 32);
					if (SSEReg.?(d)) {
						if (fl == 0x0 && fh == 0x0) {
							xorpd(SSEReg.!(d), SSEReg.!(d));
						} else {
							push_i(fh);
							push_i(fl);
							movq_s_sm(SSEReg.!(d), esp.plusSSE(0x0));
							add.rm_i(esp, 0x8);
						}
					} else {
						var addr_low = SSEAddr.!(d).toX86Addr();
						var addr_high = X86Addr.new(addr_low.base, addr_low.index, addr_low.scale, addr_low.disp + 0x4);
						movd_rm_i(addr_low, fl);
						movd_rm_i(addr_high, fh);
					}
			  }
			}
		} else { // default value is 0
			if (SSEReg.?(d)) xorpd(SSEReg.!(d), SSEReg.!(d));
			else {
				var sse_scratch = Regs.SSE_SCRATCH;
				xorpd(sse_scratch, sse_scratch);
				movsd_sm_s(SSEAddr.!(d), sse_scratch);
			}
		}
	}
	// convert a location into an x86 register
	def loc_r(frame: MachFrame, loc: int) -> X86Reg {
		match (loc) {
			Regs.EAX => return eax;
			Regs.EBX => return ebx;
			Regs.ECX => return ecx;
			Regs.EDX => return edx;
			Regs.ESI => return esi;
			Regs.EDI => return edi;
			Regs.EBP => return ebp;
		}
		return failLocation("required x86 register", loc, frame.conv.regSet);
	}
	// convert a location into an x86 register/memory reference
	def loc_rm(frame: MachFrame, loc: int) -> X86Rm {
		match (loc) {
			0 => return failLocation("unassigned location", loc, frame.conv.regSet);
			Regs.EAX => return eax;
			Regs.EBX => return ebx;
			Regs.ECX => return ecx;
			Regs.EDX => return edx;
			Regs.ESI => return esi;
			Regs.EDI => return edi;
			Regs.EBP => return ebp;
		}
		if (loc > 0) loc &= ~(frame.IS_64);
		var offset = calc_offset(frame, loc, false);
		return esp.plus(offset);
	}
	def failLocation(msg: string, loc: int, regSet: MachRegSet) -> X86Reg {
		mach.fail(Strings.format2("%s: %s", msg, regSet.identify(loc)));
		return Regs.SCRATCH;
	}
	// convert a location into an SSE register
	def loc_s(frame: MachFrame, loc: int) -> SSEReg {
		match (loc) {
			Regs.XMM0 => return xmm0;
			Regs.XMM1 => return xmm1;
			Regs.XMM2 => return xmm2;
			Regs.XMM3 => return xmm3;
			Regs.XMM4 => return xmm4;
			Regs.XMM5 => return xmm5;
			Regs.XMM6 => return xmm6;
			Regs.XMM7 => return xmm7;
		}
		return failSSELocation("required SSE register", loc, frame.conv.regSet);
	}
	// convert a location into an SSE register/memory reference
	def loc_sm(frame: MachFrame, loc: int) -> SSERm {
		match (loc) {
			0 => return failSSELocation("unassigned location", loc, frame.conv.regSet);
			Regs.XMM0 => return xmm0;
			Regs.XMM1 => return xmm1;
			Regs.XMM2 => return xmm2;
			Regs.XMM3 => return xmm3;
			Regs.XMM4 => return xmm4;
			Regs.XMM5 => return xmm5;
			Regs.XMM6 => return xmm6;
			Regs.XMM7 => return xmm7;
		}
		if (loc > 0) loc &= ~(frame.IS_64);
		var offset = calc_offset(frame, loc, true);
		return esp.plusSSE(offset);
	}
	def failSSELocation(msg: string, loc: int, regSet: MachRegSet) -> SSEReg {
		mach.fail(Strings.format2("%s: %s", msg, regSet.identify(loc)));
		return Regs.SSE_SCRATCH;
	}
	def calc_offset(frame: MachFrame, loc: int, isSSE: bool) -> int {
		var regSet = frame.conv.regSet, wordSize = mach.data.addressSize;
		if (loc >= regSet.calleeStart) {
			return wordSize * (loc - regSet.calleeStart);
		} else if (loc >= regSet.callerStart) {
			return frame.size() + (wordSize * (loc - regSet.callerStart));
		} else if (loc >= regSet.spillStart) {
			return wordSize * (loc - regSet.spillStart + frame.spillArgs);
		} else {
			if (isSSE) failSSELocation("invalid SSE spill location", loc, regSet);
			else failLocation("invalid spill location", loc, regSet);
			return 0;
		}
	}
	// patch an absolute address, scanning backwards up to "start"
	def recordPatch(start: int, target: Addr) {
		// scan backwards, looking for the absolute constant
		var pos = findAbsConst(start);
		if (pos >= 0) return machBuffer.recordPatch(target, pos);
	}
	def findAbsConst(start: int) -> int {
		var data = w.data;
		for (i = w.pos; i >= start; i--) {
			if (data[i-4] != X86Addrs.ABS_CONST0) continue;
			if (data[i-3] != X86Addrs.ABS_CONST1) continue;
			if (data[i-2] != X86Addrs.ABS_CONST2) continue;
			if (data[i-1] != X86Addrs.ABS_CONST3) continue;
			return i - 4;
		}
		mach.fail("Could not find absolute constant to patch");
		return -1;
	}
	def jmpl_near(cond: X86Cond, label: Label) {
		var off = 0;
		if (cond == null) jmp(off);
		else j(off, 0x70 + cond.index, 0x80 + cond.index);
		if (label.pos >= 0) {
			off = label.pos - pos();
			w.at(pos() - 1).putb(byte.view(off));
			w.atEnd();
		} else {
			label.near_uses = List.new(pos(), label.near_uses);
		}
	}
	def bind(label: Label) {
		label.pos = pos();
		for (l = label.near_uses; l != null; l = l.tail) {
			w.at(l.head - 1).putb(byte.view(label.pos) - l.head);
		}
		w.atEnd();
	}
	// edx:eax / a
	def idivmod_checked(source: Source, zeroCheck: bool, negCheck: bool, a: X86Rm, div: bool) {
		var label: Label;
		if (negCheck) {
			cmp.rm_i(a, -1);
			jnz(4); // if b != -1, branch to division (both cases are 4 bytes of code)
			if (div) neg(eax); // a / -1 == 0 - a
			else xor.rm_r(edx, edx); // a % -1 == 0
			label = Label.new();
			jmpl_near(X86Conds.ALWAYS, label); // jump past division to end
		}
		cdq();
		var off = codeOffset();
		idiv(a);
		if (label != null) bind(label);
		if (zeroCheck) mach.runtime.src.recordSource(off, source);
	}
	// edx:eax / a
	def udivmod_checked(source: Source, zeroCheck: bool, a: X86Rm) {
		xor.rm_r(edx, edx);
		var off = codeOffset();
		div(a);
		if (zeroCheck) mach.runtime.src.recordSource(off, source);
	}
	def neg64(a: X86Reg, b: X86Reg) {
		var no_carry = Label.new();
		neg(a);
		jmpl_near(X86Conds.NC, no_carry);
		add.rm_i(b, 1);
		bind(no_carry);
		neg(b);
	}
	def wdiv(op: WideDivision, source: Source) {
		if (!op.signed && op.small_divisor) {
			// generate the division inline.
			return u64_divmod_small_divisor(op, source, false);
		}
		// generate a call to a stub.
		var name = op.name;
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_i64_divmod_stub(mach, _, _, codeStartOffset, op));
		}
		call_addr(addr);
		if (op.zeroCheck) mach.runtime.src.recordReturnSource(codeOffset(), source);
	}
	// (%eax, %edx) =  n0,n1 (%esi,%eax) /% d0,d1 (%edi, %edx) [kill all regs]
	def wdiv_full(op: WideDivision, source: Source, stub: bool) {
		var scratch = Regs.SCRATCH;
		if (op.signed) {
			// XXX: negative checks for numerator/denom can be folded
			xor.rm_r(scratch, scratch);
			// If n < 0, negate
			var n_positive = Label.new();
			test_rm_r(eax, eax);
			jmpl_near(X86Conds.NS, n_positive);
			neg64(esi, eax);
			movd_rm_i(scratch, 1);
			bind(n_positive);

			// if d < 0, negate
			if (!op.small_divisor) {
				var d_positive = Label.new();
				test_rm_r(edx, edx);
				jmpl_near(X86Conds.NS, d_positive);
				neg64(edi, edx);
				if (!op.mod) xor.rm_i(scratch, 1);
				bind(d_positive);
			}
		}

		// perform unsigned division based on reduction to u64_div_u32:
		// if (d1 == 0) {
		//      // u64_div_small_divisor
		//      var u = u64_div_u32(to_u64(u32_mod(n1, d0), n0), d0);
		// 	return to_u64(u32_div(n1, d0)), u);
		// } else {
		//      // u64_div_large_divisor
		// 	var s = num_leading_zeroes(d1);
		// 	var q1 = u64_div_u32(u64_shr(n, 1), high_u32(u64_shl(d, s)));
		// 	var q2 = u32_shr(q1, 31 - s);
		// 	if (u64_sub(n, u64_mul_u32(d, q2)) < 0) return to_u64(0, q2 - 1);
		// 	return to_u64(0, q2);
		// }

		var done = Label.new();
		var small_divisor = Label.new();
		if (!op.large_divisor) {
			cmp.rm_i(edx, 0);
			jmpl_near(X86Conds.Z, small_divisor);
		}

		u64_divmod_big_divisor(op, source);

		if (!op.large_divisor) {
			jmpl_near(X86Conds.ALWAYS, done);
			// fast case of d1 == 0
			bind(small_divisor);
			u64_divmod_small_divisor(op, source, stub);
		}

		bind(done);

		if (op.signed) {
			// negate result if necessary
			var done = Label.new();
			test_rm_i(scratch, 1);
			jmpl_near(X86Conds.Z, done);
			neg64(eax, edx);
			bind(done);
		}
	}
	def u64_divmod_big_divisor(op: WideDivision, source: Source) {
		// slow case of d1 != 0
		var spilled_n0 = esp.plus(0), spilled_n1 = esp.plus(4);
		var spilled_d0 = esp.plus(8), spilled_d1 = esp.plus(12);
		push(edx);
		push(edi);
		push(eax);
		push(esi);
		xchg(eax, edx);
		xchg(eax, esi);
		shrd_i(eax, edx, 1);
		shr_i(edx, 1);
		bsr(ebx, esi);
		lea(ecx, ebx.plus(-31));
		neg(ecx);
		shld_cl(esi, edi);
		div(esi);
		movd_rm_r(ecx, ebx);
		shr_cl(eax);
		// quotient adjustment
		movd_r_rm(ecx, spilled_d1);
		imul_r_rm(ecx, eax);
		xchg(edi, eax);
		mul(edi);
		if (op.mod) {
			add.rm_r(ecx, edx);
			movd_r_rm(ebx, eax);
			movd_r_rm(eax, spilled_n0);
			movd_r_rm(edx, spilled_n1);
			sub.r_rm(eax, ebx);
			sbb.r_rm(edx, ecx);
			var mod_done = Label.new();
			jmpl_near(X86Conds.NC, mod_done);
			add.r_rm(eax, spilled_d0);
			adc.r_rm(edx, spilled_d1);
			bind(mod_done);
		} else {
			add.rm_r(edx, ecx);
			sub.rm_r(spilled_n0, eax);
			sbb.rm_r(spilled_n1, edx);
			movd_rm_r(eax, edi);
			var div_done = Label.new();
			jmpl_near(X86Conds.NC, div_done);
			sub.rm_i(eax, 1);
			bind(div_done);
			xor.rm_r(edx, edx);
		}
		add.rm_i(esp, 16);
	}
	def u64_divmod_small_divisor(op: WideDivision, source: Source, stub: bool) {
		var start = codeOffset(), frame: MachFrame;
		if (op.zeroCheck && stub && mach.runtime.src != null) {
			var frame = MachFrame.new(null, null, 0);
			frame.frameSize = 0;
			mach.runtime.src.recordStubStart(start, op.name, frame);
		}
		div(edi);
		if (op.zeroCheck && mach.runtime.src != null) mach.runtime.src.recordSource(start, source);
		if (op.mod) {
			movd_r_rm(eax, esi);
			div(edi);
			movd_r_rm(eax, edx);
			xor.r_rm(edx, edx);
		} else {
			xchg(eax, esi);
			div(edi);
			movd_rm_r(edx, esi);
		}
		if (frame != null) mach.runtime.src.recordFrameEnd(codeOffset());
	}

	// a macro to convert from a single-precision float to an unsigned int
	def cvt_f2ui(dest: X86Reg, a: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		sub.rm_i(esp, 0x8);
		if (SSEAddr.?(a)) {
			fld_d(SSEAddr.!(a));
		} else {
			movss_sm_s(save, SSEReg.!(a));
			fld_d(save);
		}
		fisttp_q(save);
		movd_r_rm(dest, save.toX86Addr());
		add.rm_i(esp, 0x8);
	}
	// a macro to convert from a double-precision float to an unsigned int
	def cvt_d2ui(dest: X86Reg, a: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		sub.rm_i(esp, 0x8);
		if (SSEAddr.?(a)) {
			fld_q(SSEAddr.!(a));
		} else {
			movsd_sm_s(save, SSEReg.!(a));
			fld_q(save);
		}
		fisttp_q(save);
		movd_r_rm(dest, save.toX86Addr());
		add.rm_i(esp, 0x8);
	}
	// macro to truncate a single-precision float to an unsigned integer
	def uint_truncf(dest: X86Reg, a: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		sub.rm_i(esp, 0x4);
		if (SSEReg.?(a)) {
			movss_sm_s(save, SSEReg.!(a));
		} else {
			movss_s_sm(sse_scratch, a);
			movss_sm_s(save, sse_scratch);
		}
		var name = "f2ui_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_f2ui_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		pop(dest);
	}
	// macro to truncate a double-precision float to an unsigned integer
	def uint_truncd(dest: X86Reg, a: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		sub.rm_i(esp, 0x8);
		if (SSEReg.?(a)) {
			movsd_sm_s(save, SSEReg.!(a));
		} else {
			movsd_s_sm(sse_scratch, a);
			movsd_sm_s(save, sse_scratch);
		}
		var name = "d2ui_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_d2ui_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		pop(dest);
		add.rm_i(esp, 0x4);
	}
	/** stub that converts a single-precision float to an unsigned int with
	 *  virgil truncation semantics
	 *  @input: 32-bit floating point value on stack
	 *  @output: 32-bit unsigned integer on stack
	 */
	def f2ui_fixup() {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var inout = esp.plus(4 * 0x4); // return address + 3 saves
		var xmm0_save = esp.plusSSE(2 * 0x4);
		var save = esp.plusSSE(0x0);
		var L1 = Label.new(), L2 = Label.new(), done = Label.new();
		sub.rm_i(esp, 0xc);
		movss_sm_s(xmm0_save, xmm0);
		movss_s_sm(xmm0, inout.toSSEAddr());
		xorps(sse_scratch, sse_scratch);
		ucomiss(sse_scratch, xmm0); // if (f <= 0) return 0;
		jmpl_near(X86Conds.NC, L1);
		movd_rm_i(scratch, 0x4f800000);
		movd_s_rm(sse_scratch, scratch);
		ucomiss(xmm0, sse_scratch); // if (f >= 4294967296) return MAX_U32_INT;
		jmpl_near(X86Conds.C, L2);
		movd_rm_i(inout, 0xffffffff);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L2); // else return (unsigned int)f;
		movss_sm_s(save, xmm0);
		fld_d(save);
		fisttp_q(save);
		movd_r_rm(scratch, save.toX86Addr());
		movd_rm_r(inout, scratch);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L1);
		movd_rm_i(inout, 0x0);
		bind(done);
		movss_s_sm(xmm0, xmm0_save);
		add.rm_i(esp, 0xc);
	}
	/** stub that converts a double-precision float to an unsigned int with
	 *  virgil truncation semantics
	 *  @input: 64-bit floating point value on stack
	 *  @output: 32-bit unsigned integer on stack
	 */
	def d2ui_fixup() {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var inout = esp.plus(5 * 0x4); // return address + 4 saves
		var xmm0_save = esp.plusSSE(2 * 0x4);
		var save = esp.plusSSE(0x0);
		var L1 = Label.new(), L2 = Label.new(), done = Label.new();
		sub.rm_i(esp, 4 * 0x4);
		movsd_sm_s(xmm0_save, xmm0);
		movsd_s_sm(xmm0, inout.toSSEAddr());
		xorpd(sse_scratch, sse_scratch);
		ucomisd(sse_scratch, xmm0);
		jmpl_near(X86Conds.NC, L1); // if (d <= 0) return 0;
		movd_rm_i(esp.plus(0x0), 0x00000000);
		movd_rm_i(esp.plus(0x4), 0x41f00000);
		ucomisd(xmm0, esp.plusSSE(0x0)); // if (d >= 4294967296) return MAX_U32_INT;
		jmpl_near(X86Conds.C, L2);
		movd_rm_i(inout, 0xffffffff);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L1);
		movd_rm_i(inout, 0x0);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L2); // else return (unsigned int)d;
		movsd_sm_s(save, xmm0);
		fld_q(save);
		fisttp_q(save);
		movd_r_rm(scratch, save.toX86Addr());
		movd_rm_r(inout, scratch);
		bind(done);
		movsd_s_sm(xmm0, xmm0_save);
		add.rm_i(esp, 4 * 0x4);
	}
	// a macro to convert an unsigned integer to a single-precision float
	def cvt_ui2f(dest: SSEReg, a: X86Rm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		push(a);
		var name = "ui2f_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_ui2f_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		add.rm_i(esp, 0x4);
		movss_s_sm(dest, sse_scratch); // XXX: return value in sse_scratch
	}
	// a macro to convert an unsigned integer to a double-precision float
	def cvt_ui2d(dest: SSEReg, a: X86Rm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		push(a);
		var name = "ui2d_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_ui2d_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		add.rm_i(esp, 0x4);
		movsd_s_sm(dest, sse_scratch); // XXX: return value in sse_scratch
	}
	/** stub that converts an unsigned integer to a single-precision float
	 *  @input: 32-bit unsigned integer on stack
	 *  @output: 32-bit floating point value in SSE_SCRATCH
	 */
	def ui2f_fixup() {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plus(2 * 0x4); // return address + 1 save
		var done = Label.new();
		push(eax);
		movd_r_rm(scratch, save);
		cvtsi2ss(sse_scratch, scratch);
		test_rm_r(scratch, scratch);
		jmpl_near(X86Conds.GE, done);
		movd_r_rm(eax, scratch);
		shr_i(scratch, 0x1);
		and.rm_i(eax, 0x1);
		or.r_rm(eax, scratch);
		cvtsi2ss(sse_scratch, eax);
		addss(sse_scratch, sse_scratch); // XXX: return value in sse_scratch
		bind(done);
		pop(eax);
	}
	/** stub that converts an unsigned integer to a double-precision float
	 *  @input: 32-bit unsigned integer on stack
	 *  @output: 32-bit floating point value in SSE_SCRATCH
	 */
	def ui2d_fixup() {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plus(0x4);
		xorpd(sse_scratch, sse_scratch);
		movd_r_rm(scratch, save);
		add.rm_i(scratch, 0x80000000);
		cvtsi2sd(sse_scratch, scratch);
		push_i(0x41e00000);
		push_i(0x00000000);
		addsd(sse_scratch, esp.plusSSE(0x0)); // XXX: return value in sse_scratch
		add.rm_i(esp, 0x8);
	}
	// a macro to convert an integer to a single-precision float
	def int_truncf(a: X86Reg, b: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plus(0x0);
		var label = Label.new();
		cvttss2si(a, b);
		cmp.rm_i(a, 0x80000000); // XXX: float sign flip
		jmpl_near(X86Conds.NZ, label);
		sub.rm_i(esp, 0x4);
		if (SSEReg.?(b)) {
			movd_rm_s(save, SSEReg.!(b));
		} else {
			movss_s_sm(sse_scratch, b);
			movd_rm_s(save, sse_scratch);
		}
		var name = "f2i_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_f2i_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		pop(a);
		bind(label);
	}
	// a macro to convert an integer to a double-precision float
	def int_truncd(a: X86Reg, b: SSERm) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		var label = Label.new();
		cvttsd2si(a, b);
		cmp.rm_i(a, 0x80000000); // XXX: float sign flip
		jmpl_near(X86Conds.NZ, label);
		sub.rm_i(esp, 2 * 0x4);
		if (SSEReg.?(b)) {
			movsd_sm_s(save, SSEReg.!(b));
		} else {
			movsd_s_sm(sse_scratch, b);
			movsd_sm_s(save, sse_scratch);
		}
		var name = "d2i_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_d2i_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		pop(a);
		add.rm_i(esp, 0x4);
		bind(label);
	}
	/** stub that converts a single-precision float to an int with virgil
	 *  truncation semantics
	 *  @input: 32-bit floating point value on stack
	 *  @output: 32-bit integer on stack
	 */
	def f2i_fixup() {
		var scratch = Regs.SCRATCH;
		var inout = esp.plus(4 * 0x4); // return address + 3 saves
		var done = Label.new();
		push(eax);
		push(ecx);
		push(edx);
		movd_rm_i(eax, 0x7f800000);
		xor.r_rm(ecx, ecx);
		movd_r_rm(scratch, inout);
		movd_r_rm(edx, scratch);
		and.rm_i(edx, 0x7fffffff);
		cmp.r_rm(eax, edx); // if NaN -> 0
		jmpl_near(X86Conds.S, done); // if negative;
		test_rm_r(scratch, scratch); // signed ? MIN_INT : MAX_INT;
		movd_rm_i(ecx, 0x80000000);
		movd_rm_i(eax, 0x7fffffff);
		cmovns(ecx, eax);
		bind(done);
		movd_rm_r(inout, ecx);
		pop(edx);
		pop(ecx);
		pop(eax);
	}
	/** stub that converts a double-precision float to an int with virgil
	 *  truncation semantics
	 *  @input: 64-bit floating point value on stack
	 *  @output: 32-bit integer on stack
	 */
	def d2i_fixup() {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var inout = esp.plus(5 * 0x4); // return address + 4 saves
		var xmm0_save = esp.plusSSE(0x8);
		var L1 = Label.new(), L2 = Label.new(), done = Label.new();
		sub.rm_i(esp, 0x8);
		push_i(0x41e00000);
		push_i(0x00000000);
		movsd_sm_s(xmm0_save, xmm0);
		movsd_s_sm(sse_scratch, inout.toSSEAddr());
		ucomisd(sse_scratch, sse_scratch); // if (d == NaN) return 0;
		jmpl_near(X86Conds.P, L1);
		ucomisd(sse_scratch, esp.plusSSE(0x0)); // if (d >= 2**31 - 1) return MAX_I32_INT;
		jmpl_near(X86Conds.C, L2);
		movd_rm_i(inout, 0x7fffffff);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L2);
		movd_rm_i(esp.plus(0x4), 0xc1e00000);
		movsd_s_sm(xmm0, esp.plusSSE(0x0));
		ucomisd(xmm0, sse_scratch); // if (d <= -2**31) return MIN_I32_INT;
		jmpl_near(X86Conds.C, L1);
		movd_rm_i(inout, 0x80000000);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L1);
		movd_rm_i(inout, 0x0);
		bind(done);
		movsd_s_sm(xmm0, xmm0_save);
		add.rm_i(esp, 4 * 0x4);
	}
	// a macro to convert an unsigned long to a floating point value
	def cvt_ul2fp(dest: SSEReg, a: X86Rm, b: X86Rm, isDouble: bool) {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		var L1 = Label.new();
		push(b);
		push(a);
		fild_q(save);
		movd_r_rm(scratch, b);
		test_rm_r(scratch, scratch);
		jmpl_near(X86Conds.NS, L1);
		push_i(0x5f800000);
		fadd_d(save);
		add.rm_i(esp, 0x4);
		bind(L1);
		if (isDouble) {
			fstp_q(save);
			movsd_s_sm(dest, save);
		} else {
			fstp_d(save);
			movss_s_sm(dest, save);
		}
		add.rm_i(esp, 0x8);
	}
	// a macro to convert a long to a floating point value
	def cvt_l2fp(dest: SSEReg, a: X86Rm, b: X86Rm, isDouble: bool) {
		var save = esp.plusSSE(0x0);
		push(b);
		push(a);
		fild_q(save);
		if (isDouble) {
			fstp_q(save);
			movsd_s_sm(dest, save);
		} else {
			fstp_d(save);
			movss_s_sm(dest, save);
		}
		add.rm_i(esp, 0x8);
	}
	// a macro to convert a floating point value to a long
	def cvt_fp2l(il: X86Rm, ih: X86Rm, a: SSERm, isDouble: bool) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		var ih_save = esp.plus(0x4);
		var il_save = esp.plus(0x0);
		sub.rm_i(esp, 0x8);
		if (!isDouble) cvtss2sd(sse_scratch, a); // if single-precision float
		else if (SSEReg.?(a)) sse_scratch = SSEReg.!(a); // XXX: cheeky way to save SSEReg
		else if (SSEAddr.?(a)) movsd_s_sm(sse_scratch, a);
		movsd_sm_s(save, sse_scratch);
		fld_q(save);
		fisttp_q(save);
		movd_rm_rm(il, il_save, null);
		movd_rm_rm(ih, ih_save, null);
		add.rm_i(esp, 0x8);
	}
	// a macro to convert a floating point value to an unsigned long
	def cvt_fp2ul(il: X86Rm, ih: X86Rm, a: SSERm, isDouble: bool) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		sub.rm_i(esp, 0x8);
		if (!isDouble) cvtss2sd(sse_scratch, a); // if single-precision float
		else if (SSEReg.?(a)) sse_scratch = SSEReg.!(a); // XXX: cheeky way to save SSEReg
		else if (SSEAddr.?(a)) movsd_s_sm(sse_scratch, a);
		movsd_sm_s(save, sse_scratch);
		var name = "d2ul";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_d2ul_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		pop(il);
		pop(ih);
	}
	/** stub that converts a double-precision float to a long
	 *  @input: 64-bit floating point value on stack
	 *  @output: 64-bit long on stack
	 */
	def d2ul() {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var inout = esp.plusSSE(3 * 0x4); // return address + 2 saves
		var inout_ih = esp.plus(4 * 0x4); // return address + 3 saves
		var xmm0_save = esp.plusSSE(0x0);
		var L1 = Label.new(), done = Label.new();
		sub.rm_i(esp, 0x8);
		movsd_sm_s(xmm0_save, xmm0);
		movsd_s_sm(xmm0, inout);
		push_i(0x43e00000);
		push_i(0x00000000);
		movsd_s_sm(sse_scratch, esp.plusSSE(0x0));
		add.rm_i(esp, 0x8);
		ucomisd(xmm0, sse_scratch); // if (d >= 2**63) return ((unsigned long) (d - 2**63)) + MIN_I64_INT;
		jmpl_near(X86Conds.NC, L1);
		movsd_sm_s(inout, xmm0); // else return (unsigned long)d;
		fld_q(inout);
		fisttp_q(inout);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L1);
		subsd(xmm0, sse_scratch);
		movsd_sm_s(inout, xmm0);
		fld_q(inout);
		fisttp_q(inout);
		add.rm_i(inout_ih, 0x80000000);
		bind(done);
		movsd_s_sm(xmm0, xmm0_save);
		add.rm_i(esp, 0x8);
	}
	def ulong_truncfp(il: X86Rm, ih: X86Rm, a: SSERm, isDouble: bool) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		sub.rm_i(esp, 0x8);
		if (!isDouble) cvtss2sd(sse_scratch, a); // if single-precision float
		else if (SSEReg.?(a)) sse_scratch = SSEReg.!(a); // XXX: cheeky way to save SSEReg
		else if (SSEAddr.?(a)) movsd_s_sm(sse_scratch, a);
		movsd_sm_s(save, sse_scratch);
		var name = "d2ul_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_d2ul_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		pop(il);
		pop(ih);
	}
	/** stub that converts a double-precision float to an unsigned long with
	 *  virgil truncation semantics
	 *  @input: 64-bit floating point value on stack
	 *  @output: 64-bit unsigned long on stack
	 */
	def d2ul_fixup() {
		var sse_scratch = Regs.SSE_SCRATCH;
		var inout = esp.plusSSE(5 * 0x4); // return value + 4 saves
		var inout_il = inout.toX86Addr();
		var inout_ih = esp.plus(6 * 0x4); // return value + 5 saves
		var ih_save = esp.plus(0x4);
		var il_save = esp.plus(0x0);
		var xmm0_save = esp.plusSSE(0x8);
		var L1 = Label.new(), L2 = Label.new();
		var L3 = Label.new(), done = Label.new();
		sub.rm_i(esp, 2 * 0x8);
		movsd_sm_s(xmm0_save, xmm0);
		movsd_s_sm(xmm0, inout);
		ucomisd(xmm0, xmm0); // if (d == NaN) return 0;
		jmpl_near(X86Conds.P, L1);
		xorpd(sse_scratch, sse_scratch);
		ucomisd(sse_scratch, xmm0); // if (d <= 0) return 0;
		jmpl_near(X86Conds.NC, L1);
		movd_rm_i(il_save, 0x00000000);
		movd_rm_i(ih_save, 0x43f00000);
		movsd_s_sm(sse_scratch, esp.plusSSE(0x0));
		ucomisd(xmm0, sse_scratch); // if (d >= 2**64) return MAX_U64_INT;
		jmpl_near(X86Conds.NC, L2);
		movd_rm_i(ih_save, 0x43e00000);
		movsd_s_sm(sse_scratch, esp.plusSSE(0x0));
		ucomisd(xmm0, sse_scratch); // if (d >= 2**63) return ((unsigned long) (d - 2**63)) + MIN_I64_INT;
		jmpl_near(X86Conds.NC, L3);
		movsd_sm_s(inout, xmm0); // else return (unsigned long) d;
		fld_q(inout);
		fisttp_q(inout);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L2);
		movd_rm_i(inout_il, 0xffffffff);
		movd_rm_i(inout_ih, 0xffffffff);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L3);
		subsd(xmm0, sse_scratch);
		movsd_sm_s(inout, xmm0);
		fld_q(inout);
		fisttp_q(inout);
		add.rm_i(inout_ih, 0x80000000);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L1);
		xorpd(xmm0, xmm0);
		movsd_sm_s(inout, xmm0);
		bind(done);
		movsd_s_sm(xmm0, xmm0_save);
		add.rm_i(esp, 2 * 0x8);
	}
	// a macro to convert a floating point value to a long
	def long_truncfp(il: X86Rm, ih: X86Rm, a: SSERm, isDouble: bool) {
		var sse_scratch = Regs.SSE_SCRATCH;
		var save = esp.plusSSE(0x0);
		sub.rm_i(esp, 0x8);
		if (!isDouble) cvtss2sd(sse_scratch, a); // if single-precision float
		else if (SSEReg.?(a)) sse_scratch = SSEReg.!(a); // XXX: cheeky way to save SSEReg
		else if (SSEAddr.?(a)) movsd_s_sm(sse_scratch, a);
		movsd_sm_s(save, sse_scratch);
		var name = "d2l_fixup";
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_d2l_fixup_stub(mach, _, _, codeStartOffset));
		}
		call_addr(addr);
		pop(il);
		pop(ih);
	}
	/** stub that converts a double-precision float to a long with virgil
	 *  truncation semantics
	 *  @input: 64-bit floating point value on stack
	 *  @output: 64-bit long on stack
	 */
	def d2l_fixup() {
		var scratch = Regs.SCRATCH;
		var sse_scratch = Regs.SSE_SCRATCH;
		var inout = esp.plusSSE(5 * 0x4); // return value + 4 saves
		var inout_il = inout.toX86Addr();
		var inout_ih = esp.plus(6 * 0x4); // return value + 5 saves
		var ih_save = esp.plus(0x4);
		var il_save = esp.plus(0x0);
		var xmm0_save = esp.plusSSE(0x8);
		var L1 = Label.new(), L2 = Label.new();
		var L3 = Label.new(), done = Label.new();
		sub.rm_i(esp, 2 * 0x8);
		movsd_sm_s(xmm0_save, xmm0);
		movsd_s_sm(xmm0, inout);
		ucomisd(xmm0, xmm0);
		jmpl_near(X86Conds.P, L1); // if (d == NaN) return 0;
		movd_rm_i(ih_save, 0xc3e00000);
		movd_rm_i(il_save, 0x00000000);
		movsd_s_sm(sse_scratch, esp.plusSSE(0x0));
		ucomisd(sse_scratch, xmm0); // if (d <= -2**63) return MIN_I64_INT;
		jmpl_near(X86Conds.NC, L2);
		movd_rm_i(ih_save, 0x43e00000);
		movsd_s_sm(sse_scratch, esp.plusSSE(0x0));
		ucomisd(xmm0, sse_scratch); // if (d >= 2**63) return MAX_I64_INT;
		jmpl_near(X86Conds.NC, L3);
		movsd_sm_s(inout, xmm0); // else return (long)d;
		fld_q(inout);
		fisttp_q(inout);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L3);
		movd_rm_i(inout_il, 0xffffffff);
		movd_rm_i(inout_ih, 0x7fffffff);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L2);
		movd_rm_i(inout_il, 0x0);
		movd_rm_i(inout_ih, 0x80000000);
		jmpl_near(X86Conds.ALWAYS, done);
		bind(L1);
		xorpd(xmm0, xmm0);
		movsd_sm_s(inout, xmm0);
		bind(done);
		movsd_s_sm(xmm0, xmm0_save);
		add.rm_i(esp, 2 * 0x8);
	}
	// a macro to perform bit equality for floating point values
	def fp_bit_eq(dest: X86Reg, a: SSEReg, b: SSERm, isDouble: bool) {
		var sse_scratch = Regs.SSE_SCRATCH;
		if (isDouble) {
			if (a != sse_scratch) movsd_s_sm(sse_scratch, a);
			pcmpeqq(sse_scratch, b);
		} else {
			if (a != sse_scratch) movss_s_sm(sse_scratch, a);
			pcmpeqd(sse_scratch, b);
		}
		movd_rm_s(dest, sse_scratch);
		and.rm_i(dest, 0x1);
	}
	// a macro to generate a boolean after comparing two floating point values
	def cmp_fp(cond: X86Cond, dest: X86Reg, a: SSEReg, b: SSERm, isDouble: bool) {
		if (cond == X86Conds.NZ) movd_rm_i(dest, 0x1); // XXX: if unordered result, then default for != is true
		else movd_rm_i(dest, 0x0);
		if (isDouble) ucomisd(a, b);
		else ucomiss(a, b);
		var label = Label.new();
		jmpl_near(X86Conds.P, label);
		setx(cond, dest);
		bind(label);
	}
	// a macro to compute the absolute value of a float
	def fabs(dest: SSEReg, a: SSERm, isDouble: bool) {
		if (isDouble) {
			if (dest != a) movsd_s_sm(dest, a);
			psllq_i(dest, 0x1); // lsl followed by lsr to zero out sign bit
			psrlq_i(dest, 0x1);
		} else {
			if (dest != a) movss_s_sm(dest, a);
			pslld_i(dest, 0x1);
			psrld_i(dest, 0x1);
		}
	}
}
// A helper for generating shift operations for a given V3 opcode.
class X86Shifter(opcode: Opcode) {
	// Wide shift {rdl,rdh} in place by the amount in CL.
	def wsh(asm: X86MacroAssembler, rdl: X86Reg, rdh: X86Reg, sh: X86Rm) {
		var done = Label.new(), big = Label.new();
		// perform the 32 bit shift.
		wsh_cl(asm, rdl, rdh);
		asm.cmp.rm_i(ecx, 32);
		asm.jmpl_near(X86Conds.A, big);
		asm.jmpl_near(X86Conds.L, done);
		// shift amount was actually >= 32.
		asm.bind(big);
		wsh_adjust(asm, rdl, rdh);
		asm.jmpl_near(X86Conds.ALWAYS, done);
		// done.
		asm.bind(done);
	}
	// Shift by the amount in CL.
	def sh_cl(asm: X86MacroAssembler, dest: X86Rm) {
		match (opcode) {
			IntShr => asm.shr_cl(dest);
			IntSar => asm.sar_cl(dest);
			IntShl => asm.shl_cl(dest);
			_ => ;
		}
	}
	// Shift by an immediate.
	def sh_i(asm: X86MacroAssembler, dest: X86Rm, imm: int) {
		if (imm == 0) return;
		if (u32.!(imm) >= u32.!(32)) {
			if (Opcode.IntSar.?(opcode)) return asm.sar_i(dest, 31);
			else return asm.movd_rm_i(dest, 0);
		}
		match (opcode) {
			IntShr => asm.shr_i(dest, imm);
			IntSar => asm.sar_i(dest, imm);
			IntShl => asm.shl_i(dest, imm);
			_ => ;
		}
	}
	// Wide shift by the amount in CL.
	def wsh_cl(asm: X86MacroAssembler, rdl: X86Reg, rdh: X86Reg) {
		match (opcode) {
			IntShr => {
				asm.shrd_cl(rdl, rdh);
				asm.shr_cl(rdh);
			}
			IntSar => {
				asm.shrd_cl(rdl, rdh);
				asm.sar_cl(rdh);
			}
			IntShl => {
				asm.shld_cl(rdh, rdl);
				asm.shl_cl(rdl);
			}
			_ => ;
		}
	}
	// Adjust an already shifted result if the shift amount in CL was 32 <= x < 64.
	private def wsh_adjust(asm: X86MacroAssembler, rdl: X86Reg, rdh: X86Reg) {
		match (opcode) {
			IntShr => {
				asm.movd_rm_r(rdl, rdh);
				asm.xor.rm_r(rdh, rdh);
			}
			IntSar => {
				asm.movd_rm_r(rdl, rdh);
				asm.sar_i(rdh, 31);
			}
			IntShl => {
				asm.movd_rm_r(rdh, rdl);
				asm.xor.rm_r(rdl, rdl);
			}
			_ => ;
		}
	}
}
// Describes the properties of a signed/unsigned 64-bit modulus or division.
class WideDivision(mod: bool,		// modulus
		   signed: bool,	// numerator or denominator is signed
		   large_divisor: bool, // divisor is definitely > 32-bits
		   small_divisor: bool, // divisor is definitely <= 32 bits
		   zeroCheck: bool) {   // divisor may be zero
	var name: string;
	new() {
		var buf = StringBuilder.new();
		buf.putc(if(signed, 'i', 'u'));
		buf.puts("64_");
		buf.puts(if(mod, "mod", "div"));
		if (large_divisor) buf.putc('l');
		if (small_divisor) buf.putc('s');
		if (zeroCheck) buf.putc('z');
		name = buf.toString();
	}
}
