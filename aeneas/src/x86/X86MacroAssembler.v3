// Copyright 2014 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def eax = X86Regs.EAX;
def ebx = X86Regs.EBX;
def ecx = X86Regs.ECX;
def edx = X86Regs.EDX;
def edi = X86Regs.EDI;
def esi = X86Regs.ESI;
def ebp = X86Regs.EBP;
def esp = X86Regs.ESP;
def xmm0 = X86Regs.XMM0;
def xmm1 = X86Regs.XMM1;
def xmm2 = X86Regs.XMM2;
def xmm3 = X86Regs.XMM3;
def xmm4 = X86Regs.XMM4;
def xmm5 = X86Regs.XMM5;
def xmm6 = X86Regs.XMM6;
def xmm7 = X86Regs.XMM7;

def gen_i64_divmod_stub(mach: MachProgram, addr: Addr, u: MachDataBuffer, codeStartOffset: int, op: WideDivision) {
	var asm = X86MacroAssembler.new(mach, u, codeStartOffset);
	asm.wdiv_full(op, null, true);
	asm.ret();
}

// An extended X86Assembler that has additional machine-level utilities, such as
// recording patch locations and translating between regset locations and x86
// registers/memory.
class X86MacroAssembler extends X86Assembler {
	def mach: MachProgram;		  // machine program
	def machBuffer: MachDataBuffer; // machine-level buffer
	def codeStartOffset: int;

	new(mach, machBuffer, codeStartOffset) super(machBuffer) { }
	def codeOffset() -> int {
		return buffer.pos - codeStartOffset;
	}
	// call an absolute address and record the patch location
	def call_addr(addr: Addr) {
		call(X86Addrs.REL_CONST);
		machBuffer.recordPatch(addr, machBuffer.pos - 4);
	}
	// jump (conditionally) to an absolute address and record the patch location
	def jmpx_addr(cond: X86Cond, addr: Addr) {
		jmpx(cond, X86Addrs.REL_CONST);
		machBuffer.recordPatch(addr, machBuffer.pos - 4);
	}
	// a macro to move between any two register/memory operands
	def movd_rm_rm(d: X86Rm, s: X86Rm, scratch: X86Reg) {
		if (s == d) return;
		if (X86Reg.?(d)) return movd_r_rm(X86Reg.!(d), s);
		if (X86Reg.?(s)) return movd_rm_r(d, X86Reg.!(s));
		if (scratch != null) {
			movd_r_rm(scratch, s);
			movd_rm_r(d, scratch);
		}
		// XXX: use XMM register for memory-memory move instead of stack
		push(s);            // push value from source memory onto the stack
		pop(X86Addr.!(d));  // pop value off stack into destination memory
	}
	// a macro to move between any two SSE register/memory operands
	def movsd_sm_sm(d: SSERm, s: SSERm, scratch: SSEReg) {
		if (s == d) return;
		if (SSEReg.?(d)) return movsd_s_sm(SSEReg.!(d), s);
		if (SSEReg.?(s)) return movsd_sm_s(d, SSEReg.!(s));
		if (scratch != null) {
			movsd_s_sm(scratch, s);
			movsd_sm_s(d, scratch);
		}
		// push(s);            // push value from source memory onto the stack
		// pop(SSEAddr.!(d));  // pop value off stack into destination memory
	}
	// a macro to move a value into a location
	def movd_l_val(frame: MachFrame, loc: int, val: Val) {
		if (loc >= X86MachRegs.XMM0 && loc <= X86MachRegs.XMM7
				|| Float32Val.?(val)) {
			movd_l_fval(frame, loc, val);
		} else {
			var d = loc_rm(frame, loc);
			match (val) {
				x: Addr => {
						 var pos = buffer.pos;
						 movd_rm_i(d, X86Addrs.ABS_CONST);
						 recordPatch(pos, x);
				}
				_ => movd_rm_i(d, V3.unboxIntegral(val));
			}
		}
	}
	// a macro to move a value into an SSE location
	def movd_l_fval(frame: MachFrame, loc: int, val: Val) {
		var f = 0;
		var d = loc_sm(frame, loc);
		if (val != null) {
			match (val) {
				x: Float32Val => f = int.view(x.bits);
				x: Float64Val => f = int.view(x.bits);
			}
		}
		movd_rm_i(X86Regs.EBP, f);
		movd_s_rm(SSEReg.!(d), X86Regs.EBP); // cast will never fail since we know d is an SSEReg from loc
	}
	// convert a location into an x86 register
	def loc_r(frame: MachFrame, loc: int) -> X86Reg {
		match (loc) {
			X86MachRegs.EAX => return eax;
			X86MachRegs.EBX => return ebx;
			X86MachRegs.ECX => return ecx;
			X86MachRegs.EDX => return edx;
			X86MachRegs.ESI => return esi;
			X86MachRegs.EDI => return edi;
			X86MachRegs.EBP => return ebp;
		}
		return failLocation("required x86 register", loc, frame.conv.regSet);
	}
	// convert a location into an x86 register/memory reference
	def loc_rm(frame: MachFrame, loc: int) -> X86Rm {
		match (loc) {
			0 => return failLocation("unassigned location", loc, frame.conv.regSet);
			X86MachRegs.EAX => return eax;
			X86MachRegs.EBX => return ebx;
			X86MachRegs.ECX => return ecx;
			X86MachRegs.EDX => return edx;
			X86MachRegs.ESI => return esi;
			X86MachRegs.EDI => return edi;
			X86MachRegs.EBP => return ebp;
		}
		var regSet = frame.conv.regSet, wordSize = mach.data.addressSize, offset: int;
		if (loc >= regSet.calleeStart) offset = wordSize * (loc - regSet.calleeStart);
		else if (loc >= regSet.callerStart) offset = frame.size() + (wordSize * (loc - regSet.callerStart));
		else if (loc >= regSet.spillStart) offset = wordSize * (loc - regSet.spillStart + frame.spillArgs);
		else return failLocation("invalid spill location", loc, frame.conv.regSet);
		return esp.plus(offset);
	}
	def failLocation(msg: string, loc: int, regSet: MachRegSet) -> X86Reg {
		mach.fail(Strings.format2("%1: %2", msg, regSet.identify(loc)));
		return X86MachRegs.SCRATCH;
	}
	// convert a location into an SSE register
	def loc_s(frame: MachFrame, loc: int) -> SSEReg {
		match (loc) {
			X86MachRegs.XMM0 => return xmm0;
			X86MachRegs.XMM1 => return xmm1;
			X86MachRegs.XMM2 => return xmm2;
			X86MachRegs.XMM3 => return xmm3;
			X86MachRegs.XMM4 => return xmm4;
			X86MachRegs.XMM5 => return xmm5;
			X86MachRegs.XMM6 => return xmm6;
			X86MachRegs.XMM7 => return xmm7;
		}
		return failSSELocation("required SSE register", loc, frame.conv.regSet);
	}
	// convert a location into an SSE register/memory reference
	def loc_sm(frame: MachFrame, loc: int) -> SSERm {
		match (loc) {
			0 => return failSSELocation("unassigned location", loc, frame.conv.regSet);
			X86MachRegs.XMM0 => return xmm0;
			X86MachRegs.XMM1 => return xmm1;
			X86MachRegs.XMM2 => return xmm2;
			X86MachRegs.XMM3 => return xmm3;
			X86MachRegs.XMM4 => return xmm4;
			X86MachRegs.XMM5 => return xmm5;
			X86MachRegs.XMM6 => return xmm6;
			X86MachRegs.XMM7 => return xmm7;
		}
		var regSet = frame.conv.regSet, wordSize = mach.data.addressSize, offset: int;
		if (loc >= regSet.calleeStart) offset = wordSize * (loc - regSet.calleeStart);
		else if (loc >= regSet.callerStart) offset = frame.size() + (wordSize * (loc - regSet.callerStart));
		else if (loc >= regSet.spillStart) offset = wordSize * (loc - regSet.spillStart + frame.spillArgs);
		else return failSSELocation("invalid spill location", loc, frame.conv.regSet);
		return esp.plusSSE(offset);
	}
	def failSSELocation(msg: string, loc: int, regSet: MachRegSet) -> SSEReg {
		mach.fail(Strings.format2("%1: %2", msg, regSet.identify(loc)));
		return X86MachRegs.SSE_SCRATCH;
	}
	// patch an absolute address, scanning backwards up to "start"
	def recordPatch(start: int, target: Addr) {
		// scan backwards, looking for the absolute constant
		var pos = findAbsConst(start);
		if (pos >= 0) return machBuffer.recordPatch(target, pos);
	}
	def findAbsConst(start: int) -> int {
		var data = buffer.array;
		for (i = buffer.pos; i >= start; i--) {
			if (data[i-4] != X86Addrs.ABS_CONST0) continue;
			if (data[i-3] != X86Addrs.ABS_CONST1) continue;
			if (data[i-2] != X86Addrs.ABS_CONST2) continue;
			if (data[i-1] != X86Addrs.ABS_CONST3) continue;
			return i - 4;
		}
		mach.fail("Could not find absolute constant to patch");
		return -1;
	}
	def jmpl_near(cond: X86Cond, label: Label) {
		var off = 0;
		if (cond == null) jmp(off);
		else j(off, 0x70 + cond.index, 0x80 + cond.index);
		if (label.pos >= 0) {
			off = label.pos - pos();
			buffer.at(pos() - 1).i1(byte.!(off));
			buffer.atEnd();
		} else {
			label.near_uses = List.new(pos(), label.near_uses);
		}
	}
	def bind(label: Label) {
		label.pos = pos();
		for (l = label.near_uses; l != null; l = l.tail) {
			buffer.at(l.head - 1).i1(byte.!(label.pos) - l.head);
		}
		buffer.atEnd();
	}
	// edx:eax / a
	def idivmod_checked(source: Source, zeroCheck: bool, negCheck: bool, a: X86Rm, div: bool) {
		var label: Label;
		if (negCheck) {
			cmp.rm_i(a, -1);
			jnz(4); // if b != -1, branch to division (both cases are 4 bytes of code)
			if (div) neg(eax); // a / -1 == 0 - a
			else xor.rm_r(edx, edx); // a % -1 == 0
			label = Label.new();
			jmpl_near(X86Conds.ALWAYS, label); // jump past division to end
		}
		cdq();
		var off = codeOffset();
		idiv(a);
		if (label != null) bind(label);
		if (zeroCheck) mach.runtime.src.recordSource(off, source);
	}
	// edx:eax / a
	def udivmod_checked(source: Source, zeroCheck: bool, a: X86Rm) {
		xor.rm_r(edx, edx);
		var off = codeOffset();
		div(a);
		if (zeroCheck) mach.runtime.src.recordSource(off, source);
	}
	def neg64(a: X86Reg, b: X86Reg) {
		var no_carry = Label.new();
		neg(a);
		jmpl_near(X86Conds.NC, no_carry);
		add.rm_i(b, 1);
		bind(no_carry);
		neg(b);
	}
	def wdiv(op: WideDivision, source: Source) {
		if (!op.signed && op.small_divisor) {
			// generate the division inline.
			return u64_divmod_small_divisor(op, source, false);
		}
		// generate a call to a stub.
		var name = op.name;
		var addr = mach.stubMap[name].0;
		if (addr == null) {
			addr = Address.new(mach.codeRegion, name);
			mach.stubMap[name] = (addr, gen_i64_divmod_stub(mach, _, _, codeStartOffset, op));
		}
		call_addr(addr);
		if (op.zeroCheck) mach.runtime.src.recordReturnSource(codeOffset(), source);
	}
	// (%eax, %edx) =  n0,n1 (%esi,%eax) /% d0,d1 (%edi, %edx) [kill all regs]
	def wdiv_full(op: WideDivision, source: Source, stub: bool) {
		var scratch = X86MachRegs.SCRATCH;
		if (op.signed) {
			// XXX: negative checks for numerator/denom can be folded
			xor.rm_r(scratch, scratch);
			// If n < 0, negate
			var n_positive = Label.new();
			test_rm_r(eax, eax);
			jmpl_near(X86Conds.NS, n_positive);
			neg64(esi, eax);
			movd_rm_i(scratch, 1);
			bind(n_positive);

			// if d < 0, negate
			if (!op.small_divisor) {
				var d_positive = Label.new();
				test_rm_r(edx, edx);
				jmpl_near(X86Conds.NS, d_positive);
				neg64(edi, edx);
				if (!op.mod) xor.rm_i(scratch, 1);
				bind(d_positive);
			}
		}

		// perform unsigned division based on reduction to u64_div_u32:
		// if (d1 == 0) {
		//      // u64_div_small_divisor
		//      var u = u64_div_u32(to_u64(u32_mod(n1, d0), n0), d0);
		// 	return to_u64(u32_div(n1, d0)), u);
		// } else {
		//      // u64_div_large_divisor
		// 	var s = num_leading_zeroes(d1);
		// 	var q1 = u64_div_u32(u64_shr(n, 1), high_u32(u64_shl(d, s)));
		// 	var q2 = u32_shr(q1, 31 - s);
		// 	if (u64_sub(n, u64_mul_u32(d, q2)) < 0) return to_u64(0, q2 - 1);
		// 	return to_u64(0, q2);
		// }

		var done = Label.new();
		var small_divisor = Label.new();
		if (!op.large_divisor) {
			cmp.rm_i(edx, 0);
			jmpl_near(X86Conds.Z, small_divisor);
		}

		u64_divmod_big_divisor(op, source);

		if (!op.large_divisor) {
			jmpl_near(X86Conds.ALWAYS, done);
			// fast case of d1 == 0
			bind(small_divisor);
			u64_divmod_small_divisor(op, source, stub);
		}

		bind(done);

		if (op.signed) {
			// negate result if necessary
			var done = Label.new();
			test_rm_i(scratch, 1);
			jmpl_near(X86Conds.Z, done);
			neg64(eax, edx);
			bind(done);
		}
	}
	def u64_divmod_big_divisor(op: WideDivision, source: Source) {
		// slow case of d1 != 0
		var spilled_n0 = esp.plus(0), spilled_n1 = esp.plus(4);
		var spilled_d0 = esp.plus(8), spilled_d1 = esp.plus(12);
		push(edx);
		push(edi);
		push(eax);
		push(esi);
		xchg(eax, edx);
		xchg(eax, esi);
		shrd_i(eax, edx, 1);
		shr_i(edx, 1);
		bsr(ebx, esi);
		lea(ecx, ebx.plus(-31));
		neg(ecx);
		shld_cl(esi, edi);
		div(esi);
		movd_rm_r(ecx, ebx);
		shr_cl(eax);
		// quotient adjustment
		movd_r_rm(ecx, spilled_d1);
		imul_r_rm(ecx, eax);
		xchg(edi, eax);
		mul(edi);
		if (op.mod) {
			add.rm_r(ecx, edx);
			movd_r_rm(ebx, eax);
			movd_r_rm(eax, spilled_n0);
			movd_r_rm(edx, spilled_n1);
			sub.r_rm(eax, ebx);
			sbb.r_rm(edx, ecx);
			var mod_done = Label.new();
			jmpl_near(X86Conds.NC, mod_done);
			add.r_rm(eax, spilled_d0);
			adc.r_rm(edx, spilled_d1);
			bind(mod_done);
		} else {
			add.rm_r(edx, ecx);
			sub.rm_r(spilled_n0, eax);
			sbb.rm_r(spilled_n1, edx);
			movd_rm_r(eax, edi);
			var div_done = Label.new();
			jmpl_near(X86Conds.NC, div_done);
			sub.rm_i(eax, 1);
			bind(div_done);
			xor.rm_r(edx, edx);
		}
		add.rm_i(esp, 16);
	}
	def u64_divmod_small_divisor(op: WideDivision, source: Source, stub: bool) {
		var start = codeOffset(), frame: MachFrame;
		if (op.zeroCheck && stub && mach.runtime.src != null) {
			var frame = MachFrame.new(null);
			frame.frameSize = 0;
			mach.runtime.src.recordStubStart(start, op.name, frame);
		}
		div(edi);
		if (op.zeroCheck && mach.runtime.src != null) mach.runtime.src.recordSource(start, source);
		if (op.mod) {
			movd_r_rm(eax, esi);
			div(edi);
			movd_r_rm(eax, edx);
			xor.r_rm(edx, edx);
		} else {
			xchg(eax, esi);
			div(edi);
			movd_rm_r(edx, esi);
		}
		if (frame != null) mach.runtime.src.recordFrameEnd(codeOffset());
	}
}
// A helper for generating shift operations for a given V3 opcode.
class X86Shifter(opcode: Opcode) {
	// Wide shift {rdl,rdh} in place by the amount in CL.
	def wsh(asm: X86MacroAssembler, rdl: X86Reg, rdh: X86Reg, sh: X86Rm) {
		var done = Label.new(), big = Label.new();
		// perform the 32 bit shift.
		wsh_cl(asm, rdl, rdh);
		asm.cmp.rm_i(ecx, 32);
		asm.jmpl_near(X86Conds.A, big);
		asm.jmpl_near(X86Conds.L, done);
		// shift amount was actually >= 32.
		asm.bind(big);
		wsh_adjust(asm, rdl, rdh);
		asm.jmpl_near(X86Conds.ALWAYS, done);
		// done.
		asm.bind(done);
	}
	// Shift by the amount in CL.
	def sh_cl(asm: X86MacroAssembler, dest: X86Rm) {
		match (opcode) {
			IntShr => asm.shr_cl(dest);
			IntSar => asm.sar_cl(dest);
			IntShl => asm.shl_cl(dest);
			_ => ;
		}
	}
	// Shift by an immediate.
	def sh_i(asm: X86MacroAssembler, dest: X86Rm, imm: int) {
		if (imm == 0) return;
		if (u32.!(imm) >= u32.!(32)) {
			if (Opcode.IntSar.?(opcode)) return asm.sar_i(dest, 31);
			else return asm.movd_rm_i(dest, 0);
		}
		match (opcode) {
			IntShr => asm.shr_i(dest, imm);
			IntSar => asm.sar_i(dest, imm);
			IntShl => asm.shl_i(dest, imm);
			_ => ;
		}
	}
	// Wide shift by the amount in CL.
	def wsh_cl(asm: X86MacroAssembler, rdl: X86Reg, rdh: X86Reg) {
		match (opcode) {
			IntShr => {
				asm.shrd_cl(rdl, rdh);
				asm.shr_cl(rdh);
			}
			IntSar => {
				asm.shrd_cl(rdl, rdh);
				asm.sar_cl(rdh);
			}
			IntShl => {
				asm.shld_cl(rdh, rdl);
				asm.shl_cl(rdl);
			}
			_ => ;
		}
	}
	// Adjust an already shifted result if the shift amount in CL was 32 <= x < 64.
	private def wsh_adjust(asm: X86MacroAssembler, rdl: X86Reg, rdh: X86Reg) {
		match (opcode) {
			IntShr => {
				asm.movd_rm_r(rdl, rdh);
				asm.xor.rm_r(rdh, rdh);
			}
			IntSar => {
				asm.movd_rm_r(rdl, rdh);
				asm.sar_i(rdh, 31);
			}
			IntShl => {
				asm.movd_rm_r(rdh, rdl);
				asm.xor.rm_r(rdl, rdl);
			}
			_ => ;
		}
	}
}
// Describes the properties of a signed/unsigned 64-bit modulus or division.
class WideDivision(mod: bool,		// modulus
		   signed: bool,	// numerator or denominator is signed
		   large_divisor: bool, // divisor is definitely > 32-bits
		   small_divisor: bool, // divisor is definitely <= 32 bits
		   zeroCheck: bool) {   // divisor may be zero
	var name: string;
	new() {
		var buf = StringBuffer.new();
		buf.putc(if(signed, 'i', 'u'));
		buf.puts("64_");
		buf.puts(if(mod, "mod", "div"));
		if (large_divisor) buf.putc('l');
		if (small_divisor) buf.putc('s');
		if (zeroCheck) buf.putc('z');
		name = buf.toString();
	}
}
