// Copyright 2011 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

enum RoundingMode(value: int) {
	TO_NEAREST(0x00),
	TO_NEG_INF(0x01),
	TO_POS_INF(0x02),
	TO_ZERO(0x03)
}

// either a register or memory location
class X86Rm {
	def render(buf: StringBuffer) -> StringBuffer {
		return renderWithSize(buf, 32);
	}
	def renderWithSize(buf: StringBuffer, size: int) -> StringBuffer {
		if (X86Reg.?(this)) return buf.puts(X86Reg.!(this).name(size));
		var addr = X86Addr.!(this);
		return renderAddr(buf, addr.base, addr.index, addr.scale, addr.disp);
	}
}
def renderAddr(buf: StringBuffer, base: X86Reg, index: X86Reg, scale: byte, disp: int) -> StringBuffer {
		buf.putc('[');
		var has_base = base != null;
		if (has_base) {
			buf.puts(base.name32);
		}
		var has_index = index != null;
		if (has_index) {
			if (has_base) buf.putc('+');
			buf.puts(index.name32);
			if (scale != 1) buf.putc('*').puti(scale);
		}
		if (has_base || has_index) {
			if (disp < 0) buf.puti(disp);
			if (disp > 0) buf.putc('+').puti(disp);
		} else {
			// absolute address
			buf.putx(disp);
		}
		return buf.putc(']');
}
// object representing an X86 register
class X86Reg(name8: string, name16: string, name32: string, index: int) extends X86Rm {
	def name(size: int) -> string {
		match (size) {
			8 => return name8;
			16 => return name16;
			32 => return name32;
		}
		return null;
	}
	def indirect() -> X86Addr {
		return X86Addr.new(null, this, 1, 0);
	}
	def plus(disp: int) -> X86Addr {
		return X86Addr.new(null, this, 1, disp);
	}
	def plusSSE(disp: int) -> SSEAddr {
		return SSEAddr.new(null, this, 1, disp);
	}
}
// [base + index * scale + imm]
class X86Addr(base: X86Reg, index: X86Reg, scale: byte, disp: int) extends X86Rm {
	new() {
		if (index != null && index.index == X86Regs.ESP.index && scale != 1) {
			System.error("X86AssemblerError", "%esp cannot be scaled");
		}
	}
	def absolute() -> bool { return base == null && index == null; }
	def toSSEAddr() -> SSEAddr { return SSEAddr.new(base, index, scale, disp); }
}
class SSERm {
	def render(buf: StringBuffer) -> StringBuffer {
		if (SSEReg.?(this)) return buf.puts(SSEReg.!(this).name);
		var addr = SSEAddr.!(this);
		return renderAddr(buf, addr.base, addr.index, addr.scale, addr.disp);
	}
}
// SSE floating point registers
class SSEReg(name: string, index: int) extends SSERm {
}
class SSEAddr(base: X86Reg, index: X86Reg, scale: byte, disp: int) extends SSERm {
	new() {
		if (index != null && index.index == X86Regs.ESP.index && scale != 1) {
			System.error("X86AssemblerError", "%esp cannot be scaled");
		}
	}
	def absolute() -> bool { return base == null && index == null; }
	def toX86Addr() -> X86Addr { return X86Addr.new(base, index, scale, disp); }
}
// global constants representing registers
component X86Regs {
	def EAX = X86Reg.new("al", "ax", "eax", 0);
	def ECX = X86Reg.new("cl", "cx", "ecx", 1);
	def EDX = X86Reg.new("dl", "dx", "edx", 2);
	def EBX = X86Reg.new("bl", "bx", "ebx", 3);
	def ESP = X86Reg.new(null, "sp", "esp", 4);
	def EBP = X86Reg.new(null, "bp", "ebp", 5);
	def ESI = X86Reg.new(null, "si", "esi", 6);
	def EDI = X86Reg.new(null, "di", "edi", 7);

	def XMM0 = SSEReg.new("xmm0", 0);
	def XMM1 = SSEReg.new("xmm1", 1);
	def XMM2 = SSEReg.new("xmm2", 2);
	def XMM3 = SSEReg.new("xmm3", 3);
	def XMM4 = SSEReg.new("xmm4", 4);
	def XMM5 = SSEReg.new("xmm5", 5);
	def XMM6 = SSEReg.new("xmm6", 6);
	def XMM7 = SSEReg.new("xmm7", 7);
}
// a condition for use in jumps, set, and cmov
class X86Cond(name: string, index: int) {
	var negate: X86Cond;
	var commute: X86Cond;
}
// conditions for use in jumps, set, and cmov
component X86Conds {
	def ALWAYS: X86Cond;
	def O  = X86Cond.new("o",  0);
	def NO = X86Cond.new("no", 1);
	def C  = X86Cond.new("c",  2);
	def NC = X86Cond.new("nc", 3);
	def Z  = X86Cond.new("z",  4);
	def NZ = X86Cond.new("nz", 5);
	def NA = X86Cond.new("na", 6);
	def A  = X86Cond.new("a",  7);
	def S  = X86Cond.new("s",  8);
	def NS = X86Cond.new("ns", 9);
	def P  = X86Cond.new("p",  10);
	def NP = X86Cond.new("np", 11);
	def L  = X86Cond.new("l",  12);
	def GE = X86Cond.new("ge", 13);
	def LE = X86Cond.new("le", 14);
	def G  = X86Cond.new("g",  15);


	def all = [
		X86Conds.O,
		X86Conds.NO,
		X86Conds.C,
		X86Conds.NC,
		X86Conds.Z,
		X86Conds.NZ,
		X86Conds.NA,
		X86Conds.A,
		X86Conds.S,
		X86Conds.NS,
		X86Conds.P,
		X86Conds.NP,
		X86Conds.L,
		X86Conds.GE,
		X86Conds.LE,
		X86Conds.G
	];

	new() {
		// set up relations between conditions
		neg(O, NO);
		neg(C, NC);
		neg(Z, NZ);
		neg(A, NA);
		neg(S, NS);
		neg(P, NP);
		neg(L, GE);
		neg(G, LE);
		com(Z, Z);
		com(NZ, NZ);
		com(L, G);
		com(LE, GE);
		com(NA, NC);
		com(A, C);
	}
	def neg(a: X86Cond, b: X86Cond) {
		a.negate = b;
		b.negate = a;
	}
	def com(a: X86Cond, b: X86Cond) {
		a.commute = b;
		b.commute = a;
	}
}
// opcodes for the 8 basic integer operators
//----------------- add  or adc sbb and sub xor cmp
def X86OP_rm_r	= "\x01\x09\x11\x19\x21\x29\x31\x39";
def X86OP_r_rm	= "\x03\x0B\x13\x1B\x23\x2B\x33\x3B";
def X86OP_eax_i = "\x05\x0D\x15\x1D\x25\x2D\x35\x3D";

// 2-operand instruction that supports the standard addressing modes
class X86Op2(asm: X86Assembler, operator: int) {
	def rm_r(a: X86Rm, b: X86Reg) { // register/memory, register
		asm.emitb_rm(X86OP_rm_r[operator], a, b.index);
	}
	def r_rm(a: X86Reg, b: X86Rm) { // register, register/memory
		// TODO: this is to match NASM output, streamline
		if (X86Reg.?(b)) asm.emitb_rm(X86OP_rm_r[operator], a, X86Reg.!(b).index);
		else asm.emitb_rm(X86OP_r_rm[operator], b, a.index);
	}
	def rm_i(a: X86Rm, i: int) { // register/memory, immediate
		if (i < -128 || i > 127) {
			if (a == X86Regs.EAX) {
				asm.emitbd(X86OP_eax_i[operator], i);
			} else {
				asm.emitb_rm(0x81, a, operator);
				asm.emitd(i);
			}
		} else {
			asm.emitb_rm(0x83, a, operator);
			asm.emitb(i);
		}
	}
}
def MOD_DISP0  = 0b00000000;
def MOD_DISP8  = 0b01000000;
def MOD_DISP32 = 0b10000000;
def MOD_REG  = 0b11000000;
def MOD_BITS = 0b11000000;
// Assembles x86 instructions into the provided buffer.
class X86Assembler(buffer: DataBuffer) {
	var add: X86Op2;
	var or:	 X86Op2;
	var adc: X86Op2;
	var sbb: X86Op2;
	var and: X86Op2;
	var sub: X86Op2;
	var xor: X86Op2;
	var cmp: X86Op2;

	new() {
		// XXX: avoid creating *eight* X86Op2 objects for each assembler
		add = X86Op2.new(this, 0);
		or  = X86Op2.new(this, 1);
		adc = X86Op2.new(this, 2);
		sbb = X86Op2.new(this, 3);
		and = X86Op2.new(this, 4);
		sub = X86Op2.new(this, 5);
		xor = X86Op2.new(this, 6);
		cmp = X86Op2.new(this, 7);
	}
	def pos() -> int {
		return buffer.pos;
	}
	def bsr(a: X86Reg, b: X86Rm) { emitbb_rm(0x0f, 0xBD, b, a.index); }
	def shl_i(a: X86Rm, imm: int) { // shift left by immediate
		if (imm == 1) return emitb_rm(0xD1, a, 4);
		emitb_rm(0xC1, a, 4);
		emitb(imm);
	}
	def shl_cl(a: X86Rm) { emitb_rm(0xD3, a, 4); } // shift left by value in CL
	def shr_i(a: X86Rm, imm: int) { // shift right by immediate
		if (imm == 1) return emitb_rm(0xD1, a, 5);
		emitb_rm(0xC1, a, 5);
		emitb(imm);
	}
	def sar_i(a: X86Rm, imm: int) { // arithmetic shift right by immediate
		if (imm == 1) return emitb_rm(0xD1, a, 7);
		emitb_rm(0xC1, a, 7);
		emitb(imm);
	}
	def shr_cl(a: X86Rm) { // shift right by value in CL
		emitb_rm(0xD3, a, 5);
	}
	def shrd_cl(a: X86Rm, b: X86Reg) { // shift right by value in CL and shift in bits from b
		emitbb_rm(0x0F, 0xAD, a, b.index);
	}
	def shrd_i(a: X86Rm, b: X86Reg, imm: byte) { // shift right by immediate
		emitbb_rm(0x0F, 0xAC, a, b.index);
		emitb(imm);
	}
	def shld_cl(a: X86Rm, b: X86Reg) { emitbb_rm(0x0F, 0xA5, a, b.index); } // shift left by value in CL and shift in bits from b
	def sar_cl(a: X86Rm) { emitb_rm(0xD3, a, 7); } // arithmetic shift right by value in CL
	def mul(b: X86Rm) { emitb_rm(0xF7, b, 4); } // unsigned multiply (output in edx:eax)
	def imul(b: X86Rm) { emitb_rm(0xF7, b, 5); } // signed multiply (output in edx:eax)
	def imul_r_rm(a: X86Reg, b: X86Rm) { emitbb_rm(0x0F, 0xAF, b, a.index); } // signed multiply
	def imul_r_i(a: X86Reg, i: int) {
		if (i < -128 || i > 127) {
			emitb_rm(0x69, a, a.index);
			emitd(i);
		} else {
			emitb_rm(0x6B, a, a.index);
			emitb(i);
		}
	}
	def div(b: X86Rm) { emitb_rm(0xF7, b, 6); } // unsigned divide (output in edx:eax)
	def idiv(b: X86Rm) { emitb_rm(0xF7, b, 7); } // signed divide (output in edx:eax)
	def inc(a: X86Rm) {
		if (X86Reg.?(a)) return emitb(0x40 + X86Reg.!(a).index);
		emitb_rm(0xFF, a, 0);
	}
	def dec(a: X86Rm) {
		if (X86Reg.?(a)) return emitb(0x48 + X86Reg.!(a).index);
		emitb_rm(0xFF, a, 1);
	}
	def push(a: X86Rm) {
		if (X86Reg.?(a)) return emitb(0x50 + X86Reg.!(a).index);
		emitb_rm(0xFF, a, 6);
	}
	def push_i(imm: int) {
		if (imm < -128 || imm > 127) emitbd(0x68, imm);
		else emitbb(0x6A, imm);
	}
	def pushfd() { emitb(0x9C); }
	def pop(a: X86Rm) {
		if (X86Reg.?(a)) return emitb(0x58 + X86Reg.!(a).index);
		emitb_rm(0x8F, a, 0);
	}
	def jo (off: int) { j(off, 0x70, 0x80); }
	def jno(off: int) { j(off, 0x71, 0x81); }
	def jc (off: int) { j(off, 0x72, 0x82); }
	def jnc(off: int) { j(off, 0x73, 0x83); }
	def jz (off: int) { j(off, 0x74, 0x84); }
	def jnz(off: int) { j(off, 0x75, 0x85); }
	def jna(off: int) { j(off, 0x76, 0x86); }
	def ja (off: int) { j(off, 0x77, 0x87); }
	def js (off: int) { j(off, 0x78, 0x88); }
	def jns(off: int) { j(off, 0x79, 0x89); }
	def jp (off: int) { j(off, 0x7a, 0x8a); }
	def jnp(off: int) { j(off, 0x7b, 0x8b); }
	def jl (off: int) { j(off, 0x7c, 0x8c); }
	def jge(off: int) { j(off, 0x7d, 0x8d); }
	def jle(off: int) { j(off, 0x7e, 0x8e); }
	def jg (off: int) { j(off, 0x7f, 0x8f); }
	def j(off: int, sop: int, lop: int) {
		if (off <= 127 && off >= -128) emitbb(sop, off); // short branch (8-bit)
		else emitbbd(0x0F, lop, off); // long branch (32-bit offset)
	}
	def intK(i: int) { // software interrupt
		if (i == 3) emitb(0xCC);
		else emitbb(0xCD, i);
	}
	def call(off: int) { emitbd(0xE8, off); } // relative call
	def calld(off: int) { emitbd(0x9A, off); } // direct call
	def icall(a: X86Rm) { emitb_rm(0xFF, a, 2); } // indirect call
	def ret() { emitb(0xC3); }
	def iret() { emitb(0xCF); } // return from interrupt
	def jmp(off: int) {
		if (off <= 127 && off >= -128) emitbb(0xEB, off); // short branch (8-bit)
		else emitbd(0xE9, off); // long branch (32-bit offset)
	}
	def jmpd(abs: int) { emitbd(0xEA, abs); } // jump absolute
	def jmpx(cond: X86Cond, off: int) {
		if (cond == null) jmp(off);
		else j(off, 0x70 + cond.index, 0x80 + cond.index);
	}
	def ijmp(a: X86Rm) { emitb_rm(0xFF, a, 4); } // indirect jump
	def lea(a: X86Reg, b: X86Addr) { // load effective address
		emitb_rm(0x8D, b, a.index);
	}
	def movd_rm_r(a: X86Rm, b: X86Reg) {
		if (b == X86Regs.EAX && X86Addr.?(a) && X86Addr.!(a).absolute()) return emitbd(0xA3, X86Addr.!(a).disp);
		emitb_rm(0x89, a, b.index);
	}
	def movd_r_rm(a: X86Reg, b: X86Rm) {
		if (a == X86Regs.EAX && X86Addr.?(b) && X86Addr.!(b).absolute()) return emitbd(0xA1, X86Addr.!(b).disp);
		emitb_rm(0x8B, b, a.index);
	}
	def movd_rm_i(a: X86Rm, i: int) { // move immediate doubleword
		if (X86Reg.?(a)) {
			var r = X86Reg.!(a);
			if (i == 0) xor.r_rm(r, r);
			else emitbd(0xB8 + r.index, i);
		} else {
			emitb_rm(0xC7, a, 0);
			emitd(i);
		}
	}
	def movb_rm_i(a: X86Rm, i: int) { // move immediate byte
		if (X86Reg.?(a)) {
			emitbb(0xB0 + X86Reg.!(a).index, i);
		} else {
			emitb_rm(0xC6, a, 0);
			emitb(i);
		}
	}
	def movb_rm_r(a: X86Rm, b: X86Reg) { emitb_rm(0x88, a, b.index); }
	def movb_r_rm(a: X86Reg, b: X86Rm) { emitb_rm(0x8A, b, a.index); }
	def movbzx(a: X86Reg, b: X86Rm) { emitbb_rm(0x0F, 0xB6, b, a.index); } // byte load, zero extend
	def movbsx(a: X86Reg, b: X86Rm) { emitbb_rm(0x0F, 0xBE, b, a.index); } // byte load, sign extend
	def cmpxchngb(b: X86Rm, a: X86Reg) { emitbb_rm(0x0F, 0xB0, b, a.index); } // compare and exchange byte
	def cmpxchngw(b: X86Rm, a: X86Reg) { // compare and exchange word
		emitb(0x66);
		emitbb_rm(0x0F, 0xB1, b, a.index);
	}
	def cmpxchngd(b: X86Rm, a: X86Reg) { emitbb_rm(0x0F, 0xB1, b, a.index); } // compare and exchange doubleword
	def movw_rm_r(a: X86Rm, b: X86Reg) { emitbb_rm(0x66, 0x89, a, b.index); } // word store
	def movw_rm_i(a: X86Rm, imm: int) { // word store
		emitb(0x66);
		if (X86Reg.?(a)) {
			emitb(0xB8 + X86Reg.!(a).index);
		} else {
			emitb_rm(0xC7, a, 0);
		}
		emitbb(imm & 0xff, imm >> 8);
	}
	def movwzx(a: X86Reg, b: X86Rm) { emitbb_rm(0x0F, 0xB7, b, a.index); } // word load, zero extend
	def movwsx(a: X86Reg, b: X86Rm) { emitbb_rm(0x0F, 0xBF, b, a.index); } // word load, sign extend
	def not(a: X86Rm) { emitb_rm(0xF7, a, 2); }
	def neg(a: X86Rm) { emitb_rm(0xF7, a, 3); }
	def repz() -> this { emitb(0xF3); }
	def repne() -> this { emitb(0xF2); }
	def scasb() { emitb(0xAE); }
	def movsb() { emitb(0xA4); }
	def cdq() { emitb(0x99); } // convert doubleword to quadword
	def seto (a: X86Rm) { set(a, 0x90); }
	def setno(a: X86Rm) { set(a, 0x91); }
	def setc (a: X86Rm) { set(a, 0x92); }
	def setnc(a: X86Rm) { set(a, 0x93); }
	def setz (a: X86Rm) { set(a, 0x94); }
	def setnz(a: X86Rm) { set(a, 0x95); }
	def setna(a: X86Rm) { set(a, 0x96); }
	def seta (a: X86Rm) { set(a, 0x97); }
	def sets (a: X86Rm) { set(a, 0x98); }
	def setns(a: X86Rm) { set(a, 0x99); }
	def setp (a: X86Rm) { set(a, 0x9A); }
	def setnp(a: X86Rm) { set(a, 0x9B); }
	def setl (a: X86Rm) { set(a, 0x9C); }
	def setge(a: X86Rm) { set(a, 0x9D); }
	def setle(a: X86Rm) { set(a, 0x9E); }
	def setg (a: X86Rm) { set(a, 0x9F); }
	def setx(cond: X86Cond, a: X86Rm) { set(a, 0x90 + cond.index); }
	private def set(a: X86Rm, sop: int) { emitbb_rm(0x0f, sop, a, 0); }
	def cmovo (a: X86Reg, b: X86Rm) { cmov(a, b, 0x40); }
	def cmovno(a: X86Reg, b: X86Rm) { cmov(a, b, 0x41); }
	def cmovc (a: X86Reg, b: X86Rm) { cmov(a, b, 0x42); }
	def cmovnc(a: X86Reg, b: X86Rm) { cmov(a, b, 0x43); }
	def cmovz (a: X86Reg, b: X86Rm) { cmov(a, b, 0x44); }
	def cmovnz(a: X86Reg, b: X86Rm) { cmov(a, b, 0x45); }
	def cmovna(a: X86Reg, b: X86Rm) { cmov(a, b, 0x46); }
	def cmova (a: X86Reg, b: X86Rm) { cmov(a, b, 0x47); }
	def cmovs (a: X86Reg, b: X86Rm) { cmov(a, b, 0x48); }
	def cmovns(a: X86Reg, b: X86Rm) { cmov(a, b, 0x49); }
	def cmovp (a: X86Reg, b: X86Rm) { cmov(a, b, 0x4A); }
	def cmovnp(a: X86Reg, b: X86Rm) { cmov(a, b, 0x4B); }
	def cmovl (a: X86Reg, b: X86Rm) { cmov(a, b, 0x4C); }
	def cmovge(a: X86Reg, b: X86Rm) { cmov(a, b, 0x4D); }
	def cmovle(a: X86Reg, b: X86Rm) { cmov(a, b, 0x4E); }
	def cmovg (a: X86Reg, b: X86Rm) { cmov(a, b, 0x4F); }
	def cmovx(cond: X86Cond, a: X86Reg, b: X86Rm) { cmov(a, b, 0x40 + cond.index); }
	private def cmov(a: X86Reg, b: X86Rm, sop: int) { emitbb_rm(0x0f, sop, b, a.index); }
	def sysenter() { emitbb(0x0f, 0x34); }
	def sysexit() { emitbb(0x0f, 0x35); }
	def illegal() { emitbb(0x0F, 0xFF); }
	def test_rm_r(a: X86Rm, b: X86Reg) { emitb_rm(0x85, a, b.index); }
	def test_rm_i(a: X86Rm, i: int) {
		if (a == X86Regs.EAX) {
			emitb(0xA9);
			emitd(i);
		} else {
			emitb_rm(0xF7, a, 0);
			emitd(i);
		}
	}
	def xchg(a: X86Rm, b: X86Reg) {
		if (a == X86Regs.EAX) return emitb(0x90 + b.index);
		if (X86Reg.?(a)) {
			var ra = X86Reg.!(a);
			if (b == X86Regs.EAX) return emitb(0x90 + ra.index);
			else return emitb_rm(0x87, b, ra.index);
		}
		return emitb_rm(0x87, a, b.index);
	}
	def xadd(a: X86Rm, b: X86Reg) {
		emitbb_rm(0x0F, 0xC1, a, b.index);
	}
	def addsd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x58, a, b); }
	def subsd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x5C, a, b); }
	def mulsd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x59, a, b); }
	def divsd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x5E, a, b); }
	def sqrtsd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x51, a, b); }
	def maxsd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x5F, a, b); }
	def minsd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x5D, a, b); }
	def ucomisd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0x66, 0x0F, 0x2E, a, b); }

	def cmpeqsd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x0); }
	def cmpltsd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x1); }
	def cmplesd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x2); }
	def cmpunordsd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x3); }
	def cmpneqsd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x4); }
	def cmpnltsd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x5); }
	def cmpnlesd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x6); }
	def cmpordsd(a: SSEReg, b: SSERm) { cmpsd(a, b, 0x7); }
	private def cmpsd(a: SSEReg, b: SSERm, c: byte) {
		emitbbb_s_sm(0xF2, 0x0F, 0xC2, a, b);
		emitb(c);
	}
	def roundsd(a: SSEReg, b: SSERm, c: RoundingMode) {
		emitbbbb_s_sm(0x66, 0x0F, 0x3A, 0x0B, a, b);
		emitb(c.value);
	}

	def addss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x58, a, b); }
	def subss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x5C, a, b); }
	def mulss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x59, a, b); }
	def divss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x5E, a, b); }
	def sqrtss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x51, a, b); }
	def maxss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x5F, a, b); }
	def minss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x5D, a, b); }
	def ucomiss(a: SSEReg, b: SSERm) {
		emitbb(0x0F, 0x2E);
		emit_sm(b, a.index);
	}
	def cmpeqss(a: SSEReg, b: SSERm) { cmpss(a, b, 0x0); }
	def cmpltss(a: SSEReg, b: SSERm) { cmpss(a, b, 0x1); }
	def cmpless(a: SSEReg, b: SSERm) { cmpss(a, b, 0x2); }
	def cmpunordss(a: SSEReg, b: SSERm) { cmpss(a, b, 0x3); }
	def cmpneqss(a: SSEReg, b: SSERm) { cmpss(a, b, 0x4); }
	def cmpnltss(a: SSEReg, b: SSERm) { cmpss(a, b, 0x5); }
	def cmpnless(a: SSEReg, b: SSERm) { cmpss(a, b, 0x6); }
	def cmpordss(a: SSEReg, b: SSERm) { cmpss(a, b, 0x7); }
	private def cmpss(a: SSEReg, b: SSERm, c: byte) {
		emitbbb_s_sm(0xF3, 0x0F, 0xC2, a, b);
		emitb(c);
	}
	def roundss(a: SSEReg, b: SSERm, c: RoundingMode) {
		emitbbbb_s_sm(0x66, 0x0F, 0x3A, 0x0A, a, b);
		emitb(c.value);
	}

	def cvtsd2si(a: X86Reg, b: SSERm) {
		emitbbb(0xF2, 0x0F, 0x2D);
		emit_sm(b, a.index);
	}
	def cvtss2si(a: X86Reg, b: SSERm) {
		emitbbb(0xF3, 0x0F, 0x2D);
		emit_sm(b, a.index);
	}
	def cvtsi2sd(a: SSEReg, b: X86Rm) {
		emitbbb(0xF2, 0x0F, 0x2A);
		emit_rm(b, a.index);
	}
	def cvtsi2ss(a: SSEReg, b: X86Rm) {
		emitbbb(0xF3, 0x0F, 0x2A);
		emit_rm(b, a.index);
	}
	def cvtss2sd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF3, 0x0F, 0x5A, a, b); }
	def cvtsd2ss(a: SSEReg, b: SSERm) { emitbbb_s_sm(0xF2, 0x0F, 0x5A, a, b); }
	def cvttss2si(a: X86Reg, b: SSERm) {
		emitbbb(0xF3, 0x0F, 0x2C);
		emit_sm(b, a.index);
	}
	def cvttsd2si(a: X86Reg, b: SSERm) {
		emitbbb(0xF2, 0x0F, 0x2C);
		emit_sm(b, a.index);
	}

	def movss_sm_s(a: SSERm, b: SSEReg) { // store float
		emitbbb(0xF3, 0x0F, 0x11);
		emit_sm(a, b.index);
	}
	def movss_s_sm(a: SSEReg, b: SSERm) { // load float
		emitbbb(0xF3, 0x0F, 0x10);
		emit_sm(b, a.index);
	}
	def movsd_sm_s(a: SSERm, b: SSEReg) { // store double
		emitbbb(0xF2, 0x0F, 0x11);
		emit_sm(a, b.index);
	}
	def movsd_s_sm(a: SSEReg, b: SSERm) { // load double
		emitbbb(0xF2, 0x0F, 0x10);
		emit_sm(b, a.index);
	}

	def movd_s_rm(a: SSEReg, b: X86Rm) {
		emitbbb(0x66, 0x0F, 0x6E);
		emit_rm(b, a.index);
	}
	def movd_rm_s(a: X86Rm, b: SSEReg) {
		emitbbb(0x66, 0x0F, 0x7E);
		emit_rm(a, b.index);
	}
	def movq_s_sm(a: SSEReg, b: SSERm) {
		emitbbb(0xF3, 0x0F, 0x7E);
		emit_sm(b, a.index);
	}
	def movq_sm_s(a: SSERm, b: SSEReg) {
		emitbbb(0x66, 0x0F, 0xD6);
		emit_sm(a, b.index);
	}

	def andps(a: SSEReg, b: SSERm) { emitbb_s_sm(0x0F, 0x54, a, b); }
	def andpd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0x66, 0x0F, 0x54, a, b); }
	def xorps(a: SSEReg, b: SSERm) { emitbb_s_sm(0x0F, 0x57, a, b); }
	def xorpd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0x66, 0x0F, 0x57, a, b); }

	def psrld_i(a: SSEReg, imm: byte) { emitbbb_si_b(0x66, 0x0F, 0x72, a, 0x2, imm); }
	def psrlq_i(a: SSEReg, imm: byte) { emitbbb_si_b(0x66, 0x0F, 0x73, a, 0x2, imm); }
	def pslld_i(a: SSEReg, imm: byte) { emitbbb_si_b(0x66, 0x0F, 0x72, a, 0x6, imm); }
	def psllq_i(a: SSEReg, imm: byte) { emitbbb_si_b(0x66, 0x0F, 0x73, a, 0x6, imm); }

	def pcmpeqd(a: SSEReg, b: SSERm) { emitbbb_s_sm(0x66, 0x0F, 0x76, a, b); }
	def pcmpeqq(a: SSEReg, b: SSERm) { emitbbbb_s_sm(0x66, 0x0F, 0x38, 0x29, a, b); }

	// x87 FPU instructions
	def fadd_d(a: SSEAddr) {
		emitb(0xD8);
		emit_sm(a, 0x0);
	}
	def fld_d(a: SSEAddr) {
		emitb(0xD9);
		emit_sm(a, 0x0);
	}
	def fld_q(a: SSEAddr) {
		emitb(0xDD);
		emit_sm(a, 0x0);
	}
	def fild_q(a: SSEAddr) {
		emitb(0xDF);
		emit_sm(a, 0x5);
	}
	def fstp_d(a: SSEAddr) {
		emitb(0xD9);
		emit_sm(a, 0x3);
	}
	def fstp_q(a: SSEAddr) {
		emitb(0xDD);
		emit_sm(a, 0x3);
	}
	def fisttp_q(a: SSEAddr) {
		emitb(0xDD);
		emit_sm(a, 0x1);
	}

	def emitbbb_s_sm(b1: byte, b2: byte, b3: byte, a: SSEReg, b: SSERm) {
		emitbbb(b1, b2, b3);
		emit_sm(b, a.index);
	}
	def emitbbb_si_b(b1: byte, b2: byte, b3: byte, a: SSEReg, b: int, c: byte) {
		emitbbb(b1, b2, b3);
		emit_sm(a, b);
		emitb(c);
	}
	def emitbb_s_sm(b1: byte, b2: byte, a: SSEReg, b: SSERm) {
		emitbb(b1, b2);
		emit_sm(b, a.index);
	}

	def emitbbbb_s_sm(b1: byte, b2: byte, b3: byte, b4: byte,
			a: SSEReg, b: SSERm) {
		emitbbb(b1, b2, b3);
		emitb(b4);
		emit_sm(b, a.index);
	}

	def emitb_rm(b0: int, a: X86Rm, eop: int) {
		emitb(b0);
		emit_rm(a, eop);
	}
	def emitbb_rm(b0: int, b1: int, a: X86Rm, eop: int) {
		emitbb(b0, b1);
		emit_rm(a, eop);
	}
	def emit_rm(a: X86Rm, eop: int) {
		eop = (eop & 0b111) << 3;
		if (X86Reg.?(a)) {
			// register addressing mode
			return emitb(MOD_REG | eop | X86Reg.!(a).index); // mod = 11
		}
		var addr = X86Addr.!(a);
		return emit_m(addr.base, addr.index, addr.scale, addr.disp, eop);
	}
	def emit_sm(a: SSERm, eop: int) {
		eop = (eop & 0b111) << 3;
		if (SSEReg.?(a)) {
			// register addressing mode
			return emitb(MOD_REG | eop | SSEReg.!(a).index); // mod = 11
		}
		var addr = SSEAddr.!(a);
		return emit_m(addr.base, addr.index, addr.scale, addr.disp, eop);
	}
	def emit_m(base: X86Reg, index: X86Reg, scale: byte, disp: int, eop: int) {
		// memory addressing mode
		if (index == null) {
			if (base == null) {
				// absolute 32-bit address.
				return emitbd(eop | X86Regs.EBP.index, disp);
			}
			// base register only
			return emit_rm_1(eop, base.index, disp);
		}

		if (base == null) {
			if (scale == 1) {
				// index register only
				return emit_rm_1(eop, index.index, disp);
			}
			if (scale == 2) {
				// reg*2 => reg+reg
				scale = 1;
				base = index;
			}
		}

		// compute low part of mod/rm byte
		var mod_rm = eop | X86Regs.ESP.index; // ESP indicates SIB byte comes next

		// compute sib byte
		var sib = index.index << 3;
		if (scale == 2) {
			sib |= 0b01000000;
		} else if (scale == 4) {
			sib |= 0b10000000;
		} else if (scale == 8) {
			sib |= 0b11000000;
		}

		if (base != null) {
			sib |= base.index;
			// finish mod/rm byte
			if (disp < -128 || disp > 127) mod_rm |= MOD_DISP32;		// disp32
			else if (disp != 0) mod_rm |= MOD_DISP8;			// disp8
			else if (base.index == X86Regs.EBP.index) mod_rm |= MOD_DISP8;	// force disp8 for EBP+0
		} else {
			// no base register => mod=00 but must emit disp32
			sib |= X86Regs.EBP.index;
		}

		// emit code
		if (base == null || disp < -128 || disp > 127) emitbbd(mod_rm, sib, disp);
		else if ((mod_rm & MOD_BITS) == MOD_DISP8) emitbbb(mod_rm, sib, disp);
		else emitbb(mod_rm, sib);
	}
	private def emit_rm_1(eop: int, regnum: int, disp: int) {
		def mod_rm = eop | regnum;
		if (regnum == X86Regs.ESP.index) {
			def sib = 0b00100100;
			if (disp == 0) return emitbb(MOD_DISP0 | mod_rm, sib);
			else if (disp < -128 || disp > 127) emitbbd(MOD_DISP32 | mod_rm, sib, disp);
			else emitbbb(MOD_DISP8 | mod_rm, sib, disp);
		} else if (disp == 0) {
			if (regnum == X86Regs.EBP.index) emitbb(MOD_DISP8 | mod_rm, 0); // +disp8=0 for EBP
			else emitb(MOD_DISP0 | mod_rm);
		} else if (disp < -128 || disp > 127) {
			emitbd(MOD_DISP32 | mod_rm, disp);
		} else {
			emitbb(MOD_DISP8 | mod_rm, disp);
		}
	}
	def emitb(b0: int) {
		buffer.i1(b0);
	}
	def emitd(d0: int) {
		buffer.i4le(d0);
	}
	def emitbd(b0: int, d0: int) {
		buffer.i1(b0);
		buffer.i4le(d0);
	}
	def emitbb(b0: int, b1: int) {
		buffer.bb(b0, b1);
	}
	def emitbbb(b0: int, b1: int, b2: int) {
		buffer.bbb(b0, b1, b2);
	}
	def emitbbd(b0: int, b1: int, d0: int) {
		buffer.bb(b0, b1);
		buffer.i4le(d0);
	}
}
