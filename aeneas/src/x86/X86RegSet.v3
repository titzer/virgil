// Copyright 2011 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Register sets for X86, used in register allocation.
component X86MachRegs {
	def EAX = '\x01';
	def ECX = '\x02';
	def EDX = '\x03';
	def EBX = '\x04';
	def ESI = '\x05';
	def EDI = '\x06';
	def XMM0 = '\x07';
	def XMM1 = '\x08';
	def XMM2 = '\x09';
	def XMM3 = '\x0a';
	def XMM4 = '\x0b';
	def XMM5 = '\x0c';
	def XMM6 = '\x0d';
	def XMM7 = '\x0e';
	def EBP = '\x0f';
	def GPR = '\x10';
	def BYTE = '\x11';
	def EAX_EDX = '\x12';
	def NOT_EDX = '\x13';
	def NOT_ECX = '\x14';
	def GPR_CLASS = '\x15';
	def SSE_CLASS = '\x16';
	def ALL = '\x1d';
	def SCRATCH = X86Regs.EBP;
	def SSE_SCRATCH = X86Regs.XMM7;

	// a billion spill slots ought to be enough...
	def CALLER_SPILL_START = 1000000000;
	def CALLEE_SPILL_START = 2000000000;

	var regs: MachRegSet;

	new() {
		var rarray = Array<Array<byte>>.new(ALL + 1);
		rarray[EAX] = [EAX];
		rarray[EBX] = [EBX];
		rarray[ECX] = [ECX];
		rarray[EDX] = [EDX];
		rarray[ESI] = [ESI];
		rarray[EDI] = [EDI];
		rarray[XMM0] = [XMM0];
		rarray[XMM1] = [XMM1];
		rarray[XMM2] = [XMM2];
		rarray[XMM3] = [XMM3];
		rarray[XMM4] = [XMM4];
		rarray[XMM5] = [XMM5];
		rarray[XMM6] = [XMM6];
		rarray[XMM7] = [XMM7];
		rarray[EBP] = [];	// XXX: EBP is off limits because it's scratch
		rarray[GPR] = [EAX, EBX, ECX, EDX, ESI, EDI];
		rarray[BYTE] = [EDX, ECX, EAX, EBX];
		rarray[EAX_EDX] = [EAX, EDX];
		rarray[NOT_EDX] = [EAX, EBX, ECX, ESI, EDI];
		rarray[NOT_ECX] = [EAX, EBX, EDX, ESI, EDI];
		rarray[GPR_CLASS] = [EAX, ECX, EDX, EBX, ESI, EDI];
		rarray[SSE_CLASS] = [XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7];
		rarray[ALL] = [EAX, EBX, ECX, EDX, ESI, EDI, XMM0, XMM1, XMM2, XMM3, XMM4, XMM5, XMM6, XMM7];

		// TODO: workaround for unstable move insertion order
		for (i < rarray.length) {
			if (rarray[i] == null) rarray[i] = [];
		}

		var rnames = Array<string>.new(rarray.length);
		rnames[EAX] = "eax";
		rnames[EBX] = "ebx";
		rnames[ECX] = "ecx";
		rnames[EDX] = "edx";
		rnames[ESI] = "esi";
		rnames[EDI] = "edi";
		rnames[XMM0] = "xmm0";
		rnames[XMM1] = "xmm1";
		rnames[XMM2] = "xmm2";
		rnames[XMM3] = "xmm3";
		rnames[XMM4] = "xmm4";
		rnames[XMM5] = "xmm5";
		rnames[XMM6] = "xmm6";
		rnames[XMM7] = "xmm7";
		rnames[EBP] = "ebp";
		rnames[GPR] = "{gpr}";
		rnames[BYTE] = "{byte}";
		rnames[EAX_EDX] = "{eax,edx}";
		rnames[NOT_EDX] = "{~edx}";
		rnames[NOT_ECX] = "{~ecx}";
		rnames[GPR_CLASS] = "{gpr}";
		rnames[SSE_CLASS] = "{sse}";
		rnames[ALL] = "{all}";

		// TODO: workaround for unstable move insertion order
		for (i < rnames.length) {
			if (rnames[i] == null) rnames[i] = [];
		}

		var rclasses = Array<byte>.new(2);
		rclasses[0] = GPR_CLASS;
		rclasses[1] = SSE_CLASS;

		// XXX: don't use EBP for scratch
		regs = MachRegSet.new(14, rarray, rnames, rclasses, EBP, CALLER_SPILL_START, CALLEE_SPILL_START);
	}
}
// Defines the standard parameter and return registers for X86 calls between Virgil methods.
component X86VirgilCallConv {
	def paramRegs = [X86MachRegs.EDI, X86MachRegs.EAX, X86MachRegs.EDX, X86MachRegs.ECX, X86MachRegs.ESI];
	def retRegs = [X86MachRegs.EAX, X86MachRegs.EDX, X86MachRegs.ECX, X86MachRegs.ESI];
	def floatParamRegs = [X86MachRegs.XMM0, X86MachRegs.XMM1, X86MachRegs.XMM2, X86MachRegs.XMM3,
			X86MachRegs.XMM4, X86MachRegs.XMM5, X86MachRegs.XMM6];
	def floatRetRegs = [X86MachRegs.XMM0, X86MachRegs.XMM1];

	private def compute(paramTypes: Array<Type>, returnTypes: Array<Type>) -> MachCallConv {
		// compute the locations of each parameter
		var spillStart = X86MachRegs.regs.spillStart;
		var ploc = Array<int>.new(paramTypes.length);
		var pspill = 0, iprm = 0, fprm = 0;
		for (i < ploc.length) {
			match (Regs.toRegClass(paramTypes[i])) {
				REF, I32 => {
					if (iprm < paramRegs.length) ploc[i] = paramRegs[iprm++];
					else ploc[i] = spillStart + pspill++;
				}
				F32 => { // TODO: make F32 also spill 2 slots
					if (fprm < floatParamRegs.length) ploc[i] = floatParamRegs[fprm++];
					else ploc[i] = spillStart + pspill++;
				}
				_ => { // I64 or F64
					ploc[i] = spillStart + pspill;
					pspill += 2;
				}
			}
		}
		// compute locations of each return value
		var len = returnTypes.length;
		if (len == 0) len = 1;  // TODO: fix return count
		var rloc = Array<int>.new(len), rspill = 0;
		iprm = 0;
		fprm = 0;
		if (returnTypes.length != 0) {	// XXX: working around the return count
			for (i < rloc.length) {
				match (Regs.toRegClass(returnTypes[i])) {
					REF, I32 => {
						if (iprm < retRegs.length) rloc[i] = retRegs[iprm++];
						else rloc[i] = spillStart + rspill++;
					}
					F32 => { // TODO: make F32 also spill 2 slots
						if (fprm < floatRetRegs.length) rloc[i] = floatRetRegs[fprm++];
						else rloc[i] = spillStart + rspill++;
					}
					_ => { // I64 or F64
						ploc[i] = spillStart + pspill;
						pspill += 2;
					}
				}
			}
		} else {
			rloc[0] = retRegs[0];
		}
		if (pspill > rspill) rspill = pspill;
		return MachCallConv.new(X86MachRegs.regs, paramTypes, ploc, returnTypes, rloc, rspill);
	}
	def getForGraph(graph: SsaGraph) -> MachCallConv {
		// XXX: cache calling conventions by type for SSA graph
		return compute(Arrays.map(graph.params, SsaParam.vtype), Tuple.toTypeArray(graph.returnType));
	}
	def getForFunc(funcRep: Mach_FuncRep) -> MachCallConv {
		if (funcRep.callConv != null) return funcRep.callConv;
		return funcRep.callConv = compute(funcRep.paramTypes, funcRep.returnTypes);
	}
}
