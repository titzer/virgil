// Copyright 2016 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

def X86_ADD    = 0x02;
def X86_OR     = 0x03;
def X86_ADC    = 0x04;
def X86_SBB    = 0x05;
def X86_AND    = 0x06;
def X86_SUB    = 0x07;
def X86_XOR    = 0x08;
def X86_CMP    = 0x09;
def X86_JMP    = 0x0a;
def X86_JC     = 0x0b;
def X86_SETC   = 0x0c;
def X86_CMOVC  = 0x0d;
def X86_THROW  = 0x0e;
def X86_THROWC = 0x0f;
def X86_SWITCH = 0x10;
def X86_MUL    = 0x12;
def X86_NEG    = 0x13;
def X86_NOT    = 0x14;
def X86_TEST   = 0x15;
def X86_MOV    = 0x16;
def X86_LEA    = 0x18;
def X86_LD8    = 0x19;
def X86_LDS8   = 0x1a;
def X86_LDU8   = 0x1b;
def X86_LD16   = 0x1c;
def X86_LDS16  = 0x1d;
def X86_LDU16  = 0x1e;
def X86_LD32   = 0x1f;
def X86_ST8    = 0x20;
def X86_ST16   = 0x21;
def X86_ST32   = 0x22;
def X86_CXG8   = 0x23;
def X86_CXG16  = 0x24;
def X86_CXG32  = 0x25;
def X86_DIV    = 0x26;
def X86_IDIV   = 0x27;
def X86_INC    = 0x28;
def X86_DEC    = 0x29;
def X86_SHL    = 0x2a;
def X86_SAR    = 0x2b;
def X86_SHR    = 0x2c;
def X86_SEXT   = 0x2d;
def X86_CALL   = 0x2e;
def X86_CALLER_IP = 0x2f;
def X86_CALLER_SP = 0x30;
// Addressing modes:
// REG = register, OP = register/stack, IMM = immediate
// MRRSD = [reg + reg * scale + disp]
def X86A_NONE	   = 0x00;
def X86A_OP_REG	   = 0x01;
def X86A_OP_IMM	   = 0x02;
def X86A_REG_OP	   = 0x03;
def X86A_ADDR_IMM  = 0x04;
def X86A_ADDR_REG  = 0x05;
def X86A_MRRSD_REG = 0x06;
def X86A_MRRSD_IMM = 0x07;
def X86A_REG_MRRSD = 0x08;
def X86A_OP	   = 0x09;

def ABS_MARKER = 0x11223344;
def REL_MARKER = 0xAABBCCDD;

def MATCH_NEG = true;
def MATCH_OP_I = true;
def MATCH_ADDR_ADD = true;
def MATCH_ADDR_ADD_IMM = true;
def MATCH_ADDR_SCALE = true;
def MATCH_SCALE = true;

// Burndown
// wide arithmetic
// subword arithmetic
// calls

// XXX encode scale in opcode?
class X86CodeGen2 extends ArchCodeGen {
	def rt: MachRuntime;
	def asm: X86MacroAssembler;
	def m = SsaInstrMatcher.new();

	new(context: SsaContext, blocks: SsaBlockOrder, mach: MachProgram, rt, asm, w: MachDataWriter) super(context, mach, X86MachRegs.regs, w) {
		anyReg = X86MachRegs.GPR;
	}

	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => emitSimpleBinop(X86_ADD, i, m.intbinop(i));
			IntSub => {
				var yval = m.intbinop(i);
				if (MATCH_NEG && m.xconst && m.xint == 0) return emit2(X86_NEG, ovwReg(i), useReg(m.y));
				emitSimpleBinop(X86_SUB, i, yval);
			}
			IntMul => {
				// XXX: multiply by 1, 2, 3, 4, 5, 9, powers of 2
				var yval = m.intbinop(i);
				if (MATCH_OP_I && m.yconst) return emit3(X86_MUL, ovwReg(i), useReg(m.x), useInt(yval));
				return emit3(X86_MUL, ovwReg(i), useReg(m.x), use(m.y));
			}
			IntDiv => {
				// TODO: signed / cdq
				var signed = V3.isSigned(i.op.typeArgs[0]);
				var d = dfnFixed(i, X86MachRegs.EAX);
				var x = kill(-1, X86MachRegs.EDX);
				var a = useFixed(i.input0(), X86MachRegs.EAX);
				var b = useConstant(context.graph.zeroConst(), X86MachRegs.EDX);
				var c = use(i.input1());
				// TODO: source position for DivideByZeroException
				return emitN(if(signed, X86_IDIV, X86_DIV));
			}
			IntMod => {
				// TODO: signed / cdq
				var signed = V3.isSigned(i.op.typeArgs[0]);
				var d = dfnFixed(i, X86MachRegs.EDX);
				var x = kill(-1, X86MachRegs.EAX);
				var a = useFixed(i.input0(), X86MachRegs.EAX);
				var b = useConstant(context.graph.zeroConst(), X86MachRegs.EDX);
				var c = use(i.input1());
				// TODO: source position for DivideByZeroException
				return emitN(if(signed, X86_IDIV, X86_DIV));
			}
			IntAnd => emitSimpleBinop(X86_AND, i, m.intbinop(i));
			IntOr => emitSimpleBinop(X86_OR, i, m.intbinop(i));
			IntXor => emitSimpleBinop(X86_XOR, i, m.intbinop(i));
			IntShl => visitShift(block, i, X86_SHL);
			IntSar => visitShift(block, i, X86_SAR);
			IntShr => visitShift(block, i, X86_SHR);
			IntWide => ; // TODO
			IntConvert => {
				var tt = IntType.!(i.op.typeArgs[1]);
				if (tt.width == 32) {
					// XXX: make sure nop int converts are removed by lowering
					emit2(X86_MOV | (X86A_REG_OP << 8), dfnReg(i), use(i.input0()));
				} else if (!tt.signed) {
					emit3(X86_AND, dfnReg(i), ovwReg(i.input0()), useImm(tt.max));
				} else {
					emit3(X86_SEXT, dfnReg(i), ovwReg(i.input0()), useInt(tt.width));
				}
			}
			BoolEq,
			BoolNot,
			RefEq,
			IntEq,
			IntLt,
			IntLteq,
			PtrLt,
			PtrLteq => {
				var cmp = matchCmp(i, false);
				emitCmp(cmp);
				// TODO: zero-extend?
				emit1(ArchInstrs.FLAG_NO_GAP | X86_SETC | encodeCond(cmp.cond), dfnFixed(i, X86MachRegs.BYTE));
			}
			ConditionalThrow(exception) => {
				var cmp = matchCmp(i.input0(), true);
				emitCmp(cmp);
				emit1(ArchInstrs.FLAG_NO_GAP | X86_THROWC | encodeCond(cmp.cond), useExSource(exception, i.source));
			}
			PtrLoad => {
				var lt = i.op.typeArgs[1], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				if (size == 0) {
					useMrrsd(t);
					useInt(0);
					return emitN(X86_TEST | X86A_MRRSD_IMM << 8);
				}
				var opcode: int, signed = V3.isSigned(lt);
				if (size == 1) opcode = if(signed, X86_LDS8, X86_LDU8);
				else if (size == 2) opcode = if(signed, X86_LDS16, X86_LDU16);
				else if (size == 4) opcode = X86_LD32;
				else context.fail("invalid size for load");
				dfnReg(i);
				useMrrsd(t);
				// TODO: source position for NullCheckException
				return emitN(opcode | (X86A_REG_MRRSD << 8));
			}
			PtrStore => {
				var lt = i.op.typeArgs[0], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				if (size == 0) {
					useMrrsd(t);
					useInt(0);
					return emitN(X86_TEST | X86A_MRRSD_IMM << 8);
				}
				var opcode: int;
				useMrrsd(t);
				if (size == 1) {
					useFixed(i.input1(), X86MachRegs.BYTE);
					opcode = X86_ST8;
				} else if (size == 2) {
					useReg(i.input1());
					opcode = X86_ST16;
				} else if (size == 4) {
					useReg(i.input1());
					opcode = X86_ST32;
				} else {
					context.fail("invalid size for store");
				}
				// TODO: source position for NullCheckException
				return emitN(opcode | (X86A_MRRSD_REG << 8));
			}
			PtrCmpSwp => {
				var lt = i.op.typeArgs[0], size = mach.sizeOf(lt);
				var t = matchMrrsd(i.input0());
				var opcode: int;
				useMrrsd(t);
				var expect = useFixed(i.input1(), X86MachRegs.EAX);
				if (size == 1) {
					useFixed(i.input1(), X86MachRegs.BYTE);
					opcode = X86_CXG8;
				} else if (size == 2) {
					useReg(i.input1());
					opcode = X86_CXG16;
				} else if (size == 4) {
					useReg(i.input1());
					opcode = X86_CXG32;
				} else {
					context.fail("invalid size for cmpswp");
				}
				// TODO: source position for NullCheckException
				emitN(opcode | (X86A_MRRSD_REG << 8));
				emit1(ArchInstrs.FLAG_NO_GAP | X86_SETC | encodeCond(X86Conds.Z), dfnFixed(i, X86MachRegs.BYTE));
			}
			PtrAdd => {
				// XXX: use binop matching for PtrAdd
				emit3(X86_ADD, dfnReg(i), ovwReg(i.input0()), useReg(i.input1()));
			}
			PtrSub => {
				// XXX: use binop matching for PtrAdd
				emit3(X86_SUB, dfnReg(i), ovwReg(i.input0()), useReg(i.input1()));
			}
			Alloc => ;	// TODO
			CallAddress(funcRep) => visitCall(block, i, funcRep);
			CallerIp => {
				emit1(X86_CALLER_IP, dfnReg(i));
			}
			CallerSp => {
				emit1(X86_CALLER_SP, dfnReg(i));
			}
			CallKernel => ; // TODO
			_ => return context.fail("unexpected opcode");
		}
	}
	def visitCall(block: SsaBlock, call: SsaApplyOp, funcRep: Mach_FuncRep) {
		var func = call.input0(), mi: MachInstr;
		var conv = frame.allocCallerSpace(X86VirgilCallConv.getForFunc(funcRep));

		// define the return value(s) of the call
		var rv = getProjections(call);
		for (i < rv.length) {
			dfnFixed(rv[i], conv.calleeRet(i));
		}
		// TODO: var lp = if(rtgc != null, livePoint());
		kill(newLivepoint(), X86MachRegs.ALL);	// TODO: split liveness across call
		var skip = 0;
		if (SsaConst.?(func)) {
			var target = Address<IrMethod>.!(SsaConst.!(func).val);
			useImm(target);
			if (V3.isComponent(target.val.receiver)) skip = 1;
		} else {
			useReg(func);
		}

		// use the arguments to the call
		var inputs = call.inputs;
		for (i = 1 + skip; i < inputs.length; i++) {  // input[0] == func
			useFixed(inputs[i].dest, conv.calleeParam(i - 1));
		}
		emitN(X86_CALL);
	}
	def visitShift(block: SsaBlock, i: SsaApplyOp, opcode: int) {
		var yval = m.intbinop(i);
		if (MATCH_OP_I && m.yconst) return emit3(opcode, ovwReg(i), useReg(m.x), useInt(yval));
		return emit3(opcode, ovwReg(i), useReg(m.x), useFixed(m.y, X86MachRegs.ECX));
	}
	def useMrrsd(x: SsaInstr, y: SsaInstr, scale: byte, disp: Val) {
		if(x == null) useInt(0);
		else useReg(x);
		useReg(y);
		useInt(scale);
		useImm(disp);
	}
	def matchMrrsd(i: SsaInstr) -> (SsaInstr, SsaInstr, byte, Val) {
		var t = matchAddImm(i), disp = t.1;
		i = t.0;
		var xadd: SsaApplyOp;
		if (MATCH_ADDR_ADD && (xadd = cover(Opcode.PtrAdd.tag, i)) != null) {
			var x = xadd.input0(), y = xadd.input1();
			var ys = matchScale(y);
			if (ys.1 != 1) return (x, ys.0, ys.1, disp);
			var xs = matchScale(x);
			if (xs.1 != 1) return (y, xs.0, xs.1, disp);
			return (x, y, 1, disp);
		}
		var is = matchScale(i);
		return (null, is.0, is.1, disp);
	}
	def matchScale(i: SsaInstr) -> (SsaInstr, byte) {
		if (!MATCH_SCALE || !SsaApplyOp.?(i)) return (i, 1);
		var apply = SsaApplyOp.!(i);
		if (Opcode.IntMul.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 1) return (m.x, 1);
			if (yval == 2) return (m.x, 2);
			if (yval == 4) return (m.x, 4);
			if (yval == 8) return (m.x, 8);
		} else if (Opcode.IntShl.?(apply.op.opcode)) {
			var yval = m.intbinop(apply);
			if (!m.yconst) return (apply, 1);
			if (yval == 0) return (m.x, 1);
			if (yval == 1) return (m.x, 2);
			if (yval == 2) return (m.x, 4);
			if (yval == 3) return (m.x, 8);
		}
		return (apply, 1);
	}
	def emitSimpleBinop(opcode: int, i: SsaApplyOp, yval: int) {
		// XXX: select better left operand using liveness
		// XXX: cover loads with mrrsd operand
		if (MATCH_OP_I && m.yconst) return emit3(opcode, ovwReg(i), useReg(m.x), useInt(yval));
		return emit3(opcode, ovwReg(i), useReg(m.x), use(m.y));
	}
	def visitThrow(block: SsaBlock, i: SsaThrow) {
		emit1(X86_THROW, useExSource(i.exception, i.source));
	}
	def visitIf(block: SsaBlock, i: SsaIf) {
		var key = i.input0(), cmp = matchCmp(key, true), succ = i.block().succ;
		var s0 = succ(0).dest, s1 = succ(1).dest, target: SsaBlock, jmp: SsaBlock;
		if (blocks.isImmediatelyAfter(context.block, s1)) { // fall through to s1
			target = s0;
		} else if (blocks.isImmediatelyAfter(context.block, s0)) {  // fall through to s0
			cmp = cmp.negate();
			target = s1;
		} else {  // cannot fall through
			target = s0;
			jmp = s1;
		}
		emitCmp(cmp);
		emit1(ArchInstrs.FLAG_NO_GAP | X86_JC | encodeCond(cmp.cond), useLabel(target));
		if (jmp != null) emit1(ArchInstrs.FLAG_NO_GAP | X86_JMP, useLabel(jmp));
	}
	def visitSwitch(block: SsaBlock, i: SsaSwitch) {
		use(i.input0());
		useImm(Int.box(i.minValue));
		useScratch();
		for (s in block.succs()) useLabel(s.dest);
		emitN(X86_SWITCH);
	}
	def visitGoto(block: SsaBlock, i: SsaGoto) {
		var target = i.target();
		if (!blocks.isImmediatelyAfter(context.block, target)) {
			emit1(X86_JMP, useLabel(target));  // jump to block if not successor
		}
	}

	def emitCmp(cmp: X86CmpMatch) {
		if (cmp.y == null) {
			emit2(X86_CMP, use(cmp.x), useImm(cmp.val));
		} else {
			emit2(X86_CMP, useReg(cmp.x), use(cmp.y));
		}
	}

	def assemble(opcode: int, a: Array<Operand>) {
		var start = asm.pos(), addr: Addr;
		var code = opcode & 0xff, mode = opcode >> 8;
		match (mode) {
			X86A_NONE => {
				assemble_none(code, a);
			}
			X86A_OP => {
				assemble_rm(code, toRm(a[0]));
			}
			X86A_OP_REG => {
				assemble_rm_r(code, toRm(a[0]), toReg(a[1]));
			}
			X86A_OP_IMM => {
				assemble_rm_i(code, toRm(a[0]), toImm(a[1]));
			}
			X86A_REG_OP => {
				assemble_r_rm(code, toReg(a[0]), toRm(a[1]));
			}
			X86A_MRRSD_REG => {
				var t = toMrrsd(a, 0);
				addr = t.1;
				assemble_rm_r(code, t.0, toReg(a[4]));
			}
			X86A_MRRSD_IMM => {
				var t = toMrrsd(a, 0);
				addr = t.1;
				assemble_rm_i(code, t.0, toImm(a[3]));
			}
			X86A_REG_MRRSD => {
				var t = toMrrsd(a, 1);
				addr = t.1;
				assemble_r_rm(code, toReg(a[0]), t.0);
			}
			_ => return context.fail1("unknown addressing mode %d", mode);
		}
		if (addr != null) asm.recordPatch(asm.findAbsConst(start), addr);
	}
	def assemble_rm(code: int, rm: X86Rm) {
		var func: (X86Rm) -> void;
		match (code) {
			X86_NEG => func = asm.neg;
			X86_NOT => func = asm.not;
			X86_MUL => func = asm.imul;
			X86_DIV => func = asm.div;
			X86_IDIV => func = asm.idiv;
			X86_INC => func = asm.inc;
			X86_DEC => func = asm.dec;
			X86_SHL => func = asm.shl_cl;
			X86_SAR => func = asm.sar_cl;
			X86_SHR => func = asm.shr_cl;
		}
		func(rm);
	}
	def assemble_r_rm(code: int, r: X86Reg, rm: X86Rm) {
		var func: (X86Reg, X86Rm) -> void;
		match (code) {
			X86_ADD => func = asm.add.r_rm;
			X86_OR => func = asm.or.r_rm;
			X86_ADC => func = asm.adc.r_rm;
			X86_SBB => func = asm.sbb.r_rm;
			X86_AND => func = asm.and.r_rm;
			X86_SUB => func = asm.sub.r_rm;
			X86_XOR => func = asm.xor.r_rm;
			X86_CMP => func = asm.cmp.r_rm;
			X86_MUL => func = asm.imul_r_rm;
			X86_LEA => return asm.lea(r, X86Addr.!(rm));
			X86_MOV => func = asm.movd_r_rm;
			X86_LD8 => func = asm.movb_r_rm;
			X86_LDS8 => func = asm.movbsx;
			X86_LDU8 => func = asm.movbzx;
			X86_LDS16 => func = asm.movwsx;
			X86_LDU16 => func = asm.movwzx;
			X86_LD32 => func = asm.movd_r_rm;
			_ => return context.fail1("invalid opcode %d", code);
		}
		func(r, rm);
	}
	def assemble_rm_r(code: int, rm: X86Rm, r: X86Reg) {
		var func: (X86Rm, X86Reg) -> void;
		match (code) {
			X86_ADD => func = asm.add.rm_r;
			X86_OR =>  func = asm.or.rm_r;
			X86_ADC => func = asm.adc.rm_r;
			X86_SBB => func = asm.sbb.rm_r;
			X86_AND => func = asm.and.rm_r;
			X86_SUB => func = asm.sub.rm_r;
			X86_XOR => func = asm.xor.rm_r;
			X86_CMP => func = asm.cmp.rm_r;
			X86_TEST => func = asm.test_rm_r;
			X86_ST8 => func = asm.movb_rm_r;
			X86_ST16 => func = asm.movw_rm_r;
			X86_ST32 => func = asm.movd_rm_r;
			X86_CXG8 => func = asm.cmpxchngb;
			X86_CXG16 => func = asm.cmpxchngw;
			X86_CXG32 => func = asm.cmpxchngd;
			_ => return context.fail1("invalid opcode %d", code);
		}
		func(rm, r);
	}
	def assemble_rm_i(code: int, rm: X86Rm, val: Val) {
		var func: (X86Rm, int) -> void;
		match (code) {
			X86_ADD => func = asm.add.rm_i;
			X86_OR =>  func = asm.or.rm_i;
			X86_ADC => func = asm.adc.rm_i;
			X86_SBB => func = asm.sbb.rm_i;
			X86_AND => func = asm.and.rm_i;
			X86_SUB => func = asm.sub.rm_i;
			X86_XOR => func = asm.xor.rm_i;
			X86_CMP => func = asm.cmp.rm_i;
			X86_TEST => func = asm.test_rm_i;
			X86_SHL => func = asm.shl_i;
			X86_SAR => func = asm.sar_i;
			X86_SHR => func = asm.shr_i;
			_ => return context.fail1("invalid opcode %d", code);
		}
		if (Addr.?(val)) {
			func(rm, ABS_MARKER);
			asm.recordPatch(asm.pos() - 4, Addr.!(val));  // XXX: manual patch record
		} else {
			func(rm, Int.unbox(val));
		}
	}
	def assemble_none(code: int, a: Array<Operand>) {
		match (code) {
			X86_JMP => {
				var label = toLabel(a[0]);
				if (label.pos >= 0) asm.jmp(label.pos - asm.pos());
				else asm.jmp(REL_MARKER);
				recordRelLabel(asm.pos() - 4, label);
			}
			X86_JC => {
				var label = toLabel(a[0]), cond = decodeCond(code);
				if (label.pos >= 0) asm.jmpx(cond, label.pos - asm.pos());
				else asm.jmpx(cond, REL_MARKER);
				recordRelLabel(asm.pos() - 4, label);
			}
			X86_SETC => {
				var reg = toReg(a[0]), cond = decodeCond(code);
				asm.xor.r_rm(reg, reg);	 // XXX: remove zeroing for setc?
				asm.setx(cond, reg);
				asm.movbzx(reg, reg); // XXX: remove zero extend for setc?
			}
			X86_CMOVC => {
				var reg = toReg(a[0]), cond = decodeCond(code), rm = toRm(a[1]);
				asm.cmovx(cond, reg, rm);
			}
			X86_THROW => {
				var exSource = toExSource(a[0]);
				asm.jmpx_addr(X86Conds.ALWAYS, rt.getExceptionDest(asm.codeOffset(), exSource.0, exSource.1));
			}
			X86_THROWC => {
				var exSource = toExSource(a[0]), cond = decodeCond(code);
				asm.jmpx_addr(cond, rt.getExceptionDest(asm.codeOffset(), exSource.0, exSource.1));
			}
			X86_NEG => {
				asm.neg(toReg(a[0]));
			}
			ArchInstrs.ARCH_BLOCK => {
				asm.bind(toLabel(a[0]));
			}
			ArchInstrs.ARCH_ENTRY => {
				var adjust = frameAdjust();
				if (adjust > 0) asm.sub.rm_i(X86Regs.ESP, adjust); // allocate frame
			}
			ArchInstrs.ARCH_RET => {
				var adjust = frameAdjust();
				if (adjust > 0) asm.add.rm_i(X86Regs.ESP, adjust); // deallocate frame
				asm.ret();
			}
			X86_SWITCH => {
				var size = a.length - 4;
				var key = toRm(a[0]), minValue = Int.unbox(toImm(a[1])), scratchReg = toReg(a[1]);
				asm.movd_r_rm(scratchReg, key);
				if (minValue != 0) asm.sub.rm_i(scratchReg, minValue);
				asm.cmp.rm_i(scratchReg, size - 1);
				asm.ja(REL_MARKER);
				recordRelLabel(asm.pos() - 4, toLabel(a[a.length - 1]));
				// load from the jump table to follow
				var start = asm.pos();
				asm.movd_r_rm(scratchReg, X86Addr.new(null, scratchReg, 4, X86Addrs.ABS_CONST));
				var jtaddrpos = asm.findAbsConst(start);
				asm.ijmp(scratchReg);
				// align and emit the jump table
				asm.w.align(4);
				var jumpTableAddr = asm.machBuffer.posAddr();
				asm.w.at(jtaddrpos).put_b32(jumpTableAddr);
				asm.w.atEnd();
				// emit jump table
				for (i < size) {
					recordAbsLabel(asm.pos(), toLabel(a[i]));
					asm.w.zeroN(4);
				}
			}
			X86_SEXT => {
				var reg = toReg(a[0]), width = toInt(a[1]);
				asm.shl_i(reg, 32 - width);
				asm.sar_i(reg, 32 - width);
			}
			X86_CALL => {
				var target = a[0];
				if (Operand.Immediate.?(target)) {
					asm.call_addr(Addr.!(Operand.Immediate.!(target).val));
				} else {
					asm.icall(toReg(target));
				}
				// TODO: record source, record GC map
			}
			X86_CALLER_IP => {
				asm.movd_r_rm(toReg(a[0]), X86Regs.ESP.plus(frameAdjust()));
			}
			X86_CALLER_SP => {
				asm.lea(toReg(a[0]), X86Regs.ESP.plus(frameAdjust()));
			}
			_ => return context.fail1("unexpected X86 opcode %d", code);
		}
	}
	def toMrrsd(a: Array<Operand>, start: int) -> (X86Rm, Addr) {
		// TODO: first operand to MRRSD
		var r = toReg(a[start + 1]), scale = byte.!(toInt(a[start + 2]));
		var val = toImm(a[start + 3]);
		if (Addr.?(val)) return (X86Addr.new(null, r, scale, ABS_MARKER), Addr.!(val));
		else return (X86Addr.new(null, r, scale, Int.unbox(val)), null);
	}
	def assemble_r_rm_or_r_i(r_rm: (X86Reg, X86Rm) -> void, r_i: (X86Reg, int) -> void,
					x: Operand, y: Operand) {
		if (y.isImm()) {
			var val = toImm(y);
			if (Addr.?(val)) {
				r_i(toReg(x), ABS_MARKER - 1);	// TODO: manually recording abs marker position here.
				asm.recordPatch(asm.pos() - 4, Addr.!(val));
			} else {
				r_i(toReg(x), Int.unbox(toImm(y)));
			}
		} else {
			r_rm(toReg(x), toRm(y));
		}
	}

	def matchCmp(i: SsaInstr, inblock: bool) -> X86CmpMatch {
		if ((inblock && !inSameBlock(i)) || !SsaApplyOp.?(i)) return X86CmpMatch.new(X86Conds.NZ, i, null, null);
		match (SsaApplyOp.!(i).op.opcode) {
			BoolEq,
			IntEq,
			RefEq =>		return newCmp(i, X86Conds.Z);
			IntLt =>		return newCmp(i, signedCmp(i, X86Conds.L, X86Conds.C));
			IntLteq =>		return newCmp(i, signedCmp(i, X86Conds.LE, X86Conds.NA));
			PtrLt =>		return newCmp(i, X86Conds.C);
			PtrLteq =>		return newCmp(i, X86Conds.NA);
			BoolNot => {
				var cmp = matchCmp(i.input0(), inblock);
				return X86CmpMatch.new(cmp.cond.negate, cmp.x, cmp.y, cmp.val);
			}
			_ => ;
		}
		return X86CmpMatch.new(X86Conds.NZ, i, null, null);
	}
	def signedCmp(i: SsaInstr, signed: X86Cond, unsigned: X86Cond) -> X86Cond {
		return if(V3.isSigned(SsaApplyOp.!(i).op.sig.paramTypes[0]), signed, unsigned);
	}
	def newCmp(i: SsaInstr, cond: X86Cond) -> X86CmpMatch {
		var x = i.input0(), y = i.input1();
		if (SsaConst.?(y)) return X86CmpMatch.new(cond, x, null, SsaConst.!(y).val);
		if (SsaConst.?(x) && cond.commute != null) return X86CmpMatch.new(cond.commute, y, null, SsaConst.!(x).val);
		return X86CmpMatch.new(cond, x, y, null);
	}

	def frameAdjust() -> int {
		// assumes return address already pushed
		return frame.size() - mach.code.addressSize;
	}
	def toReg(o: Operand) -> X86Reg {
		match (o) {
			Overwrite(i, vreg, assignment) => return asm.loc_r(frame, assignment);
			Def(i, vreg, assignment) => return asm.loc_r(frame, assignment);
			Use(i, vreg, assignment) => return asm.loc_r(frame, assignment);
			_ => return V3.fail("expected register");
		}
	}
	def toRm(o: Operand) -> X86Rm {
		match (o) {
			Overwrite(i, vreg, assignment) => return asm.loc_rm(frame, assignment);
			Def(i, vreg, assignment) => return asm.loc_rm(frame, assignment);
			Use(i, vreg, assignment) => return asm.loc_rm(frame, assignment);
			_ => return V3.fail("expected register");
		}
	}

	def encodeCond(cond: X86Cond) -> int {
		return cond.index << 8;
	}
	def decodeCond(opcode: int) -> X86Cond {
		return X86Conds.all[(opcode >> 8) & 0xF];
	}

	def recordAbsLabel(pos: int, label: Label) {} // TODO
	def recordRelLabel(pos: int, label: Label) {} // TODO
	def getOutput() -> ArchInstrBuffer {
		if (out != null) return out;
		return out = X86InstrBuffer.new(this, context.prog, regSet);
	}
}
class X86InstrBuffer extends ArchInstrBuffer {
	def x86codegen: X86CodeGen2;
	new(x86codegen, prog: Program, regSet: MachRegSet) super(x86codegen, prog, regSet) { }
	def putArchInstr(indent: int, i: ArchInstr) -> int {
		var opcode = int.!(i.opcode()), a = i.operands;
		var name: string, cond: X86Cond;
		match (opcode & 0xFF) {
			X86_ADD => name = "add";
			X86_OR =>  name = "or";
			X86_ADC => name = "adc";
			X86_SBB => name = "sbb";
			X86_AND => name = "and";
			X86_SUB => name = "sub";
			X86_CMP => name = "cmp";
			X86_MUL => name = "imul";
			X86_DIV => name = "div";
			X86_IDIV => name = "idiv";
			X86_NEG => name = "neg";
			X86_NOT => name = "not";
			X86_LEA => name = "lea";
			X86_TEST => name = "test";
			X86_LD8 => name = "movb";
			X86_LDS8 => name = "movbsx";
			X86_LDU8 => name = "movbzx";
			X86_LD16 => name = "movw";
			X86_LDS16 => name = "movwsx";
			X86_LDU16 => name = "movwzx";
			X86_LD32 => name = "mov";
			X86_ST8 => name = "movb";
			X86_ST16 => name = "movw";
			X86_ST32 => name = "mov";
			X86_CXG8 => name = "cmpxchgb";
			X86_CXG16 => name = "cmpxchgw";
			X86_CXG32 => name = "cmpxchg";
			X86_INC => name = "inc";
			X86_DEC => name = "dec";
			X86_SHL => name = "shl";
			X86_SAR => name = "sar";
			X86_SHR => name = "shr";
			X86_SEXT => name = "sext";
			X86_CALL => name = "call";
			X86_CALLER_IP => name = "caller_ip";
			X86_CALLER_SP => name = "caller_sp";
			X86_JMP => name = "j";
			X86_SETC => {
				name = "set";
				cond = x86codegen.decodeCond(opcode);
			}
			X86_CMOVC => {
				name = "cmov";
				cond = x86codegen.decodeCond(opcode);
			}
			X86_JC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			ArchInstrs.ARCH_RET => {
				putIndent(indent);
				puts(name);
				sp();
				if (codegen.frame.frameSize < 0) puts("?");
				else putd(x86codegen.frameAdjust());
				sp();
				putOperands(a);
				return indent;
			}
			X86_THROW => {
				name = "j";
			}
			X86_THROWC => {
				name = "j";
				cond = x86codegen.decodeCond(opcode);
			}
			_ => {
				return putSimpleInstr(indent, i);
			}
		}
		match (opcode >> 8) {
			X86A_MRRSD_REG,
			X86A_MRRSD_IMM => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				var offset = renderMrrsd(a, 0);
				putOperand(a[offset]);
			}
			X86A_REG_MRRSD => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperand(a[0]);
				csp();
				var offset = renderMrrsd(a, 1);
			}
			_ => {
				putIndent(indent);
				puts(name);
				if (cond != null) puts(cond.name);
				sp();
				putOperands(a);
			}
		}


		return indent;
	}
	def renderMrrsd(a: Array<Operand>, start: int) -> int {
		var x = a[start], y = a[start + 1], scale = codegen.toInt(a[start + 2]), disp = codegen.toImm(a[start + 3]);
		putc('[');
		if (!Operand.Immediate.?(x)) {
			putOperand(x);
			puts(" + ");
		}
		putOperand(y);
		if (scale > 1) {
			puts(" * ");
			putd(scale);
		}
		if (disp != null) {
			puts(" + ");
			V3.renderResult(disp, null, this);
		}
		putc(']');
		return 3;
	}
}
