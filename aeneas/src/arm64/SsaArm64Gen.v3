// Copyright 2024 Virgil Authors. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

// Arm64 instructions are ints that look like <mode args><mode><code>
// where each part of the instruction is a byte

// Masks
def MASK_CODE = 0xff;
def MASK_AM = 0xff00;
def MASK_AM_ARG = 0xff0000;

// Shifts
def SHIFT_AM: byte = 8;
def SHIFT_AM_ARG: byte = 16;

// codes
def I_ADDD = 0x01;	def I_ADDQ = 0x11;
def I_MOVZ = 0x02;
def I_MOVK = 0x03;
def I_LDR = 0x04;
def I_STR = 0x05;

def I_QD_DIFF = I_ADDQ - I_ADDD;

// addressing modes
def AM_NONE = 0x00;
def AM_IMM = 0x01;
def AM_SHIFTED_IMM = 0x02;
def AM_EXTENDED_REG_LSL = 0x03;
def AM_SHIFTED_REG_LSL = 0x04;
def AM_REG = 0x05;

// useful constants
def MAX_IMM16_MOV = 0xffff;
def MIN_IMM16_MOV = 0xffff0000;

// Constructs an opcode from the 4 parts
def makeOpcode(code: int, am: int, amArg: int) -> int {
	return code | (am << SHIFT_AM) | (amArg << SHIFT_AM_ARG);
}

// Extracts the code section of the opcode
def getCode(opcode: int) -> int {
	return opcode & MASK_CODE;
}

// Extracts the addressing mode argument section of the opcode
def getAmArg(opcode: int) -> int {
	return (opcode & MASK_AM_ARG) >> SHIFT_AM_ARG;
}

// Extracts the addressing mode section of the opcode
def getAm(opcode: int) -> int {
	return (opcode & MASK_AM) >> SHIFT_AM;
}

// Gets width of instruction corresponding to RegClass
// def getRegClassWidth(rc: RegClass) -> int {
// 	match (rc) {
// 		I32, F32 => return W_32;
// 		_ => return W_64;
// 	}
// }

def MRegs: Arm64RegSet;
def Regs: Arm64Regs;
def Conds: Arm64Conds; // TODO

// Code generation for the Arm64 backend
class SsaArm64Gen extends SsaMachGen {
	def asm: Arm64Assembler; // TODO
	def m = SsaInstrMatcher.new();
	def dwarf: Dwarf; // What is this?

	new(context: SsaContext, mach: MachProgram, asm, w: MachDataWriter, dwarf)
	super(context, mach, Arm64RegSet.SET, w) {}

	// Overidden Architecture Specific Routines
	def visitApply(block: SsaBlock, i: SsaApplyOp) {
		match (i.op.opcode) {
			IntAdd => {
				emitIntBinop(I_ADDD, i);
			}
			_ => context.fail("not implemented"); // TODO
		}
	}

	def visitThrow(block: SsaBlock, i: SsaThrow) { context.fail("not implemented"); }
	def visitIf(block: SsaBlock, i: SsaIf) { context.fail("not implemented"); }
	def visitSwitch(block: SsaBlock, i: SsaSwitch) { context.fail("not implemented"); }
	def visitGoto(block: SsaBlock, target: SsaGoto) { context.fail("not implemented"); }

	// Regalloc callbacks to add moves
	def genSaveLocal(reg: int, v: VReg) { context.fail("not implemented"); }
	def genRestoreLocal(v: VReg, reg: int) { context.fail("not implemented"); }

	def genMoveLocLoc(src: (VReg, int), dst: (VReg, int), regClass: RegClass) {
		// def width = getRegClassWidth(regClass);
		// def dstStack = regSet.isStack(dst.1), srcStack = regSet.isStack(src.1);

		// if (dstStack && srcStack) {
		// 	def opcode1 = makeOpcode(I_LDR, AM_REG, ARG_NONE, width);
		// 	emit2(opcode1, op(Operand.Def(null, Regs.SCRATCH_GPR)), op(Operand.Use(src)));	
		// 	def opcode2 = makeOpcode(I_STR, AM_REG, ARG_NONE, width);
		// 	emit2(opcode1, op(Operand.Def(dst)), op(Operand.Use(null, Regs.SCRATCH_GPR)));	
		// } else if (dstStack && !srcStack) {
		// 	def opcode = makeOpcode(I_STR, AM_REG, ARG_NONE, width);
		// 	emit2(opcode, op(Operand.Def(dst)), op(Operand.Use(src)));
		// } else if (!dstStack && srcStack) {
		// 	def opcode = makeOpcode(I_LDR, AM_REG, ARG_NONE, width);
		// 	emit2(opcode, op(Operand.Def(dst)), op(Operand.Use(src)));
		// } else {
		// 	def opcode = makeOpcode(I_MOVZ, AM_REG, ARG_NONE, width);
		// 	emit2(opcode, op(Operand.Def(dst)), op(Operand.Use(src)));
		// }
	}

	// Register allocation callback to prepend a move
	def genMoveValLoc(src: VReg, dst: (VReg, int), regClass: RegClass) {
		// def width = getRegClassWidth(regClass);

		// if (regSet.isStack(dst.1)) {
		// 	// Only registers can be stored to memory
		// 	genMoveValLoc(src, (null, Regs.SCRATCH_GPR), regClass);
		// 	def opcode = makeOpcode(I_STR, AM_REG, ARG_NONE, width);
		// 	emit2(opcode, op(Operand.Def(dst)), op(Operand.Use(null, Regs.SCRATCH_GPR)));
		// } else {
		// 	def val = SsaConst.!(src.ssa).val;
		// 	match (val) {
		// 		x: Box<int> => {
		// 			// XXX Optimize
		// 			// Move first 16-bits to reg
		// 			def opcode1 = makeOpcode(I_MOVZ, AM_REG, ARG_NONE, W_32);
		// 			emit2(opcode1, op(Operand.Def(dst)), useImm(Int.box(x.val & 0xffff)));
		// 			// Move second 16-bits to reg
		// 			def opcode2 = makeOpcode(I_MOVK, AM_SHIFTED_IMM, 16, W_32);
		// 			emit2(opcode2, op(Operand.Def(dst)), useImm(Int.box((x.val & 0xffff0000) >> 16)));
		// 		}
		// 		_ => context.fail("not implemented"); // TODO
		// 	}
		// }
	}

	def assemble(opcode: int, a: Array<Operand>) {
		if (opcode < 0) {
			match (opcode) {
				ArchInstrs.ARCH_ENTRY => {
					var adjust = frameAdjust();
					// allocate frame
					if (adjust > 0) asm.subq_r_r_i_i(Regs.SP, Regs.SP, u12.view(adjust), 0);
				}
				ArchInstrs.ARCH_BLOCK => return; // TODO
				ArchInstrs.ARCH_RET => {
					var adjust = frameAdjust();
					// deallocate frame
					if (adjust > 0) asm.addq_r_r_i_i(Regs.SP, Regs.SP, u12.view(adjust), 0);
					asm.ret();
					return;
				}
				ArchInstrs.ARCH_BLOCK_END => return; //TODO
				ArchInstrs.ARCH_END => return;
				_ => context.fail("not implemented");
			}
			return;
		}

		def dest = toGpr(a[0]);
		def lhs = toGpr(a[1]);
		def rhs = toImm(a[2]);
		asm.addd_r_r_i_i(dest, lhs, u12.view(toB32(rhs)), 0);
	}

	// Helper functions

	def selectWidth(i: SsaApplyOp, op: int) -> int {
		return if(intOpWidth(i) > 32, op + I_QD_DIFF, op);
	}

	def intOpWidth(i: SsaApplyOp) -> byte {
		// XXX: factor this out and clean it up
		var t = i.op.typeArgs[0];
		if (IntType.?(t)) return IntType.!(t).width;
		if (t.typeCon.kind == Kind.ENUM) return V3.getVariantTagType(t).width;
		if (t.typeCon.kind == Kind.ENUM_SET) return V3.getEnumSetType(t).width;
		return 64;
	}

	// Emit code for an integer binop
	def emitIntBinop(opcode: int, i: SsaApplyOp) {
		def width = intOpWidth(i);
		emitSimpleBinop(selectWidth(i, opcode), i);
	}

	// Emit code for a simple binop (add, sub, mul, etc...)
	def emitSimpleBinop(opcode: int, i: SsaApplyOp) {
		// XXX: select better left operand using liveness
		m.intbinop(i);
		dfnReg(i);
		use(m.x);
		if (tryUseImm32(m.y)) {
			opcode |= AM_IMM << SHIFT_AM;
		} else {
			opcode |= AM_IMM << SHIFT_AM;
			opcode |= AM_SHIFTED_REG_LSL << SHIFT_AM;
			use(m.y);
		}
		emitN(opcode);
	}

	def tryUseImm32(i: SsaInstr) -> bool {
		if (i == null) { useInt(0); return true; }
		if (SsaConst.?(i)) {
			var val = SsaConst.!(i).val;
			match (val) {
				null => { useImm(val); return true; }
				x: Box<int> => { useImm(val); return true; }
				x: Box<long> => if(x.val == int.view(x.val)) { useInt(int.view(x.val)); return true; }
				x: Addr => { useImm(val); return true; }
				x: Box<bool> => { useInt(if(x.val, 1, 0)); return true; }
				x: ArrayRangeStart => { useImm(val); return true; }
			}
		}
		return false;
	}

	def toLoc(o: Operand) -> int {
		match (o) {
			Overwrite(dst, src, assignment) => return assignment;
			Def(vreg, assignment) => return assignment;
			Use(vreg, assignment) => return assignment;
			_ => return V3.fail("expected operand with assignment");
		}
	}

	def toGpr(o: Operand) -> Arm64Gpr {
		return loc_r(toLoc(o));
	}

	def loc_r(loc: int) -> Arm64Gpr {
		var gpr = MRegs.toGpr(loc);
		if (gpr == null) return V3.fail1("expected GPR, got %s", regSet.identify(loc));
		return gpr;
	}

	def toB32(val: Val) -> int {
		var addr: Addr, b: int;
		match (val) {
			x: Box<int> 		=> b = x.val;
			x: Box<long> 		=> b = int.view(x.val);
			x: Box<bool> 		=> b = if(x.val, 1);
			x: Float32Val 		=> b = int.view(x.bits);
			x: Float64Val 		=> b = int.view(x.bits);
			x: ArrayRangeStart 	=> b = x.start;
			null 			=> b = 0;
			_ => ;
		}
		return b;
	}
	def frameAdjust() -> int {
		// assumes return address already pushed
		return frame.size() - mach.code.addressSize;
	}
}