// Copyright 2011 Google Inc. All rights reserved.
// See LICENSE for details of Apache 2.0 license.

enum ArithWidth(subword: bool, width: byte) {
	Sub8(true, 8),
	Exactly8(false, 8),
	Sub16(true, 16),
	Exactly16(false, 16),
	Sub32(true, 32),
	Exactly32(false, 32),
	Sub64(true, 64),
	Exactly64(false, 64)
}
class MachLoweringConfig {
	var IntDivZeroTraps = true;	 // int division traps on zero
	var IntDivOverflowMinint = true; // int division overflow is minint
	var Int8Arith = false;		 // native support for int8 arithmetic
	var Int16Arith = false;		 // native support for int16 arithmetic
	var Int32Arith = true;		 // native support for int32 arithmetic
	var Int64Arith = false;		 // native support for int64 arithmetic
	var StoreNarrow = true;		 // enables store narrowing optimization
	var Int8StoreNarrow = true;	 // storing > 8 bits can narrow
	var Int16StoreNarrow = true;	 // storing > 16 bits can narrow
	var Int32StoreNarrow = true;	 // storing > 32 bits can narrow
	var Int8LoadZeroExtend = true;	 // loading 8 bits can zero extend
	var Int8LoadSignExtend = true;	 // loading 8 bits can sign extend
	var Int16LoadZeroExtend = true;	 // loading 16 bits can zero extend
	var Int16LoadSignExtend = true;	 // loading 16 bits can sign extend
	var Int32LoadZeroExtend = true;	 // loading 32 bits can zero extend to 64
	var Int32LoadSignExtend = true;	 // loading 32 bits can sign extend to 64
	var Int32ShiftSaturate = false;	 // shift saturates on overflow
	var NullcheckAreaSize = 0;	 // hardware reserved null check area size
	var ImplicitNullChecks = true;	 // nullchecks implicit with loads+stores
	var ExplicitDivChecks = false;	 // insert explicit maxint / -1 checks
	var ExplicitModChecks = false;	 // insert explicit maxint % -1 checks
	var IntConvertFUnsigned = true;	 // native support for unsigned float conversions
	var FloatConvertIUnsigned = true; // native support for unsigned int conversions
	var IntConvertFMapsNanToZero = true;
	var IntConvertFPosSaturates = true; // proper positive saturation
	var IntConvertFNegSaturates = true; // proper negative saturation
	var FloatPromoteU64 = true;
	var IntCastFTraps = false;	// IntCastF machine instruction traps
	var ObjectSystem = false;	// target platform has an object system
	var NativeCmpSwp = true;	// target platform supports native compare+swap

	def getArithWidth(tt: IntType) -> ArithWidth {
		// XXX: speed up this routine with a lookup table
		if (Int8Arith) {
			if (tt.width < 8) return ArithWidth.Sub8;
			if (tt.width == 8) return ArithWidth.Exactly8;
		}
		if (Int16Arith) {
			if (tt.width < 16) return ArithWidth.Sub16;
			if (tt.width == 16) return ArithWidth.Exactly16;
		}
		if (Int32Arith) {
			if (tt.width < 32) return ArithWidth.Sub32;
			if (tt.width == 32) return ArithWidth.Exactly32;
		}
		if (tt.width < 64) return ArithWidth.Sub64;
		return ArithWidth.Exactly64;
	}
}

// Helper for normalizing a graph in place.
class SsaGraphNormalizer(context: SsaContext) {
	def buffer = Vector<SsaInstr>.new();  // reusable input buffer
	def replacements = Vector<Array<SsaInstr>>.new();
	var cfopt: SsaCfOptimizer;
	var singleMark: int;
	var curBlock: SsaBuilder;
	var phis: List<SsaPhi>;

	// Map {i_old} to multiple instructions in {i_new}.
	def mapN(i_old: SsaInstr, ai_new: Array<SsaInstr>) {
		mapNkeep(i_old, ai_new);
		i_old.kill();
		i_old.remove();
	}
	// Map {i_old} to multiple instructions in {i_new}.
	def mapNkeep(i_old: SsaInstr, ai_new: Array<SsaInstr>) {
		if (ai_new.length == 1) return map1(i_old, ai_new[0]);
		i_old.mark = context.graph.markGen++;
		i_old.instrVal = null;
		var index = i_old.mark - singleMark - 1;
		replacements.grow(index + 1);
		replacements[index] = ai_new;
		if (index >= replacements.length) replacements.length = index + 1;
		for (i in ai_new) i.mark = singleMark;
	}
	// Map {i_old} 1-1 to {i_new}.
	def map1(i_old: SsaInstr, i_new: SsaInstr) {
		i_old.mark = singleMark;
		i_old.instrVal = i_new;
		i_new.mark = singleMark;
		i_new.facts |= i_old.facts & Facts.V_FACTS; // TODO: is this always safe?
		i_old.replace(i_new);
		i_old.remove();
	}
	// Map {i_old} 1-1 to {i_new} but don't kill or remove.
	def map1keep(i_old: SsaInstr, i_new: SsaInstr) {
		i_old.mark = singleMark;
		i_old.instrVal = i_new;
		i_new.mark = singleMark;
		i_old.replace(i_new);
	}
	// Map {i_old} 1-0.
	def map0(i_old: SsaInstr) {
		mapN(i_old, Ssa.NO_INSTRS);
	}
	// Mark {i_old} as a 1-1 mapping with itself and return.
	def id(i_old: SsaInstr) -> SsaInstr {
		i_old.mark = singleMark;
		i_old.instrVal = null;
		return i_old;
	}
	// Reset internal state for a new graph.
	def reset(graph: SsaGraph) {
		singleMark = ++graph.markGen;
		++graph.markGen;
		replacements.clear();
		buffer.clear();
		curBlock = SsaBuilder.new(context, graph, null);
	}
	// Get the replacements for a given instruction, if it is not a 1-1 mapping.
	// Returns {null} if there is a 1-1 mapping.
	def getReplacements(i: SsaInstr) -> Array<SsaInstr> {
		if (i.mark == singleMark) return null;
		if (i.mark > singleMark) return replacements[i.mark - singleMark - 1];
		if (SsaConst.?(i)) normConst(SsaConst.!(i));
		else if (SsaPhi.?(i)) normPhi(SsaPhi.!(i));
		else context.fail1("no replacement for @%d", i.uid);
		if (i.mark == singleMark) return null;
		if (i.mark > singleMark) return replacements[i.mark - singleMark - 1];
		context.fail1("no replacement for @%d", i.uid);
		return null;
	}
	// Normalizes a phi and updates internal map.
	def normPhi(i_old: SsaPhi) {
		var tn = normType(i_old.vtype);
		phis = List.new(i_old, phis);
		// Check for common case of no normalization.
		if (tn == null || tn.newType == i_old.vtype) return void(id(i_old));
		// Degenerate case of zero-width phi.
		if (tn.size == 0) {
			i_old.kill();
			i_old.remove();
			return map0(i_old);
		}
		// Map the old phi to a new phi with the new type.
		if (tn.size == 1) {
			var i_new = SsaPhi.new(tn.newType, i_old.block, Ssa.NO_INSTRS);
			return map1keep(i_old, i_new);
		}
		// Complex phi.
		var phis = Array<SsaInstr>.new(tn.size);
		for (i < phis.length) {
			phis[i] = SsaPhi.new(tn.sub[i], i_old.block, Ssa.NO_INSTRS);
		}
		return mapNkeep(i_old, phis);
	}
	// Normalizes a constant and updates internal map.
	def normConst(i_old: SsaConst) {
		var tn = normType(i_old.vtype);
		// Check for common case of no normalization.
		if (tn == null) {
			var nv = genSimpleVal(i_old.val, tn);
			if (nv == i_old.val) return void(id(i_old));
			return map1(i_old, context.graph.valConst(i_old.vtype, nv));
		}
		// Degenerate case of zero-width value.
		if (tn.size == 0) return map0(i_old);
		// Map the old value to a new value with the new type.
		if (tn.size == 1) {
			var nv = genSimpleVal(i_old.val, tn);
			if (nv == i_old.val && tn.oldType == tn.newType) return void(id(i_old));
			return map1(i_old, context.graph.valConst(tn.newType, nv));
		}
		// Complex value.
		return mapN(i_old, normConstAsArray(tn, i_old));
	}
	def normConstAsArray(tn: TypeNorm, v: SsaConst) -> Array<SsaInstr> {
		var vals = Array<Val>.new(tn.size);
		genValIntoArray(v.val, tn, vals, 0);
		var instrs = Array<SsaInstr>.new(tn.size);
		for (i < vals.length) {
			instrs[i] = context.graph.valConst(tn.sub[i], vals[i]);
		}
		return instrs;
	}
	def normRef1(e: SsaDfEdge) -> SsaInstr {
		var i = e.dest;
		getReplacements(i);
		if (i.mark != singleMark) context.fail1("instruction @%d should map 1-1", i.uid);
		return if (i.instrVal != null, i.instrVal, i);
	}
	def normRefN(e: SsaDfEdge) -> Array<SsaInstr> {
		var x = e.dest;
		var r = getReplacements(x);
		if (r != null) return r;
		while (x.instrVal != null) x = x.instrVal;
		return [x];
	}
	def normRefs(ai_old: Array<SsaDfEdge>) -> Array<SsaInstr> {
		buffer.length = 0;
		buffer.grow(ai_old.length);
		for (e in ai_old) {
			var i_old = e.dest;
			var ai_new = getReplacements(i_old);
			if (ai_new == null) buffer.put(if(i_old.instrVal != null, i_old.instrVal, i_old));
			else buffer.puta(ai_new);
		}
		return buffer.extract();
	}
	def normInputs(i_old: SsaInstr) -> SsaInstr {
		var ai_new = normRefs(i_old.inputs);
		i_old.setInputs(ai_new);
		return i_old;
	}
	def normType(t: Type) -> TypeNorm;
	def genSimpleVal(v: Val, tn: TypeNorm) -> Val;
	def genValIntoArray(v: Val, tn: TypeNorm, dest: Array<Val>, index: int);
	def normType1(t: Type) -> Type {
		var tn = normType(t);
		return if(tn != null, tn.newType, t);
	}
	def normTypeQ(t: Type) -> bool {
		var tn = normType(t);
		return tn != null && (tn.size != 1 || tn.newType != t);
	}

	def mapMultiReturn(i_old: SsaInstr, i_new: SsaInstr, tn: TypeNorm) {
		if (tn == null || tn.size == 1) return map1(i_old, i_new);
		if (tn.size == 0) return map0(i_old);
		var values = Array<SsaInstr>.new(tn.size);
		for (i < tn.size) {
			var get = SsaApplyOp.new(null, V3Op.newTupleGetElem(tn.newType, i), [i_new]);
			get.insertBefore(i_new.next);
			values[i] = get;
		}
		mapN(i_old, values);
	}
	def genEqualN(i_old: SsaApplyOp, tn: TypeNorm) -> SsaInstr {
		var expr: SsaInstr, ai_new = normRefs(i_old.inputs);
		for (i < tn.size) {
			var cmp: SsaInstr, a = ai_new[i], b = ai_new[i + tn.size];
			cmp = curBlock.opEqual(tn.sub[i], a, b);
			if (expr == null) expr = cmp;
			else expr = curBlock.opBoolAnd(V3Op.opBoolAnd, expr, cmp);
		}
		map1(i_old, expr);
		return expr;
	}
	def genEqual2(t: Type, tn: TypeNorm, xa: Array<SsaInstr>, ya: Array<SsaInstr>) -> SsaInstr {
		if (tn == null) return curBlock.opEqual(t, xa[0], ya[0]);
		var expr: SsaInstr;
		for (i < tn.size) {
			var cmp: SsaInstr, a = xa[i], b = ya[i];
			cmp = curBlock.opEqual(tn.sub[i], a, b);
			if (expr == null) expr = cmp;
			else expr = curBlock.opBoolAnd(V3Op.opBoolAnd, expr, cmp);
		}
		return expr;
	}
}

// Lowers Virgil code to machine level in-place by replacing object-level operations
// with loads and stores. Introduces truncations and wrapping for integer operations
// according to the {config}.
class MachLowering(mach: MachProgram, compiler: Compiler, config: MachLoweringConfig) extends SsaGraphNormalizer(SsaContext.new(compiler, mach.prog)) {
	var maybeDead: List<SsaInstr>;
	def doMethod(method: IrMethod) {
		context.enterMethod(method);
		var graph = method.ssa;
		if (graph == null) return;
		reset(graph);
		// Map parameters.
		for (i_param in graph.params) {
			if (normTypeQ(i_param.vtype)) {
				genParams(graph);
				break;
			}
			id(i_param);
		}

		// Map return type.
		graph.returnType = normType1(graph.returnType);

		var queue = Vector<SsaBlock>.new();
		queue.put(graph.startBlock);
		graph.startBlock.mark = singleMark;
		for (i = 0; i < queue.length; i++) {
			var block = queue[i];
			doBlock(block);
			var succs = curBlock.block.succs();
			for (s in succs) {
				if (s.dest.mark < singleMark) {
					queue.put(s.dest);
					s.dest.mark = singleMark;
				}
			}
		}

		for (l = phis; l != null; l = l.tail) genPhi(l.head);
		phis = null;

		for (l = maybeDead; l != null; l = l.tail) {
			if (l.head.useList == null) {
				l.head.kill();
				l.head.remove();
			}
		}
		context.printSsa("Machine");
	}
	def genParams(graph: SsaGraph) {
		def buffer = Vector<SsaParam>.new().grow(graph.params.length);
		for (i_old in graph.params) {
			var tn = normType(i_old.vtype);
			if (tn == null || (tn.size == 1 && tn.oldType == tn.newType)) {
				buffer.put(i_old);
				id(i_old);
			} else if (tn.size == 0) {
				map0(i_old);
			} else if (tn.size == 1) {
				var i_new = SsaParam.new(buffer.length, tn.newType);
				buffer.put(i_new);
				map1(i_old, i_new);
			} else {
				var ai_new = Array<SsaInstr>.new(tn.size);
				for (j < ai_new.length) {
					var i_new = SsaParam.new(buffer.length, tn.sub[j]);;
					ai_new[j] = i_new;
					buffer.put(i_new);
				}
				mapN(i_old, ai_new);
			}
		}
		graph.params = buffer.extract();
	}
	def genPhi(phi: SsaPhi) {
		if (phi.facts.O_KILLED) return;
		var r = getReplacements(phi);
		if (r == null) {
			if (phi.instrVal == null) {
				// phi is left in place.
				for (i < phi.inputs.length) if (phi.inputs[i].dest == null) context.fail1("phi input[%d] is null", i);
				normInputs(phi);
			} else {
				// phi is replaced 1->1.
				var nphi = SsaPhi.!(phi.instrVal);
				nphi.insertBefore(phi);
				nphi.setInputs(normRefs(phi.inputs));
				phi.remove();
			}
		} else {
			// phi is replaced 1->N.
			var refs = normRefs(phi.inputs), width = r.length;
			for (j = 0; j < width; j++) {
				var nphi = SsaPhi.!(r[j]);
				nphi.insertBefore(phi);
				var ninputs = Array<SsaInstr>.new(phi.inputs.length);
				for (k = 0; k < phi.inputs.length; k++) {
					ninputs[k] = refs[j + k * width];
				}
				nphi.setInputs(ninputs);
			}
			phi.remove();
		}
	}
	def doBlock(block: SsaBlock) {
		curBlock.clear();
		context.block = curBlock.block = block;
		var i = block.next;
		while (true) {
			if (i == null) break;
			if (SsaBlock.?(i)) break;
			var n = i.next;
			match (i) {
				x: SsaApplyOp => {
					curBlock.pt = i;
					curBlock.source = x.source;
					genApplyOp(x);
					if (curBlock.end) break;
				}
				x: SsaPhi => getReplacements(x);
				x: SsaReturn => x.setInputs(normRefs(x.inputs));
				x: SsaInstr => normInputs(x);
			}
			i = n;
		}
	}
	def genSimpleVal(v: Val, tn: TypeNorm) -> Val {
		return mach.machVal(v);
	}
	def genValIntoArray(v: Val, tn: TypeNorm, dest: Array<Val>, index: int) {
		if (tn == null || tn.size == 1) return void(dest[index] = v);
		if (IntNorm.?(tn)) {
			if (Box<int>.?(v)) mach.intNorm.normIntIntoArray(IntType.!(tn.oldType), Box<int>.!(v).val, dest, index);
			if (Box<long>.?(v)) mach.intNorm.normLongIntoArray(IntType.!(tn.oldType), Box<long>.!(v).val, dest, index);
		}
	}
	def normType(t: Type) -> TypeNorm {
		match (t.typeCon.kind) {
			ENUM_SET => return mach.intNorm.normType(V3.getEnumSetType(t));
			INT => return mach.intNorm.normType(t);
			TUPLE => return normTypeTuple(t);
		} else {
			var mt = mach.machType(t);
			return if(mt != t, TypeNorm.new(t, mt, null));
		}
	}

	def normTypeTuple(t: Type) -> TupleNorm {
		// TODO: cache flattened tuples which occur in returns
		// flatten tuples
		var vecT = Vector<Type>.new();
		var vecO = Vector<int>.new();
		var vecN = Vector<TypeNorm>.new();
		for (p = t.nested; p != null; p = p.tail) {
			var n = normType(p.head);
			vecO.put(vecT.length);
			vecN.put(n);
			if (n == null) vecT.put(p.head);
			else n.addTo(vecT);
		}
		var ta = vecT.extract();
		return TupleNorm.new(t, Tuple.newType(Lists.fromArray(ta)), ta, vecN.extract(), vecO.extract());
	}
	def normIntType(t: Type) -> IntNorm {
		return mach.intNorm.normType(t);
	}
	def genApplyOp(i_old: SsaApplyOp) {
		var i_new: SsaInstr;
		match(i_old.op.opcode) {
			// Simple operators require no conversion other than normalization
			IntEq =>			i_new = genEqualOp(i_old);
			IntLt =>			i_new = genIntCmp(i_old, IntType.opLt, IntType.opLt);
			IntLteq =>			i_new = genIntCmp(i_old, IntType.opLt, IntType.opLtEq);
			// Output of integer operations must be normalized
			IntAdd,				// --
			IntSub,				// --
			IntMul =>			return genTruncatingIntOp(i_old);
			IntDiv,
			IntMod => 			return genIntDivOrMod(i_old);
			IntShl,				// --
			IntSar,				// --
			IntShr =>			return genShiftOp(i_old);
			// Output of integer operations must be normalized
			IntAnd =>			return genParallelIntOp(i_old, IntType.opAnd);
			IntOr =>			return genParallelIntOp(i_old, IntType.opOr);
			IntXor =>			return genParallelIntOp(i_old, IntType.opXor);
			// Floating point truncations, casts, and queries may have range checks
			IntCastF =>			return genIntCastF(i_old);
			IntQueryF =>			return genIntQueryF(i_old);
			IntViewF(isDouble) => {
				// identity, except for wide integer types
				var itt = IntType.!(i_old.op.sig.returnType());
				var tn = normIntType(itt);
				var results = wideOutputs(i_old.op, tn, [i_old.input0()], Facts.NONE);
				return mapN(i_old, results);
			}
			IntTruncF =>			return genIntTruncF(i_old);
			FloatCastI =>			return genFloatCastI(i_old);
			FloatCastD =>			return genFloatCastD(i_old);
			FloatQueryI =>			return genFloatQueryI(i_old);
			FloatPromoteI(is64) => {
				var inputs = normRefs(i_old.inputs);
				return map1(i_old, genFloatConvertI(i_old.op, inputs));
			}
			FloatViewI(is64) => {
				// identity, except for wide integer types
				var tn = normIntType(i_old.op.typeArgs[0]);
				var inputs = normRefs(i_old.inputs);
				i_new = wideInputs(i_old.op, tn, inputs, Facts.NONE);
			}
			FloatRoundI(is64) => {
				var inputs = normRefs(i_old.inputs);
				return map1(i_old, genFloatConvertI(i_old.op, inputs));
			}
			FloatQueryD =>			return genFloatQueryD(i_old);
			// Conversions have to be normalized specially
			IntViewI =>			return genIntViewI(i_old);
			TypeCast(cast) => 		return genTypeCast(i_old, cast);
			TypeQuery(query) => 		i_new = genTypeQuery(i_old, query);
			TypeSubsume => 			return genTypeSubsume(i_old);
			IntRepViewRef =>		return mapN(i_old, normRefs(i_old.inputs));
			RefViewIntRep =>		return mapN(i_old, normRefs(i_old.inputs));
			ArrayAlloc => 			i_new = genArrayAlloc(i_old);
			ArrayInit => 			i_new = genArrayInit(i_old);
			ArrayTupleInit(elems, length) => i_new = genArrayTupleInit(i_old, elems, length);
			ArrayGetElem => 		return genArrayGetElem(i_old, 0);
			ArraySetElem => 		return genArraySetElem(i_old, 0);
			RangeStartPlusIndex => {
				var arrayType = i_old.op.typeArgs[0];
				var inputs = normRefs(i_old.inputs);
				var rangeStart = inputs[0];
				var index = inputs[1]; // XXX: discarding upper words for normalized integers
				var offset = genArrayElemOffset(0, mach.getArrayElemScale(arrayType), index);
				i_new = ptrAdd(rangeStart, offset);
			}
			NormRangeGetElem =>		return genNormRangeGetElem(i_old, 0);
			NormRangeGetElemElem(index) =>	return genNormRangeGetElem(i_old, index);
			NormRangeSetElem =>		return genNormRangeSetElem(i_old, 0);
			NormRangeSetElemElem(index) =>	return genNormRangeSetElem(i_old, index);
			ByteArrayGetField(offset) => return genByteArrayGetField(i_old, offset);
			ByteArraySetField(offset) => return genByteArraySetField(i_old, offset);
			ArrayGetElemElem(index) => 	return genArrayGetElem(i_old, index);
			ArraySetElemElem(index) => 	return genArraySetElem(i_old, index);
			ArrayGetLength => 		i_new = genArrayGetLength(i_old);
			ClassAlloc(method) => 		return genClassAlloc(i_old, method);
			ClassGetField(field) => 	return genClassGetField(false, i_old, field);
			ClassInitField(field) =>	return genClassSetField(i_old, field, true);
			ClassSetField(field) => 	return genClassSetField(i_old, field, false);
			ClassGetMethod(method) => 	i_new = genClassGetMethod(i_old, method);
			ClassGetSelector(selector) => 	i_new = genClassGetSelector(i_old, selector);
			VariantGetField(field) => 	return genClassGetField(true, i_old, field);
			VariantGetMethod(method) => 	i_new = genVariantGetMethod(i_old, method);
			VariantGetSelector(selector) => i_new = genVariantGetSelector(i_old, selector);
			VariantReplaceNull =>		i_new = genVariantReplaceNull(i_old);
			ComponentGetField(field) => 	return genComponentGetField(i_old, field);
			ComponentSetField(field) => 	return genComponentSetField(i_old, field);
			TupleGetElem(index) => 		return genTupleGetElem(i_old, index);
			NullCheck =>			i_new = genNullCheck(i_old);
			BoundsCheck => 			i_new = genBoundsCheck(i_old, Facts.NONE).0;
			CallMethod(method) => 		return genCallMethod(i_old, method);
			CallClassMethod(selector) => 	return genCallClassMethod(i_old, selector);
			CallClassSelector(selector) => 	return genCallClassSelector(i_old, selector);
			CallVariantSelector(selector) => 	return genCallVariantSelector(i_old, selector);
			CallFunction => {
				var funcRep = mach.getFuncRep(i_old.op.typeArgs[0]);
				call(i_old, funcRep, normRefs(i_old.inputs));
				return;
			}
			VariantGetTag => {
				var i_obj = normRef1(i_old.inputs[0]);
				i_new = genIfNull(i_old, i_old.op.sig.returnType(), i_obj, null, genVariantGetTag(i_old, _));
			}
			PtrAtLength => {
				var i_array = normRef1(i_old.inputs[0]);
				var i_add = newPtrAdd(i_old.op.sig.returnType());
				var i_offset = context.graph.intConst(mach.getArrayLengthOffset(i_old.op.typeArgs[0]));
				i_new = apply(i_old.source, i_add, [i_array, i_offset]);
			}
			PtrAtObject => {
				i_new = normRef1(i_old.inputs[0]);
			}
			PtrAtArrayElem => {
				genBoundsCheck(i_old, Fact.O_NO_NULL_CHECK); // XXX: propagate nullity
				var i_array = normRef1(i_old.inputs[0]), arrayType = i_old.op.typeArgs[0];
				var i_index = normRef1(i_old.inputs[1]);
				var hsize = mach.getArrayElemOffset(arrayType, 0), scale = mach.getArrayElemScale(arrayType);
				var i_offset = genArrayElemOffset(hsize, scale, i_index);
				i_new = ptrAdd(i_array, i_offset);
			}
			PtrAtEnd => {
				var i_obj = normRef1(i_old.inputs[0]);
				var objType = i_old.op.typeArgs[0];
				var size = mach.getObjectSize(objType, null);
				i_new = ptrAdd(i_obj, context.graph.intConst(size));
				// TODO: PtrAtEnd<Array<T>> should add array length
			}
			PtrAtRef => {
				var i_array = normRef1(i_old.inputs[0]), arrayType = V3.arrayByteType;
				var i_index = normRef1(i_old.inputs[1]);
				var hsize = mach.getArrayElemOffset(arrayType, 0), scale = 1;
				var i_offset = genArrayElemOffset(hsize, scale, i_index);
				i_new = ptrAdd(i_array, i_offset);
			}
			PtrAtComponentField(field) => {
				var fieldRef = V3Op.extractIrSpec(i_old.op, field);
				i_new = componentFieldPtr(fieldRef);
			}
			PtrAtObjectField(field) => {
				var fieldRef = V3Op.extractIrSpec(i_old.op, field);
				var i_obj = normRef1(i_old.inputs[0]);
				var offset = context.graph.intConst(mach.classFieldOffset(fieldRef));
				i_new = ptrAdd(i_obj, offset);
			}
			PtrAtRefLayoutField(offset1) => {
				var i_array = normRef1(i_old.inputs[0]), arrayType = V3.arrayByteType;
				var i_index = normRef1(i_old.inputs[1]);
				var hsize = mach.getArrayElemOffset(arrayType, 0), scale = 1;
				var i_offset2 = genArrayElemOffset(hsize, scale, i_index);
				var i_offset = curBlock.opIntAdd(i_offset2, context.graph.intConst(offset1));
				i_new = ptrAdd(i_array, i_offset);
			}
			PtrCmpSwp => {
				if (!config.NativeCmpSwp) {
					var split = SsaBlockSplit.new(context, curBlock);
					var i_ptr = i_old.input0(), i_expect = i_old.input1(), i_val = i_old.inputs[2].dest;
					var t = i_old.op.typeArgs[1];
					var i_load: SsaInstr;
					var op_cmp: Operator;
					match (mach.machType(t)) {
						x: FloatType => op_cmp = x.opcache().opEqual;
						x: IntType => op_cmp = x.opEq();
						x: BoolType => op_cmp = Int.getType(false, 8).opEq();
						x: ClassType,
						x: ArrayType,
						x: FuncType,
						x: PointerType => op_cmp = V3Op.newRefEq(t);
// TODO						x: VoidType => ;
						_ => context.fail("unknown type for compare-swap");
					}
					if (!i_old.facts.O_NO_NULL_CHECK) i_load = ptrLoadT(i_old.source, t, i_ptr, 0);
					else i_load = ptrLoad(t, i_ptr, 0);
					var i_cmp = curBlock.pure(op_cmp, [i_load, i_expect]);
					curBlock = split.addIf(i_cmp);
					ptrStore(t, i_ptr, 0, i_val);
					curBlock = split.addElse();
					curBlock = split.finish();
					id(i_cmp);
					i_new = i_cmp;
					i_old.replace(i_cmp);
					i_old.remove();
				}
				i_new = i_old;
			}
			PtrAddRangeStart => {
				var i_ptr = normRef1(i_old.inputs[0]), i_rs = normRef1(i_old.inputs[1]);
				i_new = ptrAdd(i_ptr, i_rs);
			}
			TupleCreate,		// --
			ClassGetVirtual,	// --
			Init,			// --
			CallClassVirtual,	// --
			CallClosure =>		return unexpected(i_old);
			CallKernel,
			SystemCall => {
				var i_new = apply(i_old.source, i_old.op, normRefs(i_old.inputs));
				i_new.facts = i_new.facts | i_old.facts;
				mapMultiReturn(i_old, i_new, normType(i_old.op.sig.returnType()));
				return;
			}
			_ => i_new = normId(i_old); // By default, normalize inputs to other instructions.
		}
		if (i_new != i_old) map1(i_old, i_new);
		else id(i_old);
	}
	def unexpected(i_old: SsaApplyOp) {
		context.fail1("unexpected operator %q", i_old.op.render);
	}
	def normId(i_old: SsaApplyOp) -> SsaInstr {
		normInputs(i_old);
		id(i_old);
		return i_old;
	}
	def genIntCmp(i_old: SsaApplyOp, infixH: IntType -> Operator, infixL: IntType -> Operator) -> SsaInstr {
		var tn = normIntType(i_old.op.typeArgs[0]);
		if (tn == null || tn.size == 1) return normInputs(i_old);
		var ai_new = normRefs(i_old.inputs);
		return genIntCmpRec(i_old, tn, ai_new, tn.bigEndIndex(), infixH, infixL);
	}
	def genIntCmpRec(i_old: SsaApplyOp, tn: IntNorm, ai_new: Array<SsaInstr>, i: int, infixH: IntType -> Operator, infixL: IntType -> Operator) -> SsaInstr {
		var i_a = ai_new[i], i_b = ai_new[i + tn.size];
		var tt = IntType.!(tn.sub[i]);
		if (i == tn.littleEndIndex()) {
			var op = infixL(tt);
			return apply(i_old.source, op, [i_a, i_b]);
		}
		var op = if(i == tn.littleEndIndex(), infixL, infixH)(tt);
		var i_cmp = apply(i_old.source, op, [i_a, i_b]);
		var i_eq = apply(i_old.source, tt.opEq(), [i_a, i_b]);
		var next = i + if(mach.intNorm.bigEndian, 1, -1);
		var i_sub = genIntCmpRec(i_old, tn, ai_new, next, infixH, infixL);
		var i_and = apply(i_old.source, V3Op.opBoolAnd, [i_eq, i_sub]);
		return apply(i_old.source, V3Op.opBoolOr, [i_cmp, i_and]);
	}
	def genEqualOp(i_old: SsaApplyOp) -> SsaInstr {
		var tn = normType(i_old.op.typeArgs[0]);
		if (tn == null || tn.size == 1) return normInputs(i_old);
		return genEqualN(i_old, tn);
	}
	def genShiftOp(i_old: SsaApplyOp) {
		var op = i_old.op, tt = IntType.!(op.sig.returnType());
		// Introduce an explicit if-then-else for shift overflow.
		if (!i_old.facts.O_NO_SHIFT_CHECK) {
			// XXX: check the config for saturating shifts (e.g. arm32).
			var split = SsaBlockSplit.new(context, curBlock);
			var lt = Byte.TYPE.opLtEq();
			var max = context.graph.intConst(tt.width - 1);
			var ok = curBlock.add(lt, [normRef1(i_old.inputs[1]), max], Fact.O_PURE);
			// Split the block
			i_old.remove();
			var tblock = curBlock = split.addIf(ok);
			// Move shift to true case
			curBlock.append(i_old);
			i_old.facts |= Fact.O_NO_SHIFT_CHECK;
			// Construct false case
			var fval: SsaInstr;
			var fblock = curBlock = split.addElse();
			if (Opcode.IntSar.?(op.opcode)) {
				fval = curBlock.add(op, [i_old.input0(), max], Fact.O_PURE);
			} else {
				fval = context.graph.nullConst(tt);
			}
			// replace all uses with the phi
			var merge = split.finish();
			var phi = split.addPhi(tt, [fval, fval]);
			i_old.replace(phi);
			phi.inputs[0].update(i_old);
			// lower the original shift
			curBlock = tblock;
			curBlock.end = false;
			curBlock.pt = i_old;
			doShift(i_old, op, tt);
			// lower the false shift
			if (SsaApplyOp.?(fval)) {
				curBlock = fblock;
				curBlock.end = false;
				curBlock.pt = fval;
				doShift(SsaApplyOp.!(fval), op, tt);
			}
			// continue at the merge
			curBlock = merge;
			getReplacements(phi);
		} else {
			doShift(i_old, op, tt);
		}
	}
	def doSimpleShift(i_old: SsaApplyOp, op: Operator, tt: IntType) {
		normInputs(i_old);
		var trunc = Opcode.IntShl.?(op.opcode);
		var arith = config.getArithWidth(tt);
		var diff = arith.width - tt.width;
		var i0 = i_old.input0();
		if (diff <= 0) return void(id(i_old));
		if (Opcode.IntShl.?(op.opcode)) {
			// shift left in subword
			genTruncateInPlace(i_old, tt);
		} else if (Opcode.IntShr.?(op.opcode) && tt.signed && !i0.facts.V_NON_NEGATIVE) {
			// If this is a signed subword type, the value has is already implicitly sign-extended,
			// so first shift the sign bit left to the word's sign bit position, and then shift right
			var at = Int.getType(false, arith.width);
			var i1 = i_old.input1();
			var cdiff = context.graph.intConst(diff);
			var left = apply(i_old.source, at.opShl(), [i0, cdiff]);
			var right = apply(i_old.source, Byte.TYPE.opAdd(), [i1, cdiff]);
			var i_new: SsaInstr = curBlock.pure(at.opShr(), [left, right]);
			if (!i1.facts.V_NON_ZERO) i_new = curBlock.opIntViewI0(at, tt, i_new);
			return map1(i_old, i_new);
		}
		return void(id(i_old));
	}
	def doShift(i_old: SsaApplyOp, op: Operator, tt: IntType) {
		// 2 -> 1 operation.
		if (tt.width <= mach.intNorm.width) return doSimpleShift(i_old, op, tt);
		var inputs = normRefs(i_old.inputs);
		// N*2 -> N operation.
		var tn = mach.intNorm.makeType(tt);
		var wordCount = getWordShiftCount(inputs, tt, tn);
		if (wordCount > 0) {
			// Recognize shifts by constants that are a multiple of the word size.
			var bigEnd = tn.bigEndIndex();
			inputs = Arrays.range(inputs, 0, tn.size);
			var zero = context.graph.nullConst(mach.intNorm.word);
			if (Opcode.IntShl.?(op.opcode)) {
				tn.shiftLeft(inputs, wordCount, zero);
				inputs[bigEnd] = curBlock.opIntViewI0(mach.intNorm.word, tn.sub[bigEnd], inputs[bigEnd]);
			} else if (Opcode.IntSar.?(op.opcode)) {
				var sign = extendBigEnd(inputs[bigEnd], IntType.!(tn.sub[bigEnd]), true);
				tn.shiftRight(inputs, wordCount, sign);
			} else if (Opcode.IntShr.?(op.opcode)) {
				var upper = IntType.!(tn.sub[bigEnd]).unsigned();
				inputs[bigEnd] = curBlock.opIntViewI0(mach.intNorm.word, upper, inputs[bigEnd]);
				tn.shiftRight(inputs, wordCount, zero);
			}
			return mapN(i_old, inputs);
		}
		var trunc = false;
		if (Opcode.IntShl.?(op.opcode)) trunc = !i_old.facts.O_NO_INT_TRUNC;
		else if (Opcode.IntShr.?(op.opcode)) {
			op = mach.intNorm.shrOp;
			var bigEnd = tn.bigEndIndex();
			if ((tt.width % mach.intNorm.width != 0) && !inputs[bigEnd].facts.V_NON_NEGATIVE) {
				// convert input big end to unsigned
				var big = tn.sub[bigEnd], unsigned = IntType.!(big).unsigned();
				var cop = V3Op.newIntViewI(big, unsigned);
				inputs[bigEnd] = curBlock.opIntViewI(cop, inputs[bigEnd]);
				trunc = !inputs[tn.size].facts.V_NON_ZERO;
			}
		}
		var i_new = apply(i_old.source, V3Op.newIntWide(op, tn.sub, tn.newType), inputs);
		return mapWide(i_old, i_new, tn, trunc);
	}
	def getWordShiftCount(inputs: Array<SsaInstr>, tt: IntType, tn: IntNorm) -> int {
		for (i = tn.size + 1; i < inputs.length; i++) {
			if (!SsaConst.?(inputs[i]) || inputs[i].unbox<int>() != 0) return -1;
		}
		if (!SsaConst.?(inputs[tn.size])) return -1;
		var shift = inputs[tn.size].unbox<int>();
		if ((shift % mach.intNorm.width) != 0) return -1;
		return shift / mach.intNorm.width;
	}
	def genIntDivOrMod(i_old: SsaApplyOp) {
		if (i_old.checkFact(Fact.O_NO_DIV_CHECK)) return genTruncatingIntOp(i_old);
		if ((config.ExplicitDivChecks && Opcode.IntDiv.?(i_old.op.opcode)) ||
		    (config.ExplicitModChecks && Opcode.IntMod.?(i_old.op.opcode))) {
			var tt = IntType.!(i_old.op.sig.returnType());
			if (tt.signed) {
				var split = SsaBlockSplit.new(context, curBlock);
				var x = i_old.input0(), y = i_old.input1();
				var minus1 = context.graph.valConst(tt, tt.box(-1));
				var cmp = curBlock.add(tt.opEq(), [y, minus1], Facts.NONE);
				curBlock = split.addIf(cmp);

				var tv: SsaInstr, zero = context.graph.nullConst(tt);
				if (Opcode.IntDiv.?(i_old.op.opcode)) {
					var apply = curBlock.add(tt.opSub(), [zero, x], Facts.NONE);
					tv = genTruncateInPlace(apply, tt);  // XXX: weird to apply this to new instructions.
				} else {
					tv = zero;  // x % -1 == #0
				}
				curBlock = split.addElse();
				var i_new = id(curBlock.add(i_old.op, [x, y], i_old.facts | Fact.O_NO_DIV_CHECK));
				curBlock = split.finish();
				var phi = split.addPhi(tt, [tv, i_new]);
				i_old.replace(phi);
				i_old.remove();
				getReplacements(phi);  // XXX: side-effect of normalizing phi
				return;
			}
		}
		return genTruncatingIntOp(i_old);
	}
	def genTruncatingIntOp(i_old: SsaApplyOp) {
		var tt = IntType.!(i_old.op.sig.returnType());
		var tn = normIntType(tt);
		if (tn == null) {
			// normal width (i.e. 2 -> 1) operation.
			normInputs(i_old);
			genTruncateInPlace(i_old, tt);
		} else {
			// N*2 -> N operation.
			var i_new = apply(i_old.source, V3Op.newIntWide(i_old.op, tn.sub, tn.newType), normRefs(i_old.inputs));
			mapWide(i_old, i_new, tn, !i_old.facts.O_NO_INT_TRUNC);
		}
	}
	def genTruncateInPlace(i_old: SsaApplyOp, tt: IntType) -> SsaInstr {
		if (i_old.facts.O_NO_INT_TRUNC) return id(i_old);
		i_old.facts |= Fact.O_NO_INT_TRUNC; // {i_old} will not need truncation again
		var width = tt.width, arithWidth = mach.intNorm.width;
		if (width >= arithWidth) return id(i_old);
		if (config.Int8Arith && width <= 8) {
			if (width == 8) return id(i_old);
			arithWidth = 8;
		} else if (config.Int16Arith && width <= 16) {
			if (width == 16) return id(i_old);
			arithWidth = 16;
		} else if (config.Int32Arith && width <= 32) {
			if (width == 32) return id(i_old);
			arithWidth = 32;
		}

		curBlock.pt = i_old.next;
		var arithType = if(arithWidth == mach.intNorm.width, mach.intNorm.word, Int.getType(false, arithWidth));
		var trunc = curBlock.opIntViewI0(arithType, i_old.op.sig.returnType(), i_old);
		map1keep(i_old, trunc);
		trunc.inputs[0].update(i_old);
		return trunc;
	}
	def genParallelIntOp(i_old: SsaApplyOp, infix: IntType -> Operator) {
		var tt = IntType.!(i_old.op.sig.returnType());
		if (tt.width <= mach.intNorm.width) {
			// 2 -> 1 operation.
			normInputs(i_old);
			return void(id(i_old));
		}
		// N*2 -> N operation.
		var inputs = normRefs(i_old.inputs);
		var tn = mach.intNorm.makeType(tt);
		var vals = Array<SsaInstr>.new(tn.size);
		for (i < vals.length) {
			var op = infix(IntType.!(tn.sub[i]));
			vals[i] = apply(i_old.source, op, [inputs[i], inputs[i + tn.size]]);
		}
		return mapN(i_old, vals);
	}
	def extendBigEnd(bigEnd: SsaInstr, ft: IntType, signed: bool) -> SsaInstr {
		if (signed) {
			var shift = context.graph.intConst(mach.intNorm.width - 1);
			var i = apply(null, ft.opSar(), [bigEnd, shift]);
			i.facts |= Fact.O_NO_SHIFT_CHECK;
			return i;
		}
		return context.graph.nullConst(mach.intNorm.word);
	}
	def genIntViewI(i_old: SsaApplyOp) {
		var inputs = normRefs(i_old.inputs);
		var ft = IntType.!(i_old.op.sig.paramTypes[0]);
		var tt = IntType.!(i_old.op.sig.returnType());
		if (tt.width > mach.intNorm.width) {
			// M -> (N > 1) conversion.
			var ftn = mach.intNorm.makeType(ft);
			var ttn = mach.intNorm.makeType(tt);
			var fbig = ftn.bigEndIndex();
			if (ftn.size < ttn.size) {
				// M < N, so sign or zero extend the big end.
				var signed = ft.signed && !i_old.inputs[0].dest.facts.V_NON_NEGATIVE;
				var extend = extendBigEnd(inputs[fbig], ft, signed);
				var vals = ftn.growToN(inputs, ttn.size, extend);
				return mapN(i_old, vals);
			}
			// M >= N, so select N values from inputs.
			var vals = ftn.getLowestN(inputs, ttn.size);
			// truncate the big end if necessary.
			var fbt = ftn.sub[fbig], tbig = ftn.bigEndIndex(), tbt = ttn.sub[tbig];
			if (fbt != tbt) vals[tbig] = curBlock.opIntViewI0(fbt, tbt, vals[tbig]);
			return mapN(i_old, vals);
		}
		var i_new: SsaInstr;
		if (ft.width > mach.intNorm.width) {
			// (N > 1) -> 1 conversion.
			var ftn = mach.intNorm.makeType(ft);
			var ttn = mach.intNorm.makeType(tt);
			var lt = ftn.sub[ftn.littleEndIndex()];
			var little = inputs[ftn.littleEndIndex()];
			if (lt == ttn.newType) i_new = little;
			else i_new = opIntViewINorm(IntType.!(lt), IntType.!(ttn.newType), little);
		} else {
			// 1 -> 1 conversion.
			i_new = opIntViewINorm(ft, tt, inputs[0]);
		}
		return map1(i_old, i_new);
	}
	def genIntCastF(i_old: SsaApplyOp) {
		var ft = FloatType.!(i_old.op.typeArgs[0]);
		var itt = IntType.!(i_old.op.typeArgs[1]), tn = normIntType(itt);
		var ett = mostlyRoundUpIntType(itt);
		var x = i_old.input0();
		var i_new: SsaInstr;
		var rangeChecks = !i_old.facts.O_NO_BOUNDS_CHECK;
		var origNanCheck = i_old.facts & Fact.O_NO_NULL_CHECK;
		if (rangeChecks) {
			match (itt.rank) {
				SUBI32, SUBI64, SUBU32, SUBU64 => rangeChecks = true; // all subword types have special ranges
				U32, U64 => rangeChecks = !config.IntConvertFUnsigned || !config.IntCastFTraps; // other unsigned
				I32, I64 => rangeChecks = !config.IntCastFTraps; // if hardware can't trap
			}
		}
		if (rangeChecks) {
			// ConditionalThrow(#maxf < x); ConditionalThrow(x < #minf)
			var maxf = context.graph.valConst(ft, ft.max(itt));
			var check = apply(i_old.source, ft.opcache().opLt, [maxf, x]);
			check.facts |= origNanCheck;
			curBlock.opConditionalThrow(V3Exception.TypeCheck, check);
			var minf = context.graph.valConst(ft, ft.min(itt));
			check = apply(i_old.source, ft.opcache().opLt, [x, minf]);
			check.facts |= origNanCheck;
			curBlock.opConditionalThrow(V3Exception.TypeCheck, check);
		}
		// ConditionalThrow(x != FloatPromoteI(IntCastF(x))
		var cvts = wideOutputs(i_old.op, tn, [x], Fact.O_NO_BOUNDS_CHECK | i_old.facts);
		var back = genFloatConvertI(V3Op.newFloatPromoteI(itt, ft), cvts);
		var cmp = curBlock.opFloatBitEq(ft.is64, x, back);
		curBlock.opConditionalThrow(V3Exception.TypeCheck, curBlock.opBoolNot(cmp));
		mapN(i_old, cvts);
	}
	def wideOutputs(op: Operator, tn: TypeNorm, inputs: Array<SsaInstr>, facts: Fact.set) -> Array<SsaInstr> {
		if (tn != null) {
			var i_new = apply(curBlock.source, V3Op.newIntWide(op, op.sig.paramTypes, tn.newType), inputs);
			i_new.facts |= facts;
			var vals = Array<SsaInstr>.new(tn.sub.length);
			for (i < tn.size) vals[i] = apply(null, V3Op.newTupleGetElem(tn.newType, i), [i_new]);
			return vals;
		} else {
			var i_new = apply(curBlock.source, op, inputs);
			i_new.facts |= facts;
			return [i_new];
		}
	}
	def wideInputs(op: Operator, tn: TypeNorm, inputs: Array<SsaInstr>, facts: Fact.set) -> SsaInstr {
		if (tn != null) op = V3Op.newIntWide(op, tn.sub, op.sig.returnType());
		var i_new = apply(curBlock.source, op, inputs);
		i_new.facts |= facts;
		return i_new;
	}
	def genIntQueryF(i_old: SsaApplyOp) {
		if (i_old.facts.O_NO_BOUNDS_CHECK) return void(id(i_old));
		var ft = FloatType.!(i_old.op.typeArgs[0]);
		var itt = IntType.!(i_old.op.typeArgs[1]);
		var ett = mostlyRoundUpIntType(itt);
		var i_x = i_old.input0();
		var i_false = context.graph.falseConst();
		var origNanCheck = i_old.facts & Fact.O_NO_NULL_CHECK;
		// NB: queries always need boundary checks to avoid {int.max+1 -(saturate)-> int.max -(round)-> int.max+1}
		if (itt.signed) {
			// IntTruncF<F,iNN>(x) => if(x >= maxf, false, if(x < minf, false, IntPromote<iWW, F>(IntTruncF<F,iWW>(x)) == x))
			var i_maxf = context.graph.valConst(ft, ft.maxplus1(itt));
			var split = SsaBlockSplit.new(context, curBlock);
			var i_cmp1 = apply(i_old.source, ft.opcache().opLt, [i_x, i_maxf]);
			i_cmp1.facts |= origNanCheck;
			curBlock = split.addIfNot(i_cmp1);
			var i_minf = context.graph.valConst(ft, ft.min(itt));
			curBlock = split.addElse();
			var i_cmp2 = apply(i_old.source, ft.opcache().opLt, [i_x, i_minf]);
			i_cmp2.facts |= Facts.O_NO_NAN_CHECK; // NaN handled by first check
			curBlock = split.addIf(i_cmp2);
			curBlock = split.addElse();
			var i_cmp = genIntTruncFEqual(i_old.source, ft, ett, i_x);
			split.curBlock = curBlock; // XXX: should split even have a curblock?
			curBlock = split.finish();
			var i_new = split.addPhi(Bool.TYPE, [i_false, i_false, i_cmp]);
			map1(i_old, i_new);
			return;
		}
		if (!config.IntConvertFUnsigned) {
			// IntQueryF<F,uNN>(x) => if(x >= 0, if(x >= maxf, false, IntTruncFEqual<F,uWW>(x)), false)
			var split = SsaBlockSplit.new(context, curBlock);
			var i_fzero = context.graph.nullConst(ft);
			var i_cmp1 = apply(i_old.source, ft.opcache().opGteq, [i_x, i_fzero]);
			i_cmp1.facts |= origNanCheck;
			curBlock = split.addIfNot(i_cmp1);
			curBlock = split.addElse();
			var i_maxf = context.graph.valConst(ft, ft.maxplus1(itt));
			var i_cmp2 = apply(i_old.source, ft.opcache().opGteq, [i_x, i_maxf]);
			i_cmp2.facts |= Facts.O_NO_NAN_CHECK; // NaN handled by first check
			curBlock = split.addIf(i_cmp2);
			curBlock = split.addElse();
			var i_cmp = genIntTruncFEqual(i_old.source, ft, ett, i_x);
			split.curBlock = curBlock;
			curBlock = split.finish();
			var i_new = split.addPhi(Bool.TYPE, [i_false, i_false, i_cmp]);
			map1(i_old, i_new);
			return;
		}
		// IntTruncF<F,uNN>(x) => if(x >= maxf, maxi, IntTruncF<F,uWW>(x))
		var i_maxf = context.graph.valConst(ft, ft.maxplus1(itt));
		var split = SsaBlockSplit.new(context, curBlock);
		curBlock = split.addIf(apply(i_old.source, ft.opcache().opGteq, [i_x, i_maxf]));
		curBlock = split.addElse();
		var i_cmp = genIntTruncFEqual(i_old.source, ft, ett, i_x);
		split.curBlock = curBlock;
		curBlock = split.finish();
		var i_new = split.addPhi(Bool.TYPE, [i_false, i_cmp]);
		map1(i_old, i_new);
	}
	def mostlyRoundUpIntType(it: IntType) -> IntType {
		match (it.rank) {
			SUBU32, SUBI32 => return Int.getType(it.signed, 31);
			SUBU64, SUBI64 => return Int.getType(it.signed, 63);
			_ => return it;
		}
	}
	def genIntTruncFEqual(source: Source, ft: Type, itt: IntType, x: SsaInstr) -> SsaInstr {
		var tn = normIntType(itt);
		var truncs = wideOutputs(V3Op.newIntTruncF(ft, itt), tn, [x], Fact.O_NO_BOUNDS_CHECK);
		var back = genFloatConvertI(V3Op.newFloatPromoteI(itt, ft), truncs);
		return curBlock.opFloatBitEq(ft == Float.FLOAT64, x, back);
	}
	def genIntTruncF(i_old: SsaApplyOp) {
		if (i_old.facts.O_NO_BOUNDS_CHECK) return void(id(i_old));
		var ft = FloatType.!(i_old.op.typeArgs[0]);
		var itt = IntType.!(i_old.op.typeArgs[1]);
		var ett = mostlyRoundUpIntType(itt), tn = normIntType(ett);
		var i_x = i_old.input0();
		var origNanCheck = i_old.facts & Fact.O_NO_NULL_CHECK;
		// XXX: propagate no NaN check from {i_old} to child comparisons
		if (!config.IntConvertFUnsigned && !itt.signed) {
			// IntTruncF<F,uNN>(x) => if(x > 0, if(x >= maxf, maxi, IntTruncF<F,uWW>(x)), 0)
			var split = SsaBlockSplit.new(context, curBlock);
			var i_fzero = context.graph.nullConst(ft);
			var i_cmp1 = apply(i_old.source, ft.opcache().opGt, [i_x, i_fzero]);
			i_cmp1.facts |= origNanCheck;
			curBlock = split.addIfNot(i_cmp1);
			curBlock = split.addElse();
			var i_maxf = context.graph.valConst(ft, ft.maxplus1(itt));
			var i_cmp2 = apply(i_old.source, ft.opcache().opGteq, [i_x, i_maxf]);
			i_cmp2.facts |= Facts.O_NO_NAN_CHECK; // Nan covered by first check
			curBlock = split.addIf(i_cmp2);
			var i_maxi = context.graph.valConst(ett, itt.max);
			curBlock = split.addElse();
			var ai_truncs = wideOutputs(V3Op.newIntTruncF(ft, ett), tn, [i_x], Fact.O_NO_BOUNDS_CHECK);
			curBlock = split.finish();
			var i_izero = context.graph.nullConst(itt);
			var ai_phis = addPhis(curBlock, ett, tn, [wideConsts(tn, i_izero), wideConsts(tn, i_maxi), ai_truncs]);
			mapN(i_old, ai_phis);
			return;
		}
		var rangeCheckNeg: bool, rangeCheckPos: bool;
		var checkNan = !config.IntConvertFMapsNanToZero && !origNanCheck.O_NO_NULL_CHECK;
		match (itt.rank) {
			SUBI32, SUBI64 => {
				rangeCheckPos = !i_old.facts.O_NO_BOUNDS_CHECK;
				rangeCheckNeg = !i_old.facts.O_NO_BOUNDS_CHECK && !i_old.facts.O_NO_NEGATIVE_CHECK;
			}
			I32, I64 => {
				rangeCheckPos = !config.IntConvertFPosSaturates;
				rangeCheckNeg = !config.IntConvertFNegSaturates;
			}
			SUBU32, SUBU64 => {
				rangeCheckPos = !i_old.facts.O_NO_BOUNDS_CHECK;
				// assume that hardware does >= 0 check
			}
			U32, U64 => {
				rangeCheckPos = !config.IntConvertFPosSaturates;
			}
		}

		if (!rangeCheckNeg && !rangeCheckPos && !checkNan) {
			// target instruction does everything right
			i_old.facts |= Fact.O_NO_BOUNDS_CHECK;
			var ai_results = wideOutputs(i_old.op, tn, [i_x], Fact.O_NO_BOUNDS_CHECK);
			mapN(i_old, ai_results);
			return;
		}
		// at least one check is required.
		var results = Vector<Array<SsaInstr>>.new(); // collect cases
		var split = SsaBlockSplit.new(context, curBlock);
		var propNanCheck = origNanCheck;
		if (checkNan) {
			curBlock = split.addIfNot(apply(i_old.source, ft.opcache().opEqual, [i_x, i_x]));
			var i_izero = context.graph.nullConst(itt);
			results.put(wideConsts(tn, i_izero));
			curBlock = split.addElse();
			propNanCheck |= Facts.O_NO_NAN_CHECK;
		}
		if (rangeCheckNeg) {
			var i_fmin = context.graph.valConst(ft, ft.min(itt));
			var i_cmp = apply(i_old.source, ft.opcache().opLt, [i_x, i_fmin]);
			i_cmp.facts |= propNanCheck; // propagate previous NaN checks
			curBlock = split.addIf(i_cmp);
			var i_imin = context.graph.valConst(ett, itt.min);
			results.put(wideConsts(tn, i_imin));
			curBlock = split.addElse();
		}
		if (rangeCheckPos) {
			var i_fmax = context.graph.valConst(ft, ft.maxplus1(itt));
			var i_cmp = apply(i_old.source, ft.opcache().opGteq, [i_x, i_fmax]);
			i_cmp.facts |= propNanCheck; // propagate previous NaN checks
			curBlock = split.addIf(i_cmp);
			var i_imax = context.graph.valConst(ett, itt.max);
			results.put(wideConsts(tn, i_imax));
			curBlock = split.addElse();
		}
		// finally, do the actual truncation
		var ai_truncs = wideOutputs(V3Op.newIntTruncF(ft, ett), tn, [i_x], Fact.O_NO_BOUNDS_CHECK | propNanCheck);
		results.put(ai_truncs);
		curBlock = split.finish();
		var ai_phis = addPhis(curBlock, ett, tn, results.extract());
		mapN(i_old, ai_phis);
	}
	def wideConsts(tn: TypeNorm, v: SsaConst) -> Array<SsaInstr> {
		if (tn == null || tn.size == 1) return [v];
		return normConstAsArray(tn, v);
	}
	def addPhis(merge: SsaBuilder, t: Type, tn: TypeNorm, vals: Array<Array<SsaInstr>>) -> Array<SsaInstr> {
		var subs = if(tn == null, [t], tn.sub);
		var phis = Array<SsaInstr>.new(subs.length);
		for (i < subs.length) {
			var inputs = Array<SsaInstr>.new(vals.length);
			for (j < inputs.length) inputs[j] = vals[j][i];
			phis[i] = merge.addPhi(t, inputs);
		}
		return phis;
	}
	def genFloatCastI(i_old: SsaApplyOp) {
		var ft = FloatType.!(i_old.op.typeArgs[1]);
		var itt = IntType.!(i_old.op.typeArgs[0]), tn = normIntType(itt);
		var inputs = normRefs(i_old.inputs);
		if (TypeSystem.isPromotableToFloat(itt, ft)) {
			var i_new = wideInputs(V3Op.newFloatPromoteI(itt, ft), tn, inputs, Facts.NONE);
			return map1(i_old, i_new);
		}
		var maxi = context.graph.valConst(itt, itt.max);
		var eqmax = genEqual2(itt, tn, inputs, wideConsts(tn, maxi));
		curBlock.opConditionalThrow(V3Exception.TypeCheck, eqmax);
		var cvt = genFloatConvertI(V3Op.newFloatRoundI(itt, ft), inputs);
		var back = wideOutputs(V3Op.newIntTruncF(ft, itt), tn, [cvt], Facts.NONE);
		var cmp = genEqual2(itt, tn, inputs, back);
		var not = curBlock.opBoolNot(cmp);
		curBlock.opConditionalThrow(V3Exception.TypeCheck, not);
		return map1(i_old, cvt);
	}
	def genFloatCastD(i_old: SsaApplyOp) {
		var x = i_old.input0();
		var cvt = apply(i_old.source, V3Op.opFloatRoundD, [x]);
		var back = apply(i_old.source, V3Op.opFloatPromoteF, [cvt]);
		var cmp = curBlock.opFloatBitEq(true, x, back);
		curBlock.opConditionalThrow(V3Exception.TypeCheck, curBlock.opBoolNot(cmp));
		return map1(i_old, cvt);
	}
	def genFloatQueryI(i_old: SsaApplyOp) {
		var ft = FloatType.!(i_old.op.typeArgs[1]);
		var itt = IntType.!(i_old.op.typeArgs[0]), tn = normIntType(itt);
		if (TypeSystem.isPromotableToFloat(itt, ft)) {
			return map1(i_old, context.graph.trueConst());
		}
		var inputs = normRefs(i_old.inputs);
		var maxi = context.graph.valConst(itt, itt.max);
		var eqmax = genEqual2(itt, tn, inputs, wideConsts(tn, maxi));
		var neqmax = curBlock.opBoolNot(eqmax); // avoid int.max -(round)-> int.max+1 -(trunc)-> int.max
		var cvt = genFloatConvertI(V3Op.newFloatRoundI(itt, ft), inputs);
		var back = wideOutputs(V3Op.newIntTruncF(ft, itt), tn, [cvt], Fact.O_NO_BOUNDS_CHECK | Facts.O_NO_NAN_CHECK);
		var cmp = genEqual2(itt, tn, inputs, back);
		return map1(i_old, curBlock.opBoolAnd0(cmp, neqmax));
	}
	def genFloatConvertI(op: Operator, inputs: Array<SsaInstr>) -> SsaInstr {
		var itt = IntType.!(op.typeArgs[0]);
		var tn = normIntType(itt);
		var i_new: SsaInstr;

		if (!config.FloatConvertIUnsigned && itt.rank == IntRank.U32) {
			// zero-extend to i64 and convert
			var sit = Long.TYPE, ft = op.typeArgs[1];
			tn = normIntType(sit);
			match (op.opcode) {
				FloatPromoteI(is64) => op = V3Op.newFloatPromoteI(sit, op.typeArgs[1]);
				FloatRoundI(is64) => op = V3Op.newFloatRoundI(sit, op.typeArgs[1]);
				_ => context.fail("unexpected float conversion");
			}
			if (tn != null) {
				var p = inputs[0]; // TODO: assumes u32 not already normalized
				inputs = Array<SsaInstr>.new(tn.size);
				for (i < inputs.length) inputs[i] = context.graph.nullConst(tn.sub[0]);
				inputs[tn.littleEndIndex()] = p;
			}
			i_new = wideInputs(op, tn, inputs, Facts.NONE);
		} else if (!config.FloatConvertIUnsigned && itt.rank == IntRank.U64) {
			// if (x < 0) 2.0f * cvt((x & 1) | (x >> 1))
			var sit = Int.getType(true, itt.width), ft = FloatType.!(op.typeArgs[1]);
			tn = normIntType(itt);
			match (op.opcode) {
				FloatPromoteI(is64) => op = V3Op.newFloatPromoteI(sit, op.typeArgs[1]);
				FloatRoundI(is64) => op = V3Op.newFloatRoundI(sit, op.typeArgs[1]);
				_ => context.fail("unexpected float conversion");
			}
			var split = SsaBlockSplit.new(context, curBlock);
			var i: SsaInstr;
			var littleEndIndex: int, littleEndType: IntType;
			if (tn != null && tn.size > 0) { // normalized; compare upper end
				i = inputs[tn.bigEndIndex()];
				sit = IntType.!(tn.sub[tn.bigEndIndex()]);
				littleEndIndex = tn.littleEndIndex();
				littleEndType = IntType.!(tn.sub[littleEndIndex]);
			} else {
				i = inputs[0];
				littleEndType = sit;
			}
			var zero = context.graph.nullConst(sit);
			curBlock = split.addIf(apply(null, sit.opLtEq(), [zero, i]));
			var cvt = wideInputs(op, tn, inputs, Facts.NONE);
			curBlock = split.addElse();
			var littleEnd = inputs[littleEndIndex];
			var one = context.graph.valConst(littleEndType, Int.ONE);
			littleEnd =
				apply(null,
					littleEndType.opOr(),
					[apply(null,
						littleEndType.opShr(),
						[littleEnd, one]),
					apply(null,
						littleEndType.opAnd(),
						[littleEnd, one])]);
			var inputs2 = Arrays.dup(inputs);
			inputs2[littleEndIndex] = littleEnd;
			var cvt2 = wideInputs(op, tn, inputs2, Facts.NONE);
			var two = context.graph.valConst(ft, if(ft.is64, Float.f64(0, 1, 0), Float.f32(0, 1, 0)));
			var mul = apply(null, ft.opcache().opMul, [cvt2, two]);
			curBlock = split.finish();
			i_new = split.addPhi(ft, [cvt, mul]);
		} else {
			i_new = wideInputs(op, tn, inputs, Facts.NONE);
		}
		return id(i_new); // TODO: is id() correct here?
	}
	def genFloatQueryD(i_old: SsaApplyOp) {
		var x = i_old.input0();
		var cvt = apply(i_old.source, V3Op.opFloatRoundD, [x]);
		var back = apply(i_old.source, V3Op.opFloatPromoteF, [cvt]);
		var cmp = curBlock.opFloatBitEq(true, x, back);
		return map1(i_old, cmp);
	}
	def opIntViewINorm(ft: IntType, tt: IntType, x: SsaInstr) -> SsaInstr {
		if (ft.width == tt.width) {
			if (ft.width == mach.intNorm.width) return x;
			if (ft.width == 32 && config.Int32Arith) return x;
			if (ft.width == 16 && config.Int16Arith) return x;
			if (ft.width == 8 && config.Int8Arith) return x;
		}
		if (config.Int32Arith && TypeSystem.isIntToLong(ft, tt)) return curBlock.opIntViewI0(ft, tt, x);
		if (TypeSystem.isIntPromotable(ft, tt)) return x;
		return curBlock.opIntViewI0(ft, tt, x);
	}
	def mapWide(i_old: SsaApplyOp, i_new: SsaInstr, tn: IntNorm, truncate: bool) {
		if (tn == null) return map1(i_old, i_new);
		var vals = Array<SsaInstr>.new(tn.sub.length);
		for (i < tn.size) vals[i] = apply(i_old.source, V3Op.newTupleGetElem(tn.newType, i), [i_new]);
		if (truncate) {
			// truncate the big end of the result if necessary.
			var tbig = tn.bigEndIndex(), rem = tn.sub[tbig];
			if (IntType.!(rem).width < mach.intNorm.width) {
				vals[tbig] = apply(i_old.source, V3Op.newIntViewI(mach.intNorm.word, rem), [vals[tbig]]);
			}
		}
		mapN(i_old, vals);
	}
	def genTypeCast(i_old: SsaApplyOp, castOp: TypeCast) {
		var ft = i_old.op.typeArgs[0], tt = i_old.op.typeArgs[1];
		var i_new: SsaInstr;
		match (castOp) {
			TRUE, SUBSUME => i_new = normRef1(i_old.inputs[0]);  // XXX: maybe dead?
			CLASS_CAST, VARIANT_CAST => i_new = genClassCast(ft, tt, i_old);
			THROW => i_new = addThrow(i_old.source, V3Exception.TypeCheck);
			INT_VIEW_I => return genIntViewI(i_old);
			_ => {
				// other kinds of casts should have been removed
				context.fail1("unexpected cast %s", castOp.name);
				i_new = context.graph.nop();
			}
		}
		return map1(i_old, i_new);
	}
	def genClassCast(ft: Type, tt: Type, i_old: SsaApplyOp) -> SsaInstr {
		var oobj = i_old.inputs[0], nobj = normRef1(oobj);
		if (context.compiler.DisableTypeChecks) return nobj;
		var t = mach.classIdRange(tt), low = t.0, high = t.1;
		if (low == high) {
			// no live classes can match, only null
			if (V3Op.needsNullCheck(i_old, oobj.dest)) {
				var cmp = curBlock.opNotEqual(ft, nobj, context.graph.nullConst(ft));
				apply(i_old.source, V3Op.newConditionalThrow(V3Exception.TypeCheck), [cmp]);
				return context.graph.nullConst(tt);
			}
			return addThrow(i_old.source, V3Exception.TypeCheck);
		}
		return genIfNull(i_old, mach.machType(tt), nobj, null, genClassIdCheck(i_old, low, high, _));
	}
	def genClassIdCheck(i_old: SsaApplyOp, low: int, high: int, i_new: SsaInstr) -> SsaInstr {
		var i_tid = ptrLoad(mach.tagType, i_new, 0);
		curBlock.opIntRangeCheck(mach.code.addressSize, low, high, i_tid);
		return i_new;
	}
	def genTypeQuery(i_old: SsaApplyOp, query: TypeQuery) -> SsaInstr {
		var ft = i_old.op.typeArgs[0], tt = i_old.op.typeArgs[1];
		if (V3.isClass(ft) && V3.isClass(tt)) {
			var oobj = i_old.inputs[0], nobj = normRef1(oobj);
			var t = mach.classIdRange(tt), low = t.0, high = t.1;
			if (low == high) {
				// no live classes can match, and null is not an instance of
				return context.graph.falseConst();
			}
			return genIfNull(i_old, Bool.TYPE, nobj, null, genClassIdQuery(i_old, low, high, _));
		}
		return context.graph.trueConst();
	}
	def genClassIdQuery(i_old: SsaApplyOp, low: int, high: int, nobj: SsaInstr) -> SsaInstr {
		var tid = ptrLoad(mach.tagType, nobj, 0);
		return genRangeQuery(i_old, low, high, tid);
	}
	def genRangeQuery(i_old: SsaApplyOp, low: int, high: int, val: SsaInstr) -> SsaInstr {
		if (high == low + mach.code.addressSize) { // XXX: better factoring of degenerate range checks
			return apply(i_old.source, V3Op.newEqual(Int.TYPE), [val, context.graph.intConst(low)]);
		} else {
			var cmp1 = curBlock.opIntGteq(val, context.graph.intConst(low));
			var cmp2 = curBlock.opIntLt(val, context.graph.intConst(high));
			return apply(i_old.source, V3Op.opBoolAnd, [cmp1, cmp2]);
		}
	}
	def genTypeSubsume(i_old: SsaApplyOp) {
		mapN(i_old, normRefs(i_old.inputs));  // always a no-op at the machine level
	}
	def genArrayAlloc(i_old: SsaApplyOp) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var olen = i_old.inputs[0], arrayType = i_old.op.typeArgs[0];
		var hsize = mach.getArrayElemOffset(arrayType, 0), scale = mach.getArrayElemScale(arrayType);
		if (SsaConst.?(olen.dest)) {
			// length is known statically
			var len = olen.dest.unbox<int>();
			if (len < 0) return addThrow(i_old.source, V3Exception.LengthCheck);
			return genArrayAllocWithSize(i_old.source, arrayType, hsize, len, scale);
		}
		var nlen = normRef1(olen);
		if (!i_old.facts.O_NO_NEGATIVE_CHECK && !context.compiler.DisableLengthChecks) {
			// add a check (length < 0)
			var check = curBlock.opIntLt(nlen, context.graph.zeroConst());
			apply(i_old.source, V3Op.newConditionalThrow(V3Exception.LengthCheck), [check]);
		}
		var size: SsaInstr = context.graph.intConst(hsize);
		if (scale > 0) {
			// scale the length by the element scale
			var elemsize = nlen;
			if (scale > 1) elemsize = curBlock.opIntMul(nlen, context.graph.intConst(scale));
			if (scale != mach.data.addrAlign.alignUp(scale)) {
				// alignment is necessary
				size = curBlock.opIntAdd(elemsize, context.graph.intConst(hsize + mach.data.addrAlign.add));
				size = curBlock.opIntAnd(size, context.graph.intConst(mach.data.addrAlign.mask));
			} else {
				size = curBlock.opIntAdd(elemsize, size);
			}
		}
		// allocate the array, store tag, and store length
		mach.allocates = true;
		var narr = genAlloc(i_old.source, mach.machType(arrayType), size);
		storeObjectTag(narr, arrayType);
		ptrStore(Int.TYPE, narr, mach.getArrayLengthOffset(arrayType), nlen);
		return narr;
	}
	def genArrayAllocWithSize(source: Source, arrayType: Type, hsize: int, len: int, scale: int) -> SsaInstr {
		var totalSize = mach.data.addrAlign.alignUp(hsize + len * scale);
		// allocate the array with the known size
		mach.allocates = true;
		var narr = genAlloc(source, mach.machType(arrayType), context.graph.intConst(totalSize));
		storeObjectTag(narr, arrayType); // store tag
		ptrStore(Int.TYPE, narr, mach.getArrayLengthOffset(arrayType), context.graph.intConst(len)); // store length
		return narr;
	}
	def genAlloc(source: Source, mtype: Type, size: SsaInstr) -> SsaInstr {
		if (context.compiler.IrAlloc) {
			var m = genAllocMethod(Int.TYPE);
			var methodRef = IrSpec.new(m.receiver, TypeUtil.NO_TYPES, m);
			var funcRep = mach.funcRep(methodRef);
			var func = context.graph.valConst(funcRep.machType, mach.getCodeAddress(methodRef));
			var op = V3Op.newCallAddress(funcRep); // XXX: cache
			return apply(source, op, [func, context.graph.nullReceiver(), size]);
		} else {
			return apply(source, V3Op.newAlloc(mtype), [size]);
		}
	}
	def genArrayInit(i_old: SsaApplyOp) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var arrayType = i_old.op.typeArgs[0];
		var offset = mach.getArrayElemOffset(arrayType, 0), scale = mach.getArrayElemScale(arrayType);
		var narr = genArrayAllocWithSize(i_old.source, arrayType, offset, i_old.inputs.length, scale);
		var machType = mach.machType(V3Array.elementType(arrayType));
		var tn = normIntType(machType), stride = if(tn == null, 1, tn.size);
		var inputs = normRefs(i_old.inputs);
		for (i = 0; i < inputs.length; (i = i + stride, offset = offset + scale)) {
			// generate unchecked pointer stores to initialize the array
			// XXX: cache type normalization across calls to this method
			genNormTypedStores(i_old, Fact.O_NO_NULL_CHECK, true, machType, narr, offset, inputs, i);
		}
		return narr;
	}
	def genArrayTupleInit(i_old: SsaApplyOp, elems: int, length: int) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var arrayType = i_old.op.typeArgs[0];
		var arrayRep = mach.arrayRep(arrayType);
		var narr = genArrayAllocWithSize(i_old.source, arrayType, arrayRep.headerSize, length, arrayRep.elemScale);
		for (i < length) {
			for (j < elems) {
				var val = i_old.inputs[i * elems + j];
				var inputs = normRefN(val);
				var offset = arrayRep.headerSize + arrayRep.offsets[j] + i * arrayRep.elemScale;
				// XXX: cache type normalization across calls to this method
				genNormTypedStores(i_old, Fact.O_NO_NULL_CHECK, true, arrayRep.elemTypes[j], narr, offset, inputs, 0);
			}
		}
		return narr;
	}
	def genArrayGetElem(i_old: SsaApplyOp, elem: int) {
		if (config.ObjectSystem) return void(normId(i_old));
		var inputs = normRefs(i_old.inputs);
		var narr = inputs[0], nindex = inputs[1], arrayType = i_old.op.typeArgs[0];
		var nullity = context.compiler.nullity(i_old, narr);
		nullity = genBoundsCheck(i_old, nullity).1;
		var arrayRep = mach.arrayRep(arrayType);
		var offset = genArrayElemOffset(arrayRep.getElemElemOffset(elem), arrayRep.elemScale, nindex);
		// XXX: fold null check into pointer access if no bounds check
		var machType = mach.machType(arrayRep.getElemElemType(arrayType, elem));
		var loads = genNormTypedLoads(i_old.source, nullity, machType, ptrAdd(narr, offset), 0);
		mapN(i_old, loads);
	}
	def genArraySetElem(i_old: SsaApplyOp, elem: int) {
		if (config.ObjectSystem) return void(normId(i_old));
		var inputs = normRefs(i_old.inputs);
		var narr = inputs[0], nindex = inputs[1], arrayType = i_old.op.typeArgs[0];
		var nullity = context.compiler.nullity(i_old, narr);
		nullity = genBoundsCheck(i_old, nullity).1;
		var arrayRep = mach.arrayRep(arrayType);
		var offset = genArrayElemOffset(arrayRep.getElemElemOffset(elem), arrayRep.elemScale, nindex);
		// XXX: fold null check into pointer access if no bounds check
		var machType = mach.machType(arrayRep.getElemElemType(arrayType, elem));
		genNormTypedStores(i_old, nullity, false, machType, ptrAdd(narr, offset), 0, inputs, 2);
		i_old.kill();
		i_old.remove();
	}
	def genNormRangeGetElem(i_old: SsaApplyOp, elem: int) {
		if (config.ObjectSystem) return void(normId(i_old));
		var inputs = normRefs(i_old.inputs);
		var narr = inputs[0], rangeStart = inputs[1], nindex = inputs[2], rangeType = i_old.op.typeArgs[0];
		var arrayRep = mach.arrayRep(rangeType);
		var offset = genArrayElemOffset(if(arrayRep.offsets != null, arrayRep.offsets[elem], 0), arrayRep.elemScale, nindex);
		var pos = ptrAdd(ptrAdd(narr, rangeStart), offset);
		var machType = mach.machType(arrayRep.getElemElemType(rangeType, elem));
		var loads = genNormTypedLoads(i_old.source, Fact.O_NO_NULL_CHECK, machType, pos, 0);
		mapN(i_old, loads);
	}
	def genNormRangeSetElem(i_old: SsaApplyOp, elem: int) {
		if (config.ObjectSystem) return void(normId(i_old));
		var inputs = normRefs(i_old.inputs);
		var narr = inputs[0], rangeStart = inputs[1], nindex = inputs[2], rangeType = i_old.op.typeArgs[0];
		var arrayRep = mach.arrayRep(rangeType);
		var offset = genArrayElemOffset(if(arrayRep.offsets != null, arrayRep.offsets[elem], 0), arrayRep.elemScale, nindex);
		var pos = ptrAdd(ptrAdd(narr, rangeStart), offset);
		var machType = mach.machType(arrayRep.getElemElemType(rangeType, elem));
		genNormTypedStores(i_old, Fact.O_NO_NULL_CHECK, false, machType, pos, 0, inputs, 3);
		i_old.kill();
		i_old.remove();
	}
	def genByteArrayGetField(i_old: SsaApplyOp, offset: int) {
		var inputs = normRefs(i_old.inputs);
		var narr = inputs[0], ni_start = inputs[1];
		var fieldType = i_old.op.typeArgs[0];
		var narr_start = ptrAdd(narr, ni_start);
		var nullity = context.compiler.nullity(i_old, narr_start);
		if (!nullity.O_NO_NULL_CHECK && !config.ImplicitNullChecks) genNullCheck0(i_old.source, narr_start);
		var offsetConst = context.graph.intConst(offset);
		var arrayRep = mach.arrayRep(V3.arrayByteType);
		var arrayElemOffset = if(i_old.op.typeArgs[1] == V3Range.START_TYPE, offsetConst, genArrayElemOffset(arrayRep.getElemElemOffset(0), 1, offsetConst));
		var machType = mach.machType(fieldType);

		var base = ptrAdd(narr_start, arrayElemOffset), offset = 0;
		if (!IntType.?(machType)) return map1(i_old, ptrLoad(machType, base, offset));

		var tn = mach.intNorm.makeType(IntType.!(machType));

		// Generate full-word loads for all but the last word; TODO: little/big endian
		var loads = Array<SsaInstr>.new(tn.size), last = tn.size - 1;
		for (i < last) {
			var et = tn.sub[i];
			loads[i] = ptrLoad(et, base, offset);
			offset = offset + mach.sizeOf(et);
		}

		var et = IntType.!(tn.sub[last]);
		if (et.width >= 8 && (et.width & et.width - 1) == 0) {
			// common case; last load is a power of two and at least a full byte
			loads[last] = ptrLoad(et, base, offset);
		} else {
			// generate multiple power-of-2 byte loads
			var wordType = getMinimumArithSize(et.width);
			var etWidth = et.packedByteSize() * 8, read = 0;
			for (curWidth = mach.intNorm.width; curWidth >= 8; curWidth >>= 1) {
				if ((etWidth & curWidth) != 0) {
					var curType = Int.getType(false, curWidth);
					var load = ptrLoad(curType, base, offset + (read >> 3));
					if (curType != wordType) load = curBlock.opIntViewI0(curType, wordType, load);
					if (read != 0) {
						load = apply(i_old.source, wordType.opShl(), [load, context.graph.intConst(read)]);
						load = apply(i_old.source, wordType.opOr(), [loads[last], load]);
					}
					loads[last] = load;
					read += curWidth;
				}
			}
			// sign-extend or mask off fractional upper bits if necessary
			if (et.signed || (et.width & 7) != 0 ) loads[last] = curBlock.opIntViewI0(wordType, et, loads[last]);
		}
		mapN(i_old, loads);
	}
	def getMinimumArithSize(width: int) -> IntType {
		if (config.Int8Arith && width <= 8) return Int.getType(false, 8);
		if (config.Int16Arith && width <= 16) return Int.getType(false, 16);
		if (config.Int32Arith && width <= 32) return Int.getType(false, 32);
		return Int.getType(false, 64);
	}
	def genByteArraySetField(i_old: SsaApplyOp, offset: int) {
		var inputs = normRefs(i_old.inputs);
		var narr = inputs[0], ni_start = inputs[1], nvalue = inputs[2];
		var fieldType = i_old.op.typeArgs[0];
		var narr_start = ptrAdd(narr, ni_start);
		var nullity = context.compiler.nullity(i_old, narr_start);
		if (!nullity.O_NO_NULL_CHECK && !config.ImplicitNullChecks) genNullCheck0(i_old.source, narr_start);
		var arrayRep = mach.arrayRep(V3.arrayByteType);
		var offsetConst = context.graph.intConst(offset);
		var arrayElemOffset = if(i_old.op.typeArgs[1] == V3Range.START_TYPE, offsetConst, genArrayElemOffset(arrayRep.getElemElemOffset(0), 1, offsetConst));
		var machType = mach.machType(fieldType);
		var base = ptrAdd(narr_start, arrayElemOffset), offset = 0, vals = inputs[2 ...];

		if (!IntType.?(machType)) {
			ptrStore(machType, base, offset, vals[0]);
			i_old.kill();
			i_old.remove();
			return;
		}

		var tn = mach.intNorm.makeType(IntType.!(machType)), last = tn.size - 1;
		// Generate full-word stores for all but the last word; TODO: little/big endian
		for (i < last) {
			var et = tn.sub[i];
			ptrStore(et, base, offset, vals[i]);
			offset = offset + mach.sizeOf(et);
		}

		var et = IntType.!(tn.sub[last]), val = vals[last], op_shr = if(et.signed, et.opSar(), et.opShr());
		var etWidth = et.packedByteSize() * 8;
		var stored = 0;
		for (curWidth = mach.intNorm.width; curWidth >= 8; curWidth >>= 1) {
			// generate multiple power-of-2 byte stores
			if ((etWidth & curWidth) != 0) {
				var valTruncated = val;
				if (stored > 0) valTruncated = apply(i_old.source, op_shr, [val, context.graph.intConst(stored)]);
				var curType = Int.getType(et.signed, curWidth);
				ptrStore(curType, base, offset + (stored >> 3), valTruncated);
				stored += curWidth;
			}
		}
		i_old.kill();
		i_old.remove();
	}
	def genArrayElemOffset(headerSize: int, scale: int, index: SsaInstr) -> SsaInstr {
		if (SsaConst.?(index)) {
			// fold the offset calculation
			return context.graph.intConst(headerSize + scale * index.unbox<int>());
		} else {
			var offset = index;
			if (scale > 1) offset = curBlock.opIntMul(index, context.graph.intConst(scale));
			if (headerSize != 0) offset = curBlock.opIntAdd(offset, context.graph.intConst(headerSize));
			return offset;
		}
	}
	def genArrayGetLength(i_old: SsaApplyOp) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var oarr = i_old.inputs[0], narr = normRef1(oarr);
		var nullity = context.compiler.nullity(i_old, null);
		return refLoad(nullity, Int.TYPE, i_old, oarr, narr, mach.getArrayLengthOffset(i_old.op.typeArgs[0]));
	}
	def genClassAlloc(i_old: SsaApplyOp, method: IrMethod) {
		if (config.ObjectSystem) return void(normId(i_old));
		var classType = i_old.getType();
		var size = mach.getObjectSize(classType, null);
		// allocate the object
		mach.allocates = true;
		var nobj = genAlloc(i_old.source, mach.machType(classType), context.graph.intConst(size));
		storeObjectTag(nobj, classType);
		if (method != null) {
			var newRef = V3Op.extractIrSpec(i_old.op, method);
			var funcRep = mach.funcRep(newRef);
			// nontrivial constructor
			var func = context.graph.valConst(funcRep.machType, mach.getCodeAddress(newRef));
			var args = Arrays.prepend(func, Arrays.prepend(nobj, normRefs(i_old.inputs)));
			call(i_old, funcRep, args);
			return;
		}
		return map1(i_old, nobj);
	}
	def genClassGetField(isVariant: bool, i_old: SsaApplyOp, field: IrField) {
		if (config.ObjectSystem) return void(normId(i_old));
		var fieldRef = V3Op.extractIrSpec(i_old.op, field);
		var oobj = i_old.inputs[0], nobj = normRef1(oobj);
		var machType = mach.machType(fieldRef.getFieldType());
		var offset = mach.classFieldOffset(fieldRef);
		var nullity = context.compiler.nullity(i_old, oobj.dest);
		if (isVariant) {
			var tblock: SsaBuilder, fblock: SsaBuilder, merge: SsaBuilder;
			var sub: Array<Type>;
			if (!nullity.O_NO_NULL_CHECK) {
				var t = addIfNull(nobj);
				tblock = t.0;
				fblock = t.1;
				merge = t.2;
				curBlock = fblock;
				var tn = normIntType(machType);
				sub = if(tn == null, [machType], tn.sub);
			}
			var loads = genNormTypedLoads(i_old.source, Fact.O_NO_NULL_CHECK, machType, nobj, offset);
			if (!nullity.O_NO_NULL_CHECK) {
				fblock.addGoto(merge.block);
				tblock.addGoto(merge.block);
				curBlock = merge;
				// if (nobj == null) emit null values
				for (i < loads.length) {
					loads[i] = curBlock.addPhi(sub[i], [loads[i], context.graph.nullConst(sub[i])]);
				}
			}
			mapN(i_old, loads);
		} else {
			var loads = genNormTypedLoads(i_old.source, nullity, machType, nobj, offset);
			mapN(i_old, loads);
		}
	}
	def genClassSetField(i_old: SsaApplyOp, field: IrField, init: bool) {
		if (config.ObjectSystem) return void(normId(i_old));
		var inputs = normRefs(i_old.inputs);
		var fieldRef = V3Op.extractIrSpec(i_old.op, field), nobj = inputs[0];
		var offset = mach.classFieldOffset(fieldRef);
		var machType = mach.machType(fieldRef.getFieldType());
		var nullity = compiler.nullity(i_old, nobj);
		genNormTypedStores(i_old, nullity, init, machType, nobj, offset, inputs, 1);
		i_old.kill();
		i_old.remove();
	}
	def genClassGetMethod(i_old: SsaApplyOp, method: IrMethod) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var methodRef = V3Op.extractIrSpec(i_old.op, method);
		var funcRep = mach.funcRep(methodRef);
		genNullCheck(i_old);
		return context.graph.valConst(funcRep.machType, mach.getCodeAddress(methodRef));
	}
	def genClassGetSelector(i_old: SsaApplyOp, selector: IrSelector) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var methodRef = V3Op.extractIrSpec(i_old.op, selector);
		var funcRep = mach.funcRep(methodRef);
		var oobj = i_old.inputs[0], nobj = normRef1(oobj);
		return genMtableLookup(i_old, oobj, nobj, funcRep, methodRef);
	}
	def genVariantGetMethod(i_old: SsaApplyOp, method: IrMethod) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var methodRef = V3Op.extractIrSpec(i_old.op, method);
		var funcRep = mach.funcRep(methodRef);
		return context.graph.valConst(funcRep.machType, mach.getCodeAddress(methodRef));
	}
	def genVariantGetSelector(i_old: SsaApplyOp, selector: IrSelector) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var methodRef = V3Op.extractIrSpec(i_old.op, selector);
		var funcRep = mach.funcRep(methodRef);
		var oobj = i_old.inputs[0], nobj = normRef1(oobj);
		var defm = mach.getCodeAddress(context.prog.ir.resolveVariantDefaultMethodImpl(methodRef));
		return genIfNull(i_old, funcRep.machType, nobj, defm, genMtableLookup(i_old, oobj, _, funcRep, methodRef));
	}
	def genMtableLookup(i_old: SsaApplyOp, oobj: SsaDfEdge, nobj: SsaInstr, funcRep: Mach_FuncRep, methodRef: IrSpec) -> SsaInstr {
		// use method-table based dispatch
		var nullity = context.compiler.nullity(i_old, oobj.dest);
		var tid = refLoad(nullity, mach.tagType, i_old, oobj, nobj, 0);
		var mtbl = context.graph.valConst(mach.data.ptrType, mach.methodTable(methodRef));
		return ptrLoad(funcRep.machType, ptrAdd(mtbl, tid), 0);
	}
	def genVariantReplaceNull(i_old: SsaApplyOp) -> SsaInstr {
		var vt = i_old.op.typeArgs[0];
		var val = i_old.input0();
		if (V3.getVariantTag(vt) == 0) return val; // tag == 0 should be prepared for null
		var t = addIfNull(val);
		var tblock = t.0, fblock = t.1, merge = t.2;
		curBlock = fblock;
		fblock.addGoto(merge.block);
		tblock.addGoto(merge.block);
		curBlock = merge;
		var record = V3.makeDefaultVariantRecord(context.prog, vt);
		var addr = mach.machVal(record);
		return curBlock.addPhi(vt, [val, context.graph.valConst(mach.machType(vt), addr)]);
	}
	def genComponentGetField(i_old: SsaApplyOp, field: IrField) {
		if (config.ObjectSystem) return void(normId(i_old));
		var fieldRef = V3Op.extractIrSpec(i_old.op, field);
		var fieldType = mach.machType(fieldRef.getFieldType());
		var ptr = componentFieldPtr(fieldRef);
		var loads = genNormTypedLoads(i_old.source, Fact.O_NO_NULL_CHECK, fieldType, ptr, 0);
		mapN(i_old, loads);
	}
	def genComponentSetField(i_old: SsaApplyOp, field: IrField) {
		if (config.ObjectSystem) return void(normId(i_old));
		var fieldRef = V3Op.extractIrSpec(i_old.op, field);
		var inputs = normRefs(i_old.inputs);
		var machType = mach.machType(fieldRef.getFieldType());
		var ptr = componentFieldPtr(fieldRef);
		// generate remaining stores for normalized fields.
		genNormTypedStores(i_old, Fact.O_NO_NULL_CHECK, false, machType, ptr, 0, inputs, 1);
		i_old.kill();
		i_old.remove();
	}
	def genTupleGetElem(i_old: SsaApplyOp, index: int) {
		var tn = TupleNorm.!(normType(i_old.op.typeArgs[0]));
		mapN(i_old, tn.getElem(normRefs(i_old.inputs), index));
	}
	def genNullCheck(i_old: SsaApplyOp) -> SsaInstr {
		var oobj = i_old.inputs[0], nobj = normRef1(oobj);
		i_old.facts = Facts.NONE;
		var nullity = context.compiler.nullity(i_old, oobj.dest);
		if (!nullity.O_NO_NULL_CHECK) genNullCheck0(i_old.source, nobj);
		return nobj;
	}
	def genNullCheck0(source: Source, nobj: SsaInstr) {
		if (SsaConst.?(nobj)) {
			var oval = SsaConst.!(nobj).val;
			if (oval == null) addThrow(source, V3Exception.NullCheck);
			return; // constant is non-null
		}
//TODO		if (config.ObjectSystem) return normId(i_old);
		ptrLoadT(source, Void.TYPE, nobj, 0);
	}
	def genBoundsCheck(i_old: SsaApplyOp, nullity: Fact.set) -> (SsaInstr, Fact.set) {
		var oarr = i_old.inputs[0], narr = normRef1(oarr);
		if (context.compiler.DisableBoundsChecks || i_old.facts.O_NO_BOUNDS_CHECK) {
			if (!nullity.O_NO_NULL_CHECK) genNullCheck0(i_old.source, narr);
			return (context.graph.nullConst(Void.TYPE), nullity | Fact.O_NO_NULL_CHECK);
		}
		if (config.ObjectSystem) return (normId(i_old), nullity);
		// load length
		// XXX: CSE the array length if possible
		var len = refLoad(nullity, Int.TYPE, i_old, oarr, narr, mach.getArrayLengthOffset(i_old.op.typeArgs[0]));
		var index = normRef1(i_old.inputs[1]);
		var throwOp = V3Op.newConditionalThrow(V3Exception.BoundsCheck);
		// throw BoundsCheckException if ugteq(index, length)
		var op = Int.getType(false, 32).opLtEq();
		apply(i_old.source, throwOp, [apply(i_old.source, op, [len, index])]);
		return (index, nullity | Fact.O_NO_NULL_CHECK);
	}
	def genCallMethod(i_old: SsaApplyOp, method: IrMethod) {
		if (config.ObjectSystem) return void(normId(i_old));
		var methodRef = V3Op.extractIrSpec(i_old.op, method);
		var funcRep = mach.funcRep(methodRef);
		var func = context.graph.valConst(funcRep.machType, mach.getCodeAddress(methodRef));
		var args = Arrays.prepend(func, normRefs(i_old.inputs));
		call(i_old, funcRep, args);
	}
	def genCallClassMethod(i_old: SsaApplyOp, method: IrMethod) {
		if (config.ObjectSystem) return void(normId(i_old));
		var methodRef = V3Op.extractIrSpec(i_old.op, method);
		if (!i_old.facts.O_NO_NULL_CHECK) genNullCheck(i_old);
		var funcRep = mach.funcRep(methodRef);
		var func = context.graph.valConst(funcRep.machType, mach.getCodeAddress(methodRef));
		var args = Arrays.prepend(func, normRefs(i_old.inputs));
		call(i_old, funcRep, args);
	}
	def genCallClassSelector(i_old: SsaApplyOp, selector: IrSelector) {
		if (config.ObjectSystem) return void(normId(i_old));
		var func = genClassGetSelector(i_old, selector);
		var methodRef = V3Op.extractIrSpec(i_old.op, selector);
		var args = Arrays.prepend(func, normRefs(i_old.inputs));
		call(i_old, mach.funcRep(methodRef), args);
	}
	def genCallVariantSelector(i_old: SsaApplyOp, selector: IrSelector) {
		if (config.ObjectSystem) return void(normId(i_old));
		var func = genVariantGetSelector(i_old, selector);
		var methodRef = V3Op.extractIrSpec(i_old.op, selector);
		var args = Arrays.prepend(func, normRefs(i_old.inputs));
		call(i_old, mach.funcRep(methodRef), args);
	}
	def genVariantGetTag(i_old: SsaApplyOp, nobj: SsaInstr) -> SsaInstr {
		if (config.ObjectSystem) return normId(i_old);
		var val = ptrLoad(mach.tagType, nobj, 0);
		var root = V3.getRootType(i_old.op.typeArgs[0]);
		var min = mach.classIdRange(root).0;
		if (min != 0) {
			val = apply(i_old.source, mach.tagType.opSub(), [val, context.graph.intConst(min)]);
		}
		var shift = mach.code.addressSizeLog2;
		if (shift > 0) {
			val = apply(i_old.source, mach.tagType.opShr(), [val, context.graph.intConst(shift)]);
		}
		var conv = V3Op.newIntViewI(mach.tagType, i_old.op.sig.returnType());
		return apply(i_old.source, conv, [val]);
	}

	def explicitNullCheck(source: Source, t: Type, nobj: SsaInstr) -> SsaInstr {
		if (context.compiler.DisableNullChecks) return nobj;
		var check = apply(source, V3Op.newRefEq(t), [nobj, context.graph.nullConst(t)]);
		apply(source, V3Op.newConditionalThrow(V3Exception.NullCheck), [check]);
		return nobj;
	}
	def storeObjectTag(nobj: SsaInstr, t: Type) {
		var tag = context.graph.valConst(mach.tagType, mach.objectTag(t));
		ptrStore(mach.tagType, nobj, 0, tag);
	}
	def genIfNull(i_old: SsaApplyOp, resultType: Type, nobj: SsaInstr, nullVal: Val, gen: SsaInstr -> SsaInstr) -> SsaInstr {
		if (nobj.facts.V_ZERO) {
			return context.graph.valConst(resultType, nullVal);
		} else if (V3Op.needsNullCheck(i_old, nobj)) {
			var t = addIfNull(nobj);
			var tblock = t.0, fblock = t.1, merge = t.2;
			curBlock = fblock;
			// if (nobj != null) generate the nonnull case
			var nonNull = gen(nobj);
			fblock.addGoto(merge.block);
			tblock.addGoto(merge.block);
			curBlock = merge;
			// if (nobj == null) use the null value
			return curBlock.addPhi(resultType, [nonNull, context.graph.valConst(resultType, nullVal)]);
		} else {
			return gen(nobj);
		}
	}

	def call(i_old: SsaApplyOp, funcRep: Mach_FuncRep, args: Array<SsaInstr>) {
		if (curBlock.end) return;
		var i_new = apply(i_old.source, if(config.ObjectSystem, i_old.op, V3Op.newCallAddress(funcRep)), args);
		i_new.facts = i_new.facts | i_old.facts;
		var tn = normType(i_old.op.sig.returnType());
		mapMultiReturn(i_old, i_new, tn);
	}
	def apply(source: Source, op: Operator, args: Array<SsaInstr>) -> SsaInstr {
		return curBlock.addApply(source, op, args);
	}
	def ptrAdd(base: SsaInstr, offset: SsaInstr) -> SsaInstr {
		var pt = base.getType(), pd = pt;
		if (SsaConst.?(offset)) {
			var offsetVal = SsaConst.!(offset).val;
			// XXX: fold address calculation in optimizer, not here
			if (base.optag() == Opcode.PtrAdd.tag) {
				// match PtrAdd(PtrAdd(left, right), #offset)
				var left = base.inputs[0].dest, right = base.inputs[1].dest;
				if (SsaConst.?(left)) {
					// PtrAdd(PtrAdd(#left, right), #offset)
					var leftVal = SsaConst.!(left).val;
					match (SsaConst.!(left).val) {
						null => return ptrAdd(right, offset);
						x: Addr => left = context.graph.valConst(pd, x.add(V3.unboxI32(offsetVal)));
						_ => left = context.graph.valConst(pd, Int.box(V3.unboxI32(leftVal) + V3.unboxI32(offsetVal)));

					}
					return ptrAdd(left, right);
				}
				if (SsaConst.?(right)) {
					// PtrAdd(PtrAdd(left, #right), #offset)
					var rightVal = SsaConst.!(right).val;
					offset = context.graph.valConst(pd, Int.box(V3.unboxI32(rightVal) + V3.unboxI32(offsetVal)));
					base = left;
				}
			} else if (SsaConst.?(base)) {
				match (SsaConst.!(base).val) {
					null => if (Addr.?(offsetVal)) return offset;
					x: Addr => {
						// fold PtrAdd(#addr, #offset)
						var addr = context.graph.valConst(pd, x.add(V3.unboxI32(offsetVal)));
						addr.facts |= Fact.V_NON_ZERO;
						return addr;
					}
					_ => base = context.graph.valConst(pd, SsaConst.!(base).val);
				}
			}
		}
		return apply(null, newPtrAdd(pt), [base, offset]);
	}
	def newPtrAdd(pt: Type) -> Operator { // XXX: cache Pointer add operator
		var it = Int.getType(true, mach.data.addressWidth);
		return V3Op.newPtrAdd(pt, it);
	}
	def ptrLoad(vt: Type, p: SsaInstr, offset: int) -> SsaInstr {
		if (offset != 0) p = ptrAdd(p, context.graph.intConst(offset));
		var i = apply(null, V3Op.newPtrLoad(p.getType(), vt), [p]);
		i.facts |= Fact.O_NO_NULL_CHECK; // this load won't trap
		return i;
	}
	def ptrLoadT(source: Source, t: Type, p: SsaInstr, offset: int) -> SsaInstr {
		if (!config.ImplicitNullChecks) {
			explicitNullCheck(source, p.getType(), p);
			if (t == Void.TYPE) return context.graph.nop();
		}
		if (offset != 0) p = ptrAdd(p, context.graph.intConst(offset));
		return apply(source, V3Op.newPtrLoad(p.getType(), t), [p]); // this load may trap
	}
	def ptrStore(vt: Type, p: SsaInstr, offset: int, v: SsaInstr) -> SsaInstr {
		var i = ptrStoreCommon(null, vt, p, offset, v);
		i.facts |= Fact.O_NO_NULL_CHECK; // this store won't trap
		return i;
	}
	def ptrStoreT(source: Source, vt: Type, p: SsaInstr, offset: int, v: SsaInstr) -> SsaInstr {
		if (!config.ImplicitNullChecks) {
			explicitNullCheck(source, p.getType(), p);
			if (vt == Void.TYPE) return context.graph.nop();
		}
		return ptrStoreCommon(source, vt, p, offset, v);
	}
	def ptrStoreCommon(source: Source, vt: Type, p: SsaInstr, offset: int, v: SsaInstr) -> SsaInstr {
		if (offset != 0) p = ptrAdd(p, context.graph.intConst(offset));
		if (config.StoreNarrow && vt.typeCon.kind == Kind.INT) {
			v = eliminateStoreNarrow(IntType.!(vt), v);
		}
		var i = apply(source, V3Op.newPtrStore(p.getType(), vt), [p, v]);
		return i;
	}
	def eliminateStoreNarrow(tt: IntType, v: SsaInstr) -> SsaInstr {
		if (!SsaApplyOp.?(v)) return v;
		var apply = SsaApplyOp.!(v);
		if (apply.op.opcode != Opcode.IntViewI) return v;
		var ft = IntType.!(apply.op.typeArgs[0]);
		if (ft.width <= tt.width) return v;
		if (IntType.!(apply.op.typeArgs[1]).width < tt.width) return v;
		if (tt.width == 8 && config.Int8StoreNarrow) return getInputAndMaybeDead(apply);
		if (tt.width == 16 && config.Int16StoreNarrow) return getInputAndMaybeDead(apply);
		if (tt.width == 32 && config.Int32StoreNarrow) return getInputAndMaybeDead(apply);
		return v;
	}
	def getInputAndMaybeDead(v: SsaApplyOp) -> SsaInstr {
		var i = v.input0();
		maybeDeadCodeLater(v);
		return i;
	}
	def refLoad(nullity: Fact.set, vt: Type, i_old: SsaApplyOp, oobj: SsaDfEdge, nobj: SsaInstr, offset: int) -> SsaInstr {
		return if(nullity.O_NO_NULL_CHECK, ptrLoad(vt, nobj, offset), ptrLoadT(i_old.source, vt, nobj, offset));
	}
	def componentFieldPtr(f: IrSpec) -> SsaInstr {
		return context.graph.valConst(mach.data.ptrType, mach.componentFieldPtr(f));
	}
	def addIfNull(x: SsaInstr) -> (SsaBuilder, SsaBuilder, SsaBuilder) {
		var t = splitCurBlock(); // XXX: switch to using SsaBlockSplit
		curBlock.addIfNull(x, t.0.block, t.1.block);
		return t;
	}
	def splitCurBlock() -> (SsaBuilder, SsaBuilder, SsaBuilder) {
		var tblock = newBlock(), fblock = newBlock(), merge = newBlock();
		var exit = curBlock.block;
		var end = exit.end();
		exit.prev = null;
		end.next = null;

		var last = curBlock.pt;
		var first = last.next;
		last.next = exit;
		exit.prev = last;
		curBlock.end = false;

		first.prev = merge.block;
		merge.block.next = first;
		merge.pt = first;

		merge.block.prev = end;
		end.next = merge.block;

		return (tblock, fblock, merge);
	}
	def newBlock() -> SsaBuilder {
		var b = SsaBuilder.new(context, context.graph, SsaBlock.new());
		b.source = curBlock.source;
		return b;
	}
	// Support for normalizing loads and stores.
	def genNormTypedLoads(source: Source, nullity: Fact.set, machType: Type, base: SsaInstr, offset: int) -> Array<SsaInstr> {
		var tn = normIntType(machType);
		// check for simple case first.
		if (tn == null) {
			var result = if(!nullity.O_NO_NULL_CHECK, ptrLoadT(source, machType, base, offset), ptrLoad(machType, base, offset));
			return [result];
		}
		// generate multiple loads for normalized fields and array elements.
		var loads = Array<SsaInstr>.new(tn.size);
		var check = if(nullity.O_NO_NULL_CHECK, -1);
		for (i < loads.length) {
			var et = tn.sub[i];
			loads[i] = if(i == check, ptrLoadT(source, et, base, offset), ptrLoad(et, base, offset));
			offset = offset + mach.sizeOf(et);
		}
		return loads;
	}
	def genNormTypedStores(i_old: SsaApplyOp, nullity: Fact.set, init: bool, machType: Type, base: SsaInstr, offset: int, vals: Array<SsaInstr>, start: int) {
		var tn = normIntType(machType), nullCheck = !nullity.O_NO_NULL_CHECK;
		// check for simple case first.
		if (tn == null) {
			var val = vals[start];
			if (nullCheck) ptrStoreT(i_old.source, machType, base, offset, val);
			else if (isNonTrivialStore(init, val)) ptrStore(machType, base, offset, val);
			return;
		}
		// generate multiple stores for normalized fields and array elements.
		var check = if(nullCheck, 0, -1);
		for (i < tn.size) {
			var et = tn.sub[i], val = vals[start + i];
			if (i == check) ptrStoreT(i_old.source, et, base, offset, val);
			else if (isNonTrivialStore(init, val)) ptrStore(et, base, offset, val);
			offset = offset + mach.sizeOf(et);
		}
	}
	def isNonTrivialStore(init: bool, v: SsaInstr) -> bool {
		if (init) {
			if (SsaConst.?(v)) return !Values.equal(SsaConst.!(v).val, null);
		}
		return true;
	}
	def maybeDeadCodeLater(i: SsaInstr) {
		// XXX: this is a cheap way of dead code elimination
		maybeDead = List.new(i, maybeDead);
	}
	def addThrow(source: Source, exception: string) -> SsaInstr {
		if (curBlock.end) return null;
		curBlock.end = true;
		if (cfopt == null) cfopt = SsaCfOptimizer.new(context);
		return cfopt.insertThrow(curBlock.pt, source, exception, null);
	}

	def genAllocMethod(size: IntType) -> IrMethod {
		if (mach.allocMethod != null) return mach.allocMethod;
		var receiverType = AnyRef.TYPE, returnType = AnyRef.TYPE, ptr = mach.data.ptrType;
		var vst = VstMethod.new(false, Token.new(null, ".alloc", 0, 0), null, null, ReturnType.Explicit(TypeUtil.typeRefOf(returnType)), null);
		var p_receiver = SsaParam.new(0, receiverType);
		var p_size = SsaParam.new(1, size);
		var graph = SsaGraph.new([p_receiver, p_size], returnType);
		var ssa = SsaBuilder.new(context, graph, graph.startBlock);

		if (CLOptions.RT_TEST_GC.get()) {
			// Always call RiRuntime.gc()
			var i_collect = genCallRiGc(ssa, p_size);
			ssa.addReturn([i_collect]);
		} else {
			// Allocation using CiRuntime.{heapCurLoc, heapEndLoc}
			var k_heapCur = graph.valConst(ptr, CiRuntimeModule.HEAP_CUR_LOC);
			var k_heapEnd = graph.valConst(ptr, CiRuntimeModule.HEAP_END_LOC);
			var i_cur = ssa.add(V3Op.newPtrLoad(ptr, ptr), [k_heapCur], Fact.O_NO_NULL_CHECK);
			var i_next = ssa.add(V3Op.newPtrAdd(ptr, size), [i_cur, p_size], Fact.O_PURE);

			if (!CLOptions.RT_GC.get()) {
				// runtime support for GC is turned off; no need to test against end. TODO: ugly
				ssa.add(V3Op.newPtrStore(ptr, ptr), [k_heapCur, i_next], Fact.O_NO_NULL_CHECK);
				ssa.addReturn([i_cur]);
			} else {
				var i_end = ssa.add(V3Op.newPtrLoad(ptr, ptr), [k_heapEnd], Fact.O_NO_NULL_CHECK);
				var i_lt = ssa.add(V3Op.newPtrLteq(ptr), [i_next, i_end], Fact.O_PURE);
				var split = SsaBlockSplit.new(context, ssa);
				ssa = split.addIf(i_lt);
				ssa.add(V3Op.newPtrStore(ptr, ptr), [k_heapCur, i_next], Fact.O_NO_NULL_CHECK);
				ssa.addReturn([i_cur]);
				ssa = split.addElse();
				var i_collect = genCallRiGc(ssa, p_size);
				if (i_collect != null) ssa.addReturn([i_collect]);
				else ssa.addThrow(null, V3Exception.HeapOverflow);
				split.finish();
			}
		}

		var meth = IrMethod.new(receiverType, null, Function.sig(size, returnType));
		meth.ssa = graph;
		context.prog.ir.methods.put(meth);
		mach.allocStub = mach.addrOfMethod(meth); // is implemented by IR
		return mach.allocMethod = meth;
	}
	def genCallRiGc(ssa: SsaBuilder, p_size: SsaInstr) -> SsaInstr {
		var rt_collect = mach.runtime.getRiGc();
		if (rt_collect == null) return null;
		// XXX: tail call RiRuntime.gc()
		var i_caller_ip = ssa.add(V3Op.newCallerIp(mach.code.ptrType), Ssa.NO_INSTRS, Fact.O_PURE);
		var i_caller_sp = ssa.add(V3Op.newCallerSp(mach.data.ptrType), Ssa.NO_INSTRS, Fact.O_PURE);
		var op = V3Op.newCallMethod(IrSpec.new(rt_collect.receiver, [rt_collect.receiver], rt_collect));
		return ssa.add(op, [context.graph.nullReceiver(), p_size, i_caller_ip, i_caller_sp], Facts.NONE);
	}
}
